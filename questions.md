## Date - 2022-04-24

## Title - Mia and the feedback loop

### **Question** :

Mia was crushing her thesis!

She was about to release a new neural network architecture that promised to raise the bar on image classification problems.

Mia did not start from scratch. She modified an existing model but added a key ingredient: feedback loops.

A feedback loop is when connections between units form a directed cycle, thus creating loops in the network. This gave Mia's network the ability to save information in the hidden layers.

Mia did a lot of research before deciding in favor of this architecture. She knew the advantages of her decision.

**Which was the architecture that Mia studied to learn about feedback loops?**

### **Choices** :

- Recurrent Neural Networks
- Convolutional Neural Network
- Multilayer Perceptron
- Radial Basis Function Network

---
## Date - 2022-04-25


## Title - Harper and the small gradients


### **Question** :

Harper's team is struggling with the deep neural network they have been building.

Unfortunately, during backpropagation, the gradient values of their network decrease dramatically as the process gets closer to the initial layers, preventing them from learning at the same pace as the last set of layers.

Harper knows their model suffers from the vanishing gradient problem. She decides to research every possible option to improve their model.

**Which of the following techniques will make Harper's model more robust to the vanishing gradient problem?**


### **Choices** :

- Harper should try ReLU as the activation function since it's well-known for mitigating the vanishing gradient problem.
- Harper should modify the model architecture to introduce Batch Normalization.
- Harper should make sure they are initializing the weights properly. For example, using He initialization should help with the vanishing gradient problem.
- Harper should increase the learning rate to avoid getting stuck in local minima and thus reduce the chance of suffering vanishing gradients.

-----------------------

## Date - 2022-04-26


## Title - Exploring data before anything else


### **Question** :

An essential step in any machine learning project is the Exploratory Data Analysis process.

Before we can train a model, we need to understand our data. As the name suggests, Exploratory Data Analysis allows us to explore the data to discover potential problems or patterns that we can use.

**Which of the following are some of the steps we take during this process?**


### **Choices** :

- Learn the distribution of the target variable.
- Understand the features in the dataset and the distribution of their values.
- Evaluate the performance of our models on this data.
- Assess the data quality, including missing or corrupt values.

-----------------------

## Date - 2022-04-27


## Title - Susan needs to make a decision


### **Question** :

The deadline is approaching, and Susan still hasn't decided which version of her classification model to deploy to production.

She experimented with different hyperparameters, and now she has two models that perform pretty well. 

Her problem is that none of these models is better than the other in every situation. One model has a higher recall but worse precision than the other. Susan can improve the precision by playing with different thresholds, but now the recall decreases.

**How can Susan decide which is the best overall model?**


### **Choices** :

- Susan should tune the thresholds until both have a recall of 95% and choose the one with higher precision.
- Susan should tune the thresholds until both have a precision of 95% and choose the one with a higher recall.
- Susan should compute the area under the curve for both models and choose the one with the higher value.
- There's no objective way to decide which model is best. Susan should pick either one of them.

-----------------------

## Date - 2022-04-28


## Title - Linear regression by hand


### **Question** :

The best way to learn something new is to rip the band-aid and tackle a problem from scratch.

Imagine you get a dataset with thousands of samples of houses sold in the U.S. over the last five years. We know the value of a few different features of each home and the price it was sold for. The goal is to build a simple model capable of predicting the price of a new house given those features.

A linear regression model seems like an excellent place to start. 

But you are not writing any code yet. You want to do this manually, starting with a matrix `X` containing the value of the features and a vector `w` containing the weights.

The next step is to multiply `X` and `w`, but you aren't sure about the result of this operation.

**Which of the following better describes the result of multiplying `X` and `w`?**


### **Choices** :

- The result will be a vector `y` containing the actual price of each house as provided in the dataset.
- The result will be a vector `y` containing the predicted price of each house.
- The result will be a matrix `y` containing the actual price of each house and the features from the matrix `X`.
- The result will be a matrix `y` containing the predicted price of each house and the features from the matrix `X`.

-----------------------

## Date - 2022-04-29


## Title - 20,000 sunny and cloudy samples


### **Question** :

Today is your very first day.

You get access to weather data. Twenty thousand samples with the weather of sunny and cloudy days. You want to build a model to predict whether a future day will be sunny or cloudy.

You already know this is a binary classification problem, and now it's time to pick a model.

**Which of the following techniques can you use to build a binary classification model?**


### **Choices** :

- Logistic Regression
- k-Nearest Neighbors
- Neural Networks
- Decision Trees

-----------------------

## Date - 2022-04-30


## Title - The true meaning of hyperparameter tuning


### **Question** :

Marlene is trying to build an audience.

Writing content seems easy, but taking a complex subject and boiling it down to its essence is not an obvious task.

Marlene wants to start from the basics and write as much as possible about the fundamentals of machine learning.

She picked her first topic: hyperparameter tuning.

**If you were trying to summarize the core idea of hyperparameter tuning, which one of the following sentences would you use?**


### **Choices** :

- Hyperparameter tuning is about choosing the set of optimal features from the data to train a model.
- Hyperparameter tuning is about choosing the set of optimal samples from the data to train a model.
- Hyperparameter tuning is about choosing the optimal parameters for a learning algorithm to train a model.
- Hyperparameter tuning is about choosing the set of hypotheses that better fit the goal of the model.

-----------------------

## Date - 2022-05-01


## Title - One of these shouldn't be here


### **Question** :

Here are four different techniques commonly used in machine learning.

Although they are all related somehow, one of them is different from the rest. Your goal is to determine which of the following doesn't belong on this list.

**Can you select the odd one out?**


### **Choices** :

- Expectation–Maximization
- PCA
- DBSCAN
- K-Means

-----------------------

## Date - 2022-05-02


## Title - The bankruptcy story


### **Question** :

Suzanne wants to build an algorithm to predict whether a company is about to declare bankruptcy over the next few months.

She has access to a labeled dataset with detailed financial information from thousands of companies, including those that have declared bankruptcy over the last 100 years.

Suzanne has some ideas but would love to hear what you think.

**Understanding that there are many ways to approach a problem, what would be your first recommendation to Suzanne?**


### **Choices** :

- The best way to approach this problem is with Supervised Learning by using a regression algorithm.
- The best way to approach this problem is with Supervised Learning by using a classification algorithm.
- The best way to approach this problem is with Unsupervised Learning by using a clustering algorithm.
- The best way to approach this problem is with Reinforcement Learning.

-----------------------

## Date - 2022-05-03


## Title - A batch of rotated pictures


### **Question** :

After looking at the last batch of images, the problem was apparent:

Customers were taking pictures and sending them with different degrees of rotation. The Convolutional Neural Network that Jessica built wasn't ready to handle this.

She knew she needed to do something about it.

A couple of meetings later, Jessica knew what the right solution was. It took some time for the team to agree, but they had a plan now.

**Which of the following approaches could Jessica have proposed?**


### **Choices** :

- Extending the pipeline with a data preprocessing step to properly rotate every image coming from the customer before giving the data to the model.
- Extending the model with a layer capable of rotating the data to the correct position.
- Extending the training data with samples of images rotated across the full 360-degree spectrum to build some rotation invariability into the model.
- Configuring the network correctly since Convolutional Neural Networks are translation and rotation invariant and should handle these images correctly.

-----------------------

## Date - 2022-05-04


## Title - Alex's model is not doing well


### **Question** :

Alex is a Machine Learning Engineer working for a new photo-sharing startup.

His team started building a model to predict the likeability of every new image posted on the platform. They collected some data and built a simple classification model.

Unfortunately, Alex quickly realizes that the model doesn't perform well. He notices that the training error is not as low as expected.

**What do you think is happening with Alex's model?**


### **Choices** :

- It's very likely that the model suffers from high bias and is underfitting. This usually happens when the model is not complex enough and can't capture the relationship between input and output variables.
- It's very likely that the model suffers from low bias and is underfitting. This usually happens when the model is not complex enough and can't capture the relationship between input and output variables.
- It's very likely that the model suffers from high variance and is overfitting. This usually happens when the model is too complex and captures the noise of the data.
- It's very likely that the model suffers from low variance and is overfitting. This usually happens when the model is too complex and captures the noise of the data.

-----------------------

## Date - 2022-05-05


## Title - Behind Gradient Descent


### **Question** :

It's 2030, and neural networks are taught at high schools worldwide.

It makes sense. Few subjects are as impactful to society as machine learning, so it's only appropriate that schools get students onboard from a very early age.

Lillian spent a long time learning about gradient descent and how it's an optimization algorithm frequently used in machine learning applications.

This is Lillian's last exam. The first question asks her to describe in a few words how gradient descent works.

**Which of the following statements is a sensible description of how the algorithm works?**


### **Choices** :

- Gradient descent identifies the minimum loss and adjusts every parameter proportionally to this loss.
- Gradient descent searches every possible combination of parameters to find the optimal loss.
- Gradient descent identifies the slope in all directions and adjusts the parameters to move them in the direction of the negative slope.
- Gradient descent identifies the slope in all directions and adjusts the parameters to move them in the direction of the slope.

-----------------------

## Date - 2022-05-06


## Title - A recommendation for Adrienne


### **Question** :

Kaggle looked like the perfect opportunity for Adrienne to start practicing machine learning.

She went online and started listening to the conversations about popular Kagglers. One particular topic caught her attention: They kept discussing different ways to create ensembles.

Adrienne knew that ensemble learning is a powerful technique where you combine the decisions from multiple models to improve the overall performance. She had never used ensembles before, so she decided this was the place to start.

**Which of the following are valid ensemble techniques that Adrienne could study?**


### **Choices** :

- Max Voting: Multiple models make predictions for each sample. The final prediction is the one produced by the majority of the models.
- Weighted Voting: Multiple models make predictions for each sample, and each model is assigned a different weight. The final prediction considers the importance of the model in determining the final vote.
- Simple Averaging: Multiple models make predictions for each sample. The final prediction is the average of all of those predictions.
- Weighted Averaging: Multiple models make predictions for each sample, and each model is assigned a different weight. The final prediction is the average of all of those predictions, considering the importance of each model.

-----------------------

## Date - 2022-05-07


## Title - Sometimes, small is better


### **Question** :

Fynn is new to a team working on a neural network model. Unfortunately, they haven't been happy with the results so far.

Fynn thinks that he found the problem: they chose a batch size as large as it fits into the GPU memory. His colleagues believe this is the right approach, but Fynn believes a smaller batch size will be better.

**What would be good arguments to support Fynn's suspicion?**


### **Choices** :

- A smaller batch size is more computationally effective.
- A smaller batch size reduces overfitting because it increases the noise in the training process.
- A smaller batch size reduces overfitting because it decreases the noise in the training process.
- A smaller batch size can improve the generalization of the model.

-----------------------

## Date - 2022-05-08


## Title - Reese's baseline


### **Question** :

Starting with a simple baseline is a great way to approach a new problem.

Reese knew that, and her go-to has always been a simple Linear Regression, probably one of the most popular algorithms in statistics and machine learning.

But Reese knows that for Linear Regression to work, she must consider several assumptions about the problem.

**Which of the following are some of the assumptions that Reese should make for Linear Regression to be a good candidate for her baseline?**


### **Choices** :

- The relationship between the features in the data and the target variable must be linear.
- The features in the data are highly correlated between them.
- The features in the data and the target variable are not noisy.
- There must not be more than two relevant features plus the target variable.

-----------------------

## Date - 2022-05-09


## Title - Migrating to PyTorch Lighting


### **Question** :

Many of the old team members have left Layla's company, forcing them to start building a new team.

They have been hiring from local universities, and most new hires brought a lot of experience in PyTorch Lightning. Unfortunately for Layla's company, their main product uses TensorFlow.

After some discussions, Layla's team decided to migrate their model to PyTorch Lightning. This change, however, will not come without making some concessions. 

**Which of the following are some of the downsides of this decision?**


### **Choices** :

- The team will lose the ability to deploy the model in TPUs (Tensor Processing Units), limiting them to GPUs and CPUs.
- The team won't be able to use tools like TensorBoard during the training process, so they will need to find an equivalent tool compatible with PyTorch Lightning.
- The team will have to invest time to migrate the deployment process of their model from TensorFlow Serving to something like TorchServe or PyTorch Live.
- Migrating the existing codebase to PyTorch Lightning could introduce unforeseen problems that could cause issues with the new model.

-----------------------

## Date - 2022-05-10


## Title - The brains behind transformers


### **Question** :

It took some time, but Kinsley finished replacing her old model based on a [Long Short-Term Memory](https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/) (LSTM) network with a new version using Transformers.

The results of the new model were impressive. The whole team was thrilled with Kinsley's work, and the company organized an internal session for Kinsley to bring everyone up to speed.

After finishing her speech, a coworker asked Kinsley a question: 

**Which company invented the Transformer architecture?**


### **Choices** :

- Hugging Face
- OpenAI
- Google
- Allen Institute of AI


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In 2017, a team at [Google Brain](https://en.wikipedia.org/wiki/Google_Brain) published the now-famous paper ["Attention Is All You Need,"](https://arxiv.org/abs/1706.03762) where they introduced the Transformer architecture, which transforms one sequence into another with the help of an Encoder and a Decoder. 

[Hugging Face](https://huggingface.co) is an AI community that hosts many NLP models, including a large number of transformer models. Despite being a pioneer in adopting transformer models, Hugging Face is not behind the creation of Transformers.

OpenAI is another powerhouse that conducts AI research to promote and develop AI to benefit humanity. OpenAI is behind models like [GPT-3](https://en.wikipedia.org/wiki/GPT-3), [CLIP](https://openai.com/blog/clip/), and [DALL-E](https://openai.com/blog/dall-e/), all of which use the Transformer architecture. OpenAI, however, didn't invent Transformers.

Finally, the [Allen Institute of AI](https://allenai.org) (also known as AI2) is the AI research institute behind [Macaw](https://macaw.apps.allenai.org), a high-performance question-answering model capable of giving GPT-3 a run for its money. Despite their work with Transformers, they aren't behind its creation either.

In summary, the correct answer to this question is that Google is the inventor of Transformers.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [GPT-3](https://en.wikipedia.org/wiki/GPT-3)
* [CLIP](https://openai.com/blog/clip/)
* [DALL-E](https://openai.com/blog/dall-e/)
* [Macaw](https://macaw.apps.allenai.org)</p></details>

-----------------------

## Date - 2022-05-11


## Title - Trending recession


### **Question** :

The company's accounting team used a spreadsheet with some rudimentary charts, but it was time to get serious.

That's when Peyton came in.

Payton had a lot of experience doing time series analysis. Her mandate was simple: using the financial data of past years, predict where the company is going over the next few quarters.

Despite Payton's credentials, the team was worried: the company has been slowly recovering from a recession, and they were concerned this would skew future data. 

Payton went over the different components of time series analysis and explained how to classify this specific trend.

**Which of the following was Payton's explanation about this recession period?**


### **Choices** :

- The company's recession is part of a secular variation.
- The company's recession is part of a seasonal variation.
- The company's recession is part of a cyclical variation.
- The company's recession is part of an irregular variation.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>There are four components of a time series analysis: 
1. Secular trends—or simple trends.
2. Seasonal variations—or seasonal trends.
3. Cyclical fluctuations—or cyclical trends.
4. Irregular variations—or irregular trends.

A secular trend refers to the tendency of the series to increase, decrease, or stagnate over a long time. For example, a country's population could show an upward direction, while the number of death rates may show a downward trend. This trend is not seasonal or recurring.

On the other hand, a seasonal trend is a short-term fluctuation in a time series that occurs periodically. For example, sales during holidays trend much higher than during any other month, and the same happens every year.

A cyclical fluctuation is another variation that usually lasts for more than a year, and it's the effect of business cycles. Organizations go through these fluctuations in four different phases: prosperity, recession, depression, and recovery.

Finally, irregular variations are unpredictable fluctuations. We classify any variation that's not secular, seasonal, or cyclical as irregular. For example, we can't anticipate the effects of a hurricane on the economy.

The company here is dealing with a recession. Recessions are one of the phases of a cyclical fluctuation, so this was Payton's explanation. An important note is that cyclical variations do not have a fixed period—like seasonal variations do—but we can still predict them because we usually understand the sequence of changes that lead to these trends. 

In summary, the third choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Introduction to Time Series Analysis](https://www.jigsawacademy.com/introduction-time-series-analysis/)
* [Definition of Time Series Analysis](https://www.toppr.com/guides/fundamentals-of-business-mathematics-and-statistics/time-series-analysis/definition-of-time-series-analysis/)</p></details>

-----------------------

## Date - 2022-05-12


## Title - A way to win Kaggle competitions


### **Question** :

Victoria joined Kaggle, and halfway through her first competition, she realized her single model wouldn't perform very well on the leaderboard. 

She learned that most people were using multiple models working together. Looking into that extra boost from ensembles was the only way she would be able to increase her score.

Victoria spent a couple of days reading about stacking and blending, two of the most popular techniques for building ensemble models. Although she was clear about the high-level idea, she wasn't sure about some of the differences between both techniques.

**Which of the following are valid differences between stacking and blending?**


### **Choices** :

- The meta-model created using stacking learns how to combine the predictions from multiple models. In contrast, the meta-model created using blending uses the predictions of the best contributing model.
- Stacking doesn't need models of comparable predictive power. In contrast, blending works like a weighted average and requires models to contribute positively to the ensemble.
- The meta-model created using stacking is trained on out-of-fold predictions made during cross-validation. In contrast, a meta-model created using blending is trained on predictions made on a holdout set.
- Stacking works well for both classification and regression problems. In contrast, Blending only works for regression problems.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Victoria is right. Stacking and blending are powerful ensemble techniques and a must if you want to score high in Kaggle competitions.

Although both techniques have the same ultimate goal, there are essential differences in how they work. Let's start unraveling each of the available choices for this question to determine which ones are correct.

Stacking and blending use the concept of a "meta-model," a model that you train to average the results of other models. At a high level, both ensemble techniques use multiple models to generate predictions, and a meta-model to average those predictions and provide a final result.

The first choice argues that blending uses the predictions of the best contributing model, which is not true. Just like stacking, blending's meta-model uses the predictions of multiple models. Therefore, this is not a valid difference between both techniques.

An advantage of stacking is that it can benefit even from models that don't perform very well. In contrast, blending does require that models have a similar, good predictive power. Here is an excerpt from ["The Kaggle Book"](https://amzn.to/3kbanRb):

> (...) one interesting aspect of stacking is that you don't need models of comparable predictive power, as in averaging and often blending. In fact, even worse-performing models may be effective as part of a stacking ensemble. 

So even when using an individual model that performs poorly compared to all of the other models used by the stacking ensemble, the meta-model can use its out-of-fold predictions to improve its performance. Therefore, the second choice is a valid difference between both techniques.

Another valid difference between stacking and blending is the data they use to train the meta-model. The stacking meta-model is trained in the entire training set, using the [out-of-fold](https://machinelearningmastery.com/out-of-fold-predictions-in-machine-learning/) prediction strategy. The blending meta-model is trained in a holdout set that we randomly extract from the training dataset. Therefore, the third choice is also a valid difference between these techniques.

Finally, stacking and blending work well for regression and classification problems, so the final choice is incorrect.

In summary, the second and third choices are the correct answers to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Stacking and Blending — An Intuitive Explanation](https://medium.com/@stevenyu530_73989/stacking-and-blending-intuitive-explanation-of-advanced-ensemble-methods-46b295da413c)
* [Stacking Ensemble Machine Learning With Python](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/)
* [Blending Ensemble Machine Learning With Python](https://machinelearningmastery.com/blending-ensemble-machine-learning-with-python/)
* [How to Use Out-of-Fold Predictions in Machine Learning](https://machinelearningmastery.com/out-of-fold-predictions-in-machine-learning/)
* [The Kaggle Book](https://amzn.to/3kbanRb)</p></details>

-----------------------

## Date - 2022-05-13


## Title - Pick the one you don't like


### **Question** :

Let's get straight to the point.

Your goal is to determine which of the following doesn't belong on this list.

**Can you select the odd one out?**


### **Choices** :

- AdaGrad
- RMSProp
- Adam
- SGD


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Every example here is an optimization method used when training a machine learning model. 

However, [AdaGrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad), [RMSProp](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp), and [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) are adaptive learning rate methods, while [SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) is not.

[Adaptive learning rate methods](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1) track and update different learning rates for each model parameter, while SGD uses the same learning rate for all parameters.

The last choice is the correct answer.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- [Learning Rate Schedules and Adaptive Learning Rate Methods for Deep Learning](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1)
- [Stochastic gradient descent: Extensions and variants](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Extensions_and_variants)
- [How to Configure the Learning Rate When Training Deep Learning Neural Networks](https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/)
- [Deep Learning](https://amzn.to/3CSjPkR)</p></details>

-----------------------

## Date - 2022-05-14


## Title - Depth perception


### **Question** :

Richard finally got a job as a self-driving car engineer!

His first task is to help the car perceive depth using the onboard cameras. He wants to start with an overview of the different approaches he can use to estimate the distance to every pixel in the image.

Before diving into the existing techniques, Richard has to think about the different ways he can capture pictures.

**Which of the following mechanisms do you think Richard can use to estimate depth?**


### **Choices** :

- An image from a single camera.
- A sequence of images from a single camera.
- A pair of images from a stereo camera.
- Cameras are 2D sensors, so Richard can't use them to estimate depth.


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Cameras are indeed 2D sensors, but there are many ways to estimate distance using pictures from a camera, so the last choice is incorrect.

Using a stereo camera is one of the classical approaches to do this. For every point observed in both camera images, we can triangulate its 3D position. Therefore, the third choice is correct.

We can also use a sequence of images from a single camera to triangulate fixed points over different frames. This method is called [Structure from Motion](https://en.wikipedia.org/wiki/Structure_from_motion) and is also a correct choice. I'd recommend listening to [Andrej Karpathy's talk](https://youtu.be/Ucp0TTmvqOE?t=8479) covering the work they are doing at Tesla to estimate depth using video.

The most interesting correct choice is the first one. We don't have enough information to triangulate the distance to a point in the image, but we can use our knowledge of the world to make some assumptions and solve the problem.

Remember, we are only interested in a car driving on the street, so we can exploit our understanding of the scene and our knowledge of standard dimensions of objects to estimate the distance to each point on the image. Over the last few years, we have seen several methods to train deep neural networks using this approach. ["Single Image Depth Estimation: An Overview"](https://arxiv.org/abs/2104.06456) is a good paper covering this topic.

In summary, every choice but the last one is correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- [Stereo Vision](https://en.wikipedia.org/wiki/Computer_stereo_vision)
- [Structure from Motion](https://en.wikipedia.org/wiki/Structure_from_motion)
- [Monocular depth estimation](https://paperswithcode.com/task/monocular-depth-estimation)
- [Depth from vision by Andrej Karpathy](https://youtu.be/Ucp0TTmvqOE?t=8479)
- [Single Image Depth Estimation: An Overview](https://arxiv.org/abs/2104.06456)
- [Multiple View Geometry](https://amzn.to/3KNPhmN)</p></details>

-----------------------

## Date - 2022-05-15


## Title - The benefits of the Huber loss


### **Question** :

The Huber loss is a popular loss function used for regression problems in machine learning.

[Here is the formula](https://en.wikipedia.org/wiki/Huber_loss). Take a second and look at it. 

The formula may look complex, but there are two things you need to know about the Huber loss. 

First, it behaves like a square function for values smaller than a parameter δ (similar to MSE.) Second, it acts as the absolute function for larger values (similar to MAE.)

In essence, the Huber loss is a combination of two other popular loss functions: Mean Squared Error (MSE) and Mean Absolute Error (MAE.)

**What are the benefits of combining these two functions?**


### **Choices** :

- It adds an additional hyperparameter δ which helps tune the model.
- It is more robust against large outliers than MSE.
- It is smooth around 0 helping the training converge better.
- It is continuous and differentiable.


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The [Huber loss](https://en.wikipedia.org/wiki/Huber_loss) tries to combine the advantages of both MSE and MAE. Here is a picture showing a comparison between these three functions (Image credit to ["Regression loss functions for machine learning"](https://www.evergreeninnovations.co/blog-machine-learning-loss-functions/)):

![image](https://user-images.githubusercontent.com/1126730/167011014-92c64b36-689e-4a89-bc6e-1e963807a982.png)

If we want to have a loss function that is not affected by outliers, we typically use MAE instead of MSE. When using MAE, we don't square the errors as we do with MSE, so outliers aren't amplified. However, MAE has the problem that it is not smooth around 0 (the derivative jumps a lot at 0,) which may cause issues with convergence. 

The Huber loss behaves like MAE for large values, so it's robust against outliers, but it acts like MSE around 0, so it is smooth. In a way, we get our cake and eat it too with the Huber loss! Therefore, the second and third options are correct.

An important goal for the Huber loss was to make it continuous and differentiable. This makes the fourth choice correct as well.

Finally, the Huber loss comes with an additional hyperparameter δ. That extra parameter means that the training process will be harder to tune. Although the parameter is essential in the design of the Huber loss, it's not an advantage compared to a loss function that doesn't require tuning. Therefore, the first choice is incorrect.

In summary, the second, third, and fourth choices are correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- [Huber loss](https://en.wikipedia.org/wiki/Huber_loss)
- [Huber Loss: Why Is It, Like How It Is?](https://www.cantorsparadise.com/huber-loss-why-is-it-like-how-it-is-dcbe47936473)
- [Regression loss functions for machine learning](https://www.evergreeninnovations.co/blog-machine-learning-loss-functions/)</p></details>

-----------------------

## Date - 2022-05-16


## Title - Climbing a hill


### **Question** :

Gabriela wanted her friend to grow an appreciation for the outdoors, so they started meeting every Saturday and going for a hike together.

And what better way to spend their time than starting a discussion about hill climbing and how it relates to their day-to-day work.

It turns out that hill climbing is an optimization algorithm that attempts to find a better solution by making incremental changes until it doesn't see further improvements.

Her friend couldn't help but notice how similar to gradient descent the process was, but Gabriela knew there were a few critical differences between them. 

**Can you select every correct statement from the following comparison list?**


### **Choices** :

- Hill climbing is a general optimization algorithm, but gradient descent is only used to optimize neural networks.
- Unlike gradient descent, hill climbing can return an optimal solution even if it's interrupted at any time before it ends.
- Gradient descent is usually more efficient than hill climbing, but there are fewer problems we can tackle with gradient descent.
- Both hill climbing and gradient descent can find optimal solutions for convex problems.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Hill climbing](https://en.wikipedia.org/wiki/Hill_climbing) is an optimization algorithm, just like [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) is. You can use both to minimize a function, regardless of whether it's related to a neural network, so the first choice is incorrect. For example, the following animation comes from ["Linear Regression using Gradient Descent."](https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931) It shows a linear regression model that uses gradient descent as the optimization algorithm:

![Linear regression using gradient descent](https://miro.medium.com/max/1400/1*CjTBNFUEI_IokEOXJ00zKw.gif)

The second choice is also incorrect. Hill climbing can return a valid solution even if it's interrupted before it ends, but there's no guarantee that this will be the optimal solution. We call these types of algorithms ["anytime algorithms."](https://en.wikipedia.org/wiki/Anytime_algorithm) They find better and better solutions the longer they keep running but can return a reasonable solution at any time.

Gradient descent looks at the slope in the local neighborhood and moves in the direction of the steepest slope. This makes it much more efficient than hill climbing, which needs to look at all neighboring states to evaluate the cost function in each of them. 

The efficiency gained by gradient descent presents a trade-off: the algorithm assumes that you can compute the function's gradient in any given state, limiting the problems where we can use it. Therefore, the third choice is correct.

Finally, the fourth choice is also a correct answer. Both algorithms can find the optimal solution for a convex problem. Look at the following example of a [convex function](https://en.wikipedia.org/wiki/Convex_function). Assuming we configure hill climbing to optimize for finding the minimum, neither function should have trouble getting all the way to the bottom of this problem:

![A convex function](https://user-images.githubusercontent.com/1126730/167182794-30f47b44-2149-4700-b642-616d8d6dce51.png)

In summary, the third and fourth choices are the correct answers to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Hill Climbing Algorithms (and gradient descent variants) IRL](https://umu.to/blog/2018/06/29/hill-climbing-irl)
* [Linear Regression using Gradient Descent](https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931)
* [Hill climbing](https://en.wikipedia.org/wiki/Hill_climbing)
* [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)</p></details>

-----------------------

## Date - 2022-05-17


## Title - Which function is she using?


### **Question** :

Kiara was leaving her team, but she didn't want to go without having some fun.

She put together a simple neural network with one hidden layer. Never trained it, but she initialized its parameters and told her team that they should expect every node from the hidden layer to return a value resembling the following formula:

```
y = max(0.01 * x, 0)
```

Kiara saved the model and asked her team to test the node results without looking at the code. They found out that, in effect, the results always followed the formula mentioned by Kiara.

Kiara's question to her team was simple:

**Which of the following activation functions am I using in this network?**


### **Choices** :

- Kiara is using the sigmoid activation function.
- Kiara is using the Rectified Linear Unit activation function.
- Kiara is using the Leaky Rectified Linear Unit activation function.
- None of the above activation functions can produce this output.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>This is a fun, interesting question and one where we need to be very careful to find the correct answer.

The team doesn't have access to the network architecture, so all they know is that node outputs follow a specific pattern. They also know Kiara is using an activation function. If we use σ to represent this activation function, the result of each node should look like this:

```
z = σ(y)
```

Here, `z` is the output the team is getting out of the node, and `y` is the input to the activation function. This input results from `y = w * x + b`, where `b` is the bias, and `w` is the weight assigned to that node. Putting everything together:

```
z = σ(w * x + b)
```

Let's start with [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function), which doesn't use the `max` operation, so we can safely discard it.

Here is the formula of the [Leaky Rectified Linear Unit](https://paperswithcode.com/method/leaky-relu) (Leaky ReLU):

```
y = max(0.01 * x, x)
```

This one looks promising, and it's how Kiara wanted to prank her team. Assuming that `x` results from `w * x + b`, Leaky ReLU would almost make sense, except it returns the maximum between a scaled version of `x` and `x`, while the team is seeing something different. Leaky ReLU can't possibly be the answer.

Here is the formula of the [Rectified Linear Unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) (ReLU):

```
y = max(x, 0)
```

It looks similar, but where is the scaling factor? Well, Kiara initialized the network, so there's a good chance she did it in a way to confuse everyone. If Kiara set every weight `w` to be `0.01` and every bias term to be zero, we would get the following:

```
z = σ(w * x + b)
z = σ(0.01 * x + 0)
```

Assuming that σ is the ReLU activation function, we will get the following:

```
z = max(0.01 * x + 0, 0)
z = max(0.01 * x, 0)
```

This is the pattern the team is seeing. Kiara used ReLU as her activation function.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [A Gentle Introduction to the Rectified Linear Unit (ReLU)](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)
* [Rectifier (neural networks)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))
* [Leaky ReLU](https://paperswithcode.com/method/leaky-relu)
* [Activation Functions](https://himanshuxd.medium.com/activation-functions-sigmoid-relu-leaky-relu-and-softmax-basics-for-neural-networks-and-deep-8d9c70eed91e)</p></details>

-----------------------

## Date - 2022-05-18


## Title - Riley's speed-dating match


### **Question** :

If you spend all day sitting at a desk, you can't expect to have many opportunities to meet interesting people.

Riley decided to get to bull by the horns and checked in on one of those speed-dating sites that promise to find your perfect match.

But of course, Silicon Valley is a ridiculous caricature of the impossible, and Riley's first match decided to start blabbing about machine learning and dimensionality reduction algorithms.

And if this wasn't crazy enough, Riley didn't think this person knew what he was talking about.

**Can you guess all the possible statements about dimensionality reduction that would make Riley's match incorrect?**


### **Choices** :

- Supervised learning algorithms can be used as dimensionality reduction techniques.
- Every dimensionality reduction technique is a clustering technique, but every clustering technique is not a dimensionality reduction algorithm.
- Dimensionality reduction algorithms are primarily considered unsupervised learning techniques.
- Nowadays, the most successful dimensionality reduction techniques are deep learning algorithms.


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Here is something clear to Riley: Dimensionality reduction algorithms reduce the number of input variables in a dataset to find a lower-dimensional representation that still preserves the salient relationships in the data.

For example, PCA—short for [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)—is a dimensionality reduction algorithm often used to reduce the number of variables in a dataset while preserving as much information as possible. Another dimensionality reduction technique is [Independent Component Analysis](https://en.wikipedia.org/wiki/Independent_component_analysis) (ICA).

Everywhere you go, dimensionality reduction algorithms are classified as unsupervised learning techniques. Even auto-encoders that require training a neural network are not considered a supervised technique, as mentioned in ["Machine Learning: A Probabilistic Perspective"](https://amzn.to/3s39PRD):

> An auto-encoder is a kind of unsupervised neural network that is used for dimensionality reduction and feature discovery. More precisely, an auto-encoder is a feedforward neural network that is trained to predict the input itself.

This doesn't mean that you can't use a supervised learning method to reduce the dimensionality of a dataset. For example, here is an excerpt from ["Seven Techniques for Data Dimensionality Reduction"](https://www.knime.com/blog/seven-techniques-for-data-dimensionality-reduction):

> Decision Tree Ensembles, also referred to as random forests, are useful for feature selection in addition to being effective classifiers. One approach to dimensionality reduction is to generate a large and carefully constructed set of trees against a target attribute and then use each attribute's usage statistics to find the most informative subset of features.

At this point, we know that the first and the third choices are correct statements about dimensionality reduction. But what about the other two options?

The second choice is incorrect because every dimensionality reduction technique is not a clustering technique. For example, neither PCA nor ICA are clustering methods.

The fourth option is also incorrect because it's not true that the most successful dimensionality reduction techniques are limited to deep learning algorithms. For example, PCA is one of the most popular dimensionality reduction techniques and has nothing to do with deep learning.

If Riley's match was incorrect, he must have mentioned the second or fourth statements, so they are the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Introduction to Dimensionality Reduction for Machine Learning](https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/)
* [Machine Learning: A Probabilistic Perspective](https://amzn.to/3s39PRD)
* [Seven Techniques for Data Dimensionality Reduction](https://www.knime.com/blog/seven-techniques-for-data-dimensionality-reduction)
* [A Gentle Introduction to LSTM Autoencoders](https://machinelearningmastery.com/lstm-autoencoders/)</p></details>

-----------------------

## Date - 2022-05-19


## Title - Occam's Razor showoff


### **Question** :

Tiara's manager was a showoff. No matter the situation, he always found a way to show everyone how smart he was.

Tiara noticed that he's been getting into machine learning lately, and as cringe as it sounds, he has been using "Occam's Razor" on every occasion, even incorrectly.

Tiara started a secret list collecting every scenario when her manager used Occam's Razor to explain a situation. At the end of the week, she sent it to many of her friends to have a good laugh.

**Which of the following situations from Tiara's list are you comfortable justifying with Occam's Razor?**


### **Choices** :

- We should prefer simpler models with fewer coefficients over complex models like ensembles.
- Feature selection and dimensionality reduction help simplify models to get better results.
- Keeping the training process as fast as possible avoids overtraining and prevents overcomplicated results.
- Starting the training of the model using sensible values for the hyperparameters.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Occam's Razor](https://en.wikipedia.org/wiki/Occam%27s_razor) is a principle that says that if you have two competing ideas to explain the same phenomenon, you should prefer the simpler one.

There are a couple of situations in this list where using Occam's Razor is a stretch. The third choice is probably the simplest one to tackle first: it talks about "the speed of the training process" and relates it to overtraining and overcomplicating results. Not only does this has nothing to do with Occam's Razor, but a quick training process doesn't necessarily reduce complexity. 

The fourth choice is also not correct. Starting training using sensible values for the hyperparameters is essential, but we can't explain this using Occam's Razor.

Occam's Razor fits the first choice like a glove. Given two learning algorithms with similar tradeoffs, we should use the least complex and most straightforward to interpret. At least this time, Tiara's boss was correct.

Finally, the second choice is not an obvious fit, but we could argue it's also a correct answer. Feature selection and dimensionality reduction simplify the data we use to train our models. We use these steps to remove redundant or irrelevant information, therefore getting a simpler dataset that should perform better than a more complex one.

In summary, Tiara's manager was correct on the first two but was incorrect on the last two.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Ensemble Learning Algorithm Complexity and Occam’s Razor](https://machinelearningmastery.com/ensemble-learning-and-occams-razor/)
* [How does Occam's razor apply to machine learning?](https://www.techopedia.com/how-does-occams-razor-apply-to-machine-learning/7/33087)
* The [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor) definition in Wikipedia.</p></details>

-----------------------

## Date - 2022-05-20


## Title - Emma's list


### **Question** :

Nothing is perfect.

And no matter how much they said otherwise, Emma knew that gradient descent was no exception.

They have been discussing some of the most popular optimization algorithms for neural networks, and the team didn't want to listen despite Emma's comments regarding some of the downsides of gradient descent.

Emma decided to post a detailed list of problems on the company's Slack channel.

**Which of the following practical issues of gradient descent deserve to be on Emma's list?**


### **Choices** :

- Gradient descent can take a long time to converge to a local minimum.
- There's no guarantee that gradient descent will converge to the global minima.
- Gradient descent is susceptible to the initialization of the network's weights.
- Gradient descent is not capable of optimizing continuous functions.


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Gradient descent is one of the most popular optimization algorithms used in machine learning applications. But, despite its popularity, there are several practical issues that Emma wanted to mention.

The first issue is how gradient descent updates the model parameters after calculating the derivatives for all the observations. When working with large datasets, finding a local minimum may take a long time because the algorithm needs to compute many gradients before making a single update. [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD), a variation of gradient descent, works differently and updates the model parameters for each observation speeding up the process. Since the team is focusing on gradient descent, the first choice made it to Emma's list.

The second choice is also on the list. Assuming the are multiple local minima in a problem, there is no guarantee that gradient descent will find the global minimum. Here is an excerpt from ["Gradient Descent,"](http://www.cs.umd.edu/~djacobs/CMSC426/GradientDescent.pdf) a publication from the Computer Science Department of the University of Maryland:

> When a problem is nonconvex, it can have many local minima. And depending on where we initialize gradient descent, we might wind up in any of those local minima since they are all fixed points.

But it doesn't end there. As the previous quote mentions, gradient descent is also susceptible to the initialization of the network's weights. Assuming there are multiple local minima, the initialization of the network weights will play a fundamental role in whether the algorithm finds the global minimum: it may converge to a less optimal solution if we initialize the network too far from the global minimum. Therefore, the third choice is also a correct answer.

Finally, gradient descent can optimize a continuous function with no issues, so the fourth choice is not a correct answer.

In summary, Emma included the first three choices on her list.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Gradient Descent](http://www.cs.umd.edu/~djacobs/CMSC426/GradientDescent.pdf) is a deep dive into gradient descent and its variants from the Computer Science Department of the University of Maryland.
* [Problems with Gradient Descent](https://www.encora.com/insights/problems-with-gradient-descent)
* [Gradient Descent For Machine Learning](https://machinelearningmastery.com/gradient-descent-for-machine-learning/)</p></details>

-----------------------

## Date - 2022-05-21


## Title - Zoe's looking into KNN


### **Question** :

It was the first time Zoe dealt with k-Nearest Neighbors (KNN). She inherited the code, and now she was responsible for making it work.

Before touching the code, she decided to do some research. Her first stop was on one of the fundamental topics in machine learning: bias, variance, and their relationship with the algorithm.

She knows there's always a tradeoff between these two.

**Which of the following statements are correct concerning the bias and variance tradeoff of KNN?**


### **Choices** :

- Zoe can increase the bias of KNN by using a larger value of `k`.
- Zoe can increase the variance of KNN by using a larger value of `k`.
- Zoe can decrease the bias of KNN by using a smaller value of `k`.
- Zoe can decrease the variance of KNN by using a smaller value of `k`.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) is an algorithm with low bias and high variance. 

Let's imagine Zoe decides to use a small value of `k`, for example, `k=1`. In this case, the algorithm will likely predict the training dataset perfectly. The smaller the value of `k`, the less bias and larger variance KNN will show. This, of course, is not a great outcome because the model will overfit and have difficulty predicting unseen data.

Now let's assume Zoe decides to use a very large value of `k`; for example, set `k` to the number of samples on her training dataset. This will increase the algorithm's bias and reduce its variance, resulting in an underfit model that can't adequately capture the variance in the training dataset. Here is a quote from ["Why Does Increasing k Decrease Variance in kNN?"](https://towardsdatascience.com/why-does-increasing-k-decrease-variance-in-knn-9ed6de2f5061):

> If we take the limit as k approaches the size of the dataset, we will get a model that just predicts the class that appears more frequently in the dataset [...]. This is the model with the highest bias, but the variance is 0 [...]. High bias because it has failed to capture any local information about the model, but 0 variance because it predicts the exact same thing for any new data point.

As Zoe suspects, neither case will lead to a proper solution. She needs to find the appropriate tradeoff between the bias and variance of the algorithm.

In summary, the smaller the value of `k` is, the lower the bias and higher the variance. The larger the value of `k` is, the higher the bias and lower the variance. This means that the first and third choices are correct: we can control the algorithm's bias as explained in these two choices.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Why Does Increasing k Decrease Variance in kNN?"](https://towardsdatascience.com/why-does-increasing-k-decrease-variance-in-knn-9ed6de2f5061) is a really good article diving into the relationship of `k` and the variance of KNN.
* For a more general introduction to the bias-variance trade-off, check ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/).
* In case you prefer Twitter threads with a summary of how this works, check out ["Bias, variance, and their relationship with machine learning algorithms."](https://twitter.com/svpino/status/1506964069646884864)</p></details>

-----------------------

## Date - 2022-05-22


## Title - The 3-sigma accuracy


### **Question** :

Clara and her team are working on a drone localization project.

They have developed a neural network model that uses drone cameras to determine its position in the world so the drone can come back and land at the same spot it took off.

Clara was discussing the latest evaluation results with her colleagues when Jan mentioned that their latest model reached a 3-sigma accuracy of 20cm. 

Clara is new to the industry, and _"3-sigma accuracy of 20cm"_ didn't make much sense.

**What does a "3-sigma accuracy of 20cm" mean in this context?**


### **Choices** :

- In 66.6% of the cases, the model's error is less than 20cm
- In 68.2% of the cases, the model's error is less than 20cm
- In 95.4% of the cases, the model's error is less than 20cm
- In 99.7% of the cases, the model's error is less than 20cm


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The 3-sigma accuracy is a common way to quantify the accuracy of a model when estimating a continuous variable. Here is a quote from [Wikipedia's explanation of the 68 - 95 - 99.7 rule](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule):

> In the empirical sciences, the so-called three-sigma rule of thumb (or 3σ rule) expresses a conventional heuristic that nearly all values are taken to lie within three standard deviations of the mean, and thus it is empirically useful to treat 99.7% probability as near certainty.

We can often assume that the error of estimating a continuous variable (such as the drone's position) follows the normal distribution. If we denote the standard deviation of the normal distribution as σ (sigma), then 68.2% of the samples should fall in the region from -1σ to 1σ around the mean.

![Standard deviation](https://user-images.githubusercontent.com/1126730/169593614-ee0ecdf7-a262-41b3-943f-5ae6865afcc8.png)

If we take a larger range from -2σ to 2σ, then 95.4% of a normally distributed dataset will fall in this interval. Finally, a 3σ interval will cover 99.73% of the samples.

Therefore, when we talk about a 3-sigma accuracy of 20cm, we mean that 99.73% of the model predictions are more accurate than 20cm (because they fall in the -3σ to 3σ interval). Thus, the correct answer is the fourth choice.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution#Standard_deviation_and_coverage)
- [68–95–99.7 rule](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule)</p></details>

-----------------------

## Date - 2022-05-23


## Title - Scheduled learning


### **Question** :

A company—a bad one, because there are plenty of those out there—has been experiencing some turnaround, and they wanted to ensure the new team members were up to speed with the neural network model they were using in production.

The team has been looking at the code and writing notes every time they find something new.

They stumbled upon the training scripts and noticed the last team used a learning rate scheduler.

**Which of the following statements could explain why the last team used this scheduler? Select all that apply.**


### **Choices** :

- The last team used the learning rate scheduler to increase the learning rate as training progressed.
- The last team used the learning rate scheduler to decrease the learning rate as training progressed.
- The last team used the learning rate scheduler to give the network a better chance to converge.
- The last team used the learning rate scheduler to save the learning rate at specific intervals during the training process.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When training a neural network, setting the hyperparameters of the optimizer is essential for getting good results. One of the most critical parameters is the [learning rate](https://en.wikipedia.org/wiki/Learning_rate). Setting the learning rate too high or too low will cause problems during training.

A simple way to think about the learning rate is as follows: if we set it too low, the training process will be very slow; it will take a long time for the network to converge. Conversely, if we use a learning rate that's too high, the process will oscillate around the minimum without converging. Here is a chart from ["Deep Learning Wizard"](https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/lr_scheduling/) illustrating the effect of different learning rates:

![Differences in learning rates](https://user-images.githubusercontent.com/1126730/167927199-f6a2add7-91be-4bc6-8459-bf00ff0ea4b6.png)

A popular technique to find a good balance is to use a learning rate scheduler. This predefined schedule adjusts the learning rate between epochs or iterations as the training progresses.

The most common scenario is to start with a high learning rate and decrease it over time. In the beginning, we take significant steps towards the minimum but move more carefully as we hone in on it. 

Looking at the available choices, we can see the first choice is incorrect, but the second and third choices are correct. The team was likely trying to decrease the learning rate as training progressed. Although there are experiments showing the use of [cyclical learning rates](https://arxiv.org/abs/1506.01186), the most common practice when using a scheduler is to start with a high learning rate and reduce it over time.

Finally, the fourth choice is incorrect as well. A learning rate scheduler has nothing to do with saving the learning rate. 

In summary, the second and third choices are the correct answers to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Learning Rate Scheduling"](https://d2l.ai/chapter_optimization/lr-scheduler.html) is a great introduction to learning rate schedulers.
- ["How to Choose a Learning Rate Scheduler for Neural Networks?"](https://neptune.ai/blog/how-to-choose-a-learning-rate-scheduler) is an article from [Neptune AI](https://neptune.ai/), focusing on some practical ideas on how to use schedulers.
- ["Cyclical Learning Rates for Training Neural Networks"](https://arxiv.org/abs/1506.01186) is the paper discussing a technique to let the learning rate cyclically vary between reasonable boundary values.</p></details>

-----------------------

## Date - 2022-05-24


## Title - Balancing bias and variance


### **Question** :

The very first chapter of her machine learning book was about bias, variance, and their tradeoff. 

Callie knew that she had no alternative: she had to spend the time trying to understand these concepts before moving on.

But at the end of the day, it wasn't easy to remember the nuances of each concept, so Callie decided to get help.

**Which of the following descriptions of bias and variance are correct?**


### **Choices** :

- Bias refers to the assumptions a model makes to simplify the process of finding answers. The more assumptions it makes, the more biased the model is.
- Variance refers to the assumptions a model makes to simplify finding answers. The more assumptions it makes, the more variance in the model.
- Bias refers to how much the answers given by the model will change if we use different training data. The model has low bias if the answers stay the same regardless of the data.
- Variance refers to how much the answers given by the model will change if we use different training data. The model has low variance if the answers stay the same regardless of the data.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Every machine learning algorithm deals with three types of errors: 
* Bias error 
* Variance error 
* Irreducible error 

To answer this question, let's forget about the irreducible error and focus on the other two.

Here is what [Jason Brownlee](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) has to say about _bias_:
> Bias are the simplifying assumptions made by a model to make the target function easier to learn.

Think about a simple linear model. It assumes that the target function is linear, so the model will try to fit a line through the data regardless of its appearance. This assumption helps the model simplify the process of finding the answer, and the more assumption it makes, the more biased the model is. Often, linear models are high-bias, and nonlinear models are low-bias.

Here is a [funny depiction of biases](https://xkcd.com/2618/): the speaker believes everyone must understand selection bias. Whenever we put people in buckets to characterize or predict how they act, we use our biases to simplify our understanding of the world.

![An example of selection bias](https://user-images.githubusercontent.com/1126730/168139417-8e5d8ce5-929e-4f8f-96a1-80232d61c73e.png)

Regarding variance, [Jason](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) continues:
> Variance is the amount that the estimate of the target function will change if different training data is used.

Variance refers to how much the answers given by the model will change if we use different training data. The model has low variance if the answers stay the same regardless of the data. 

Think about a fickle person that constantly changes their mind with the news. Every new article makes the person believe something completely different. I don't want to overextend the analogy, but this is an example of high variance. Often, linear models are low-variance, and nonlinear models are high-variance.

If we consider all of this, the first and fourth choices are the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Here is Jason Brownlee's article I mentioned before: ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/).
* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).
* In case you like the simplicity of Twitter threads, here is one for you about this topic: ["Bias, variance, and their relationship with machine learning algorithms"](https://twitter.com/svpino/status/1390969728504565761).</p></details>

-----------------------

## Date - 2022-05-25


## Title - Cutting down features


### **Question** :

When Nicole finished collecting the data, she realized that there were just too many features.

She was staring at hundreds of potential variables, and it was evident that any model would have a hard time navigating them. Nicole knew that she had to reduce the dimensionality of her dataset.

Dimensionality reduction algorithms reduce the number of input variables in a dataset to find a lower-dimensional representation that still preserves the salient relationships in the data.

**Which of the following are dimensionality reduction techniques that Nicole could use?**


### **Choices** :

- Singular Value Decomposition
- Principal Component Analysis
- Linear Discriminant Analysis
- Isomap Embedding


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Every one of these is a valid dimensionality reduction technique.

The problem doesn't specify the type of data that Nicole is using, so it's hard to determine which of these techniques will be most effective, but every one of them could be potentially valuable.

Therefore, all choices are correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)
* [Understanding Dimension Reduction with Principal Component Analysis (PCA)](https://blog.paperspace.com/dimension-reduction-with-principal-component-analysis/)
* [Linear Discriminant Analysis – Bit by Bit](https://sebastianraschka.com/Articles/2014_python_lda.html)
* [Dimension Reduction - IsoMap](https://blog.paperspace.com/dimension-reduction-with-isomap/)</p></details>

-----------------------

## Date - 2022-05-26


## Title - But why deep learning?


### **Question** :

Martin met an old friend for a coffee.

They discussed Martin's latest work on image classification using deep learning. While Martin's friend used to work in computer vision 12 years ago, he is not aware of any of the latest developments.

He asks Martin why deep learning is so successful in image classification compared to the classical computer vision methods. He's heard that deep learning is better but doesn't understand why.

**If Martin wanted to summarize his reasoning with one sentence, which of the following is the best way to explain why deep learning is better for computer vision tasks?**


### **Choices** :

- Deep neural networks have many fully connected layers making the model more powerful.
- Deep neural networks can learn the best features for the task, while traditional methods rely on engineered features.
- Deep learning methods can use much larger datasets and achieve better performance.
- Deep learning algorithms can take advantage of GPUs, and we can therefore train much larger and more powerful models.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>These options explain why deep learning methods can achieve good results on computer vision tasks, but not all explain why deep learning is better than classical computer vision methods.

Having more data and algorithms optimized for GPUs can improve the results of a model. However, this is not exclusive to deep learning methods. Many traditional machine learning models can handle large datasets and take advantage of GPUs. Therefore, neither the third nor fourth choices correctly explain why deep learning is better for computer vision tasks.

The first choice is also incorrect. Although deep learning models can use many fully connected layers, the main benefits when solving computer vision problems come from using specialized layers—like convolutional layers—and not fully connected ones.

Finally, traditional computer vision methods typically rely on pre-computed features. Contrast this with [Convolutional Neural Networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) and [Vision Transformers](https://en.wikipedia.org/wiki/Vision_transformer), which can learn features directly from the dataset and don't need pre-computed features to provide good results.

The ability to learn powerful features is one of the main reasons for the superior performance of deep learning methods in computer vision, making the second choice the best explanation for Martin's friend.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Here is an introduction to ["Convolutional Neural Networks"](https://en.wikipedia.org/wiki/Convolutional_neural_network).
- And this is the introduction to ["Vision Transformers"](https://en.wikipedia.org/wiki/Vision_transformer).
- ["Learned Features"](https://christophm.github.io/interpretable-ml-book/cnn-features.html) is an excellent explanation covering the features that we can learn using convolutional layers.
- Check out ["OpenAI Microscope"](https://openai.com/blog/microscope/) for a fascinating look at the visual features inside a neural network.</p></details>

-----------------------

## Date - 2022-05-27


## Title - Choosing a loss function


### **Question** :

Ariana and Zach need to compute how different their model predictions are from the expected results.

They have been going back and forth between two different loss functions: Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). These two metrics have properties that will shine depending on the problem they want to solve.

**Which of the following is the correct way to think about these two metrics?**


### **Choices** :

- RMSE penalizes larger differences between the predictions and the expected results.
- RMSE is significantly faster to compute than MAE.
- From both metrics, RMSE is the only one indifferent to the direction of the error.
- From both metrics, MAE is the only one indifferent to the direction of the error.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When we train a machine learning model, we need to compute how different our predictions are from the expected results. For example, if we predict a house's price as `$150,000`, but the correct answer is `$200,000`, our "error" is `$50,000`.

There are multiple ways we can compute this error, but two common choices are:
* RMSE — [Root Mean Squared Error](https://en.wikipedia.org/wiki/Root-mean-square_deviation)
* MAE — [Mean Absolute Error](https://en.wikipedia.org/wiki/Mean_absolute_error)

These have different properties that will shine depending on the problem we want to solve. Remember that the optimizer will use this error to adjust the model. We want to set up the right incentives so the model learns appropriately.

Let's focus on a critical difference between these two metrics. Remember the "squared" portion of the RMSE? You are "squaring" the difference between the prediction and the expected value. Why is this relevant?

Squaring the difference "penalizes" larger values. If you expect a prediction to be 2, but you get 10, using RMSE, the error will be (2 - 10)² = 64. However, if you get 5, the error will be (2 - 5)² = 9. Do you see how it penalizes larger errors?

MAE doesn't have the same property. The error increases proportionally with the difference between predictions and target values. Understanding this is important to decide which metric is better for each case. 

Predicting a house's price is a good example where `$10,000` off is twice as bad as `$5,000`. We don't necessarily need to rely on RMSE here, and MAE may be all we need. 

But predicting the pressure of a tank may work differently. While 5 psi off may be within the expected range, 10 psi off may be a complete disaster. Here "10" is much worse than just two times "5", so RMSE may be a better solution.

Looking at the first choice, we already know it is a correct answer. RMSE penalizes larger differences between predictions and expected results.

Looking at both formulas, RMSE has extra squaring and root squaring operations, so it can't be faster to compute than MAE. The second choice is, therefore, not correct.

The third choice states that RSME is indifferent to the direction of the error, but MAE isn't. This is not correct: MAE uses the absolute value of the error, so both negative and positive values will end up being the same.

The fourth choice states that MAE is indifferent to the direction of the error, but RMSE isn't. This is not correct either: RMSE squares the error, so both negative and positive values will be the same.

In summary, the only correct answer to this question is the first choice.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["RMSE vs MAE, which should I use?"](https://stephenallwright.com/rmse-vs-mae/) this is a great summary by Stephen Allwright about the properties of these two functions and how you should think about them.
* ["Root-mean-square deviation"](https://en.wikipedia.org/wiki/Root-mean-square_deviation) is the Wikipedia page covering RMSE.
* ["Mean absolute error"](https://en.wikipedia.org/wiki/Mean_absolute_error) is the Wikipedia page covering MAE.</p></details>

-----------------------

## Date - 2022-05-28


## Title - Rolling down the hill


### **Question** :

Brooklyn was dealing with a complex problem. Although gradient descent was working relatively well, she read that adding momentum could benefit her use case.

Brooklyn needed to justify spending more time on this problem, so she wrote an email summarizing her reasoning behind using momentum, hit send, and patiently waited for her manager to respond.

**Which of the following statements are some of the reasons that Brooklyn included in her email?**


### **Choices** :

- Momentum helps when there's a lot of variance in the gradients.
- Momentum helps overcome local minima.
- Momentum helps the training process converge faster.
- Momentum helps when there aren't flat regions in the search space.


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>There's a problem with [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent): it depends entirely on the gradients it computes along the way, so whenever there's a lot of variance in these gradients, the algorithm can bounce around the search space making the optimization process slower.

Adding [momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) to gradient descent will help overcome this problem. Here is Jason Brownlee on ["Gradient Descent With Momentum from Scratch"](https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/):

> Momentum involves adding an additional hyperparameter that controls the amount of history (momentum) to include in the update equation, i.e. the step to a new point in the search space. 

This parameter will help gradient descent accelerate in one direction based on past updates. A good analogy is a ball rolling down the hill. The more momentum it gains, the faster the ball will move in the direction of travel. If we have noisy gradients, momentum will help dampen the noise and keep the algorithm moving in the correct direction. Therefore, the first choice is correct.

This explanation also helps understand why the third choice is correct as well. Having gradients with a lot of variance will cause gradient descent to spend a long time bouncing around, while adding momentum will straighten the direction of the search. This will lead to faster convergence.

Momentum helps the optimization overcome small local minima by rolling past them. Going back to our example of a ball rolling down the hill, the more momentum it has, the more likely it will be to overcome small dips in the ground. This makes the second choice correct as well.

Finally, the fourth choice is not correct because momentum does help with flat regions in the search space. In the same way it can overcome small dips in the surface, momentum can help gradient descent get past a flat region by continuing its previous movement. Here is [Jason Brownlee](https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/) again:

> (...) momentum is helpful when the search space is flat or nearly flat, e.g. zero gradient. The momentum allows the search to progress in the same direction as before the flat spot and helpfully cross the flat region.

In summary, the first three choices are correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Gradient Descent With Momentum from Scratch"](https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/) covers this question very well and includes practical examples of how to implement momentum.
- ["Momentum"](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) is the Wikipedia page covering momentum as part of [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent).
- The ["Deep Learning"](https://amzn.to/3CSjPkR) book by Goodfellow, et. al. is a fantastic source covering this topic.</p></details>

-----------------------

## Date - 2022-05-29


## Title - Choosing the wrong metric


### **Question** :

Let's assume you are working with a severely imbalanced dataset. 

We've all been there. It's a pretty typical scenario.

Now let's imagine you want to split the data into two categories using a classification learning algorithm.

It's hard to pick the best evaluation metric for this problem if we don't know what we want to accomplish. But at least we can rule out the ones that we shouldn't use.

**Which of the following metrics should you avoid using when evaluating your model's performance?**


### **Choices** :

- Recall
- Precision
- F1-Score
- Accuracy


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's illustrate this with a hypothetical example. 

Let's imagine that your team wants to build a machine learning model to predict whether a specific car will get in an accident. 

You are pretty funny, so you decide to play a prank on everyone else by committing this as a solution to the problem:

```
def is_the_car_going_to_crash_today():
    return False
```

Your team evaluates the model against a test set, and your dummy code is 99% accurate!

The National Safety Council reports that the odds of being in a car crash in the United States are less than 1%. This means that even the dumb function above will be very accurate!

The problem here is probably obvious by now: Accuracy is not a good metric when you face a very imbalanced problem. You can achieve very high accuracy even with a model that does nothing useful.

Some examples of imbalanced problems:
* Detecting fraudulent transactions
* Classifying spam messages
* Determining if a patient has cancer

The other three metrics will give you much more information than accuracy, depending on the problem and how you want to approach it.

In summary, the fourth choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Random Oversampling and Undersampling for Imbalanced Classification](https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/)
* Check ["Failure of Classification Accuracy for Imbalanced Class Distributions"](https://machinelearningmastery.com/failure-of-accuracy-for-imbalanced-class-distributions/) to understand why accuracy fails when working with imbalanced datasets.
* If you are into Twitter, [here is a much more detailed story](https://twitter.com/svpino/status/1357302018428256258) about predicting crashes with 99% accuracy.</p></details>

-----------------------

## Date - 2022-05-30


## Title - A non-boring government job


### **Question** :

Nobody thought that Hazel's job was going to be this interesting.

Her first assignment working for the government was in a lab, but not any lab. She was working with bright people on audio surveillance applications.

A week in and she got a package with maximum urgency. The government recorded a known terrorist at a coffee shop, but unfortunately, the audio was almost inaudible because the music was playing simultaneously.

Hazel needs to clean the audio so they can listen to the target.

**Which of the following techniques should Hazel use?**


### **Choices** :

- Hazel could use Independent Component Analysis to reveal the mixed-signal sources from the audio recording.
- Hazel could use a clustering algorithm to cluster the voice and the music apart on the audio recording.
- Hazel could use supervised learning to identify the signal coming from the voice from the signal coming from the music.
- There's not a good way to clean the audio.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>This problem is known as the "Cocktail Party Problem," where we need to separate two independent but previously mixed audio sources. If we make some assumptions, Hazel should be able to solve this problem.

Let's go straight to the correct answer: [Independent Component Analysis](https://en.wikipedia.org/wiki/Independent_component_analysis) (ICA) is a dimensionality reduction algorithm that should do the trick for Hazel. Here is a quote and an image depicting the problem from ["A Tutorial on Independent Component Analysis"](https://arxiv.org/pdf/1404.2986.pdf):

> Solving blind source separation using ICA has two related interpretations – filtering and dimensional reduction. (...) Filtering data based on ICA has found many applications (...), most notably audio signal processing.

![ICA](https://user-images.githubusercontent.com/1126730/169366606-b019ba62-7999-45a1-8c37-abe333e8f932.png)

Keep in mind that, for ICA to work, the source signals have to be independent of each other and not normally distributed. 

Using a clustering algorithm to separate both signals sounds like something that could potentially work, even if for a constrained use case. I wouldn't be surprised if clustering has been used before for this purpose, but I couldn't find any successful examples of solving the Cocktail Party Problem using clustering, so this is not a correct answer for this question.

The third choice argues for a supervised algorithm. I can see a scenario where we could model the problem in a way that a dataset and a trained neural network help us separate the sources, but this doesn't seem like a viable alternative for this case. I didn't find any examples about this either, so this choice is also incorrect.

In summary, the first choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [A Tutorial on Independent Component Analysis](https://arxiv.org/pdf/1404.2986.pdf)
* [Cocktail Party Problem - Eigentheory and Blind Source Separation Using ICA](https://gowrishankar.info/blog/cocktail-party-problem-eigentheory-and-blind-source-separation-using-ica/)
* [Independent component analysis](https://en.wikipedia.org/wiki/Independent_component_analysis)</p></details>

-----------------------

## Date - 2022-05-31


## Title - Fewer false negatives


### **Question** :

Allison is the Chief Data Scientist of the hospital, and she's been leading a revolutionary machine learning application to help identify patients that could potentially develop a rare disease.

The main goal of the model is to identify every patient that's prone to developing the condition. Allison has worked very hard to reduce the number of false negatives as much as possible.

**From the following list, select every accurate statement describing Allison's situation.**


### **Choices** :

- Allison has worked hard to ensure her model has high sensitivity.
- Sensitivity is the same as the True Positive Rate of the model.
- Higher sensitivity means that the model minimizes the number of false negatives.
- Allison can compute the True Positive Rate of her model by dividing the number of patients that could develop the disease by all patients selected as positive by the model.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The problem gives us an important clue about what Allison has been doing: she wants to reduce the number of false negatives as much as possible.

Let's start from the beginning and work on this problem step by step. 

A positive sample represents a patient that could develop the disease, and a negative sample represents a patient that will not develop it. Allison wants to reduce the number of false negatives, which is the number of patients that could become sick, but the model misses. In other words, if a patient could become ill and the model misses it, the hospital won't be able to offer treatment, so Allison wants to make sure that happens as infrequently as possible.

Sensitivity refers to the probability of selecting a patient as positive if the person has a genuine chance of developing the disease. We can compute sensitivity by dividing every true positive patient by every patient we think is positive. In short, `sensitivity = TP / P`. 

A model with high sensitivity minimizes the number of false negatives. To get here, we can look back at the formula for sensitivity and break down positive samples (P) into True Positives (TP) + False Negatives (FN). This will give us that `sensitivity = TP / (TP + FN)`. The more False Negatives we have, the lower the sensitivity, so a high-sensitive model keeps false negatives as low as possible.

Allison wants to keep the number of false negatives down, so she wants a high-sensitive model. This makes the first and third choices correct answers to this question.

Finally, we can compute the model's [True Positive Rate](https://www.ibm.com/docs/en/cloud-paks/cp-data/4.0?topic=overview-true-positive-rate-tpr) (TPR) as `TPR = TP / P`. Notice how this is the same formula we use to calculate sensitivity, so we can conclude that the second choice is also correct. The fourth choice's description of True Positive Rate is conveniently also accurate.

In summary, every single choice of this question is correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Everything you need to know about [Sensitivity and specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) you can find in Wikipedia.
* ["Machine Learning – Sensitivity vs Specificity Difference"](https://vitalflux.com/ml-metrics-sensitivity-vs-specificity-difference/) is a great article covering the differences between these two concepts.</p></details>

-----------------------

## Date - 2022-06-01


## Title - The Fukushima nuclear disaster


### **Question** :

The Fukushima nuclear disaster was the most severe nuclear accident since Chernobyl. Together, they have been the only ones with a level 7 classification on the International Nuclear and Radiological Event Scale.

In 2011, an earthquake followed by a tsunami caused the disaster in the Japanese plant, and it all traces back to a mistake in the safety model.

The engineers used historical earthquake data to build a regression model to determine the likelihood of significant earthquakes. Instead of using the accepted [Gutenberg-Richter](https://en.m.wikipedia.org/wiki/Gutenberg%E2%80%93Richter_law) model, they saw a kink in the data and assumed the appropriate regression was not linear but polynomial.

The correct linear model would have predicted that earthquakes of 9.0 magnitude were 70 times more likely than what the incorrect polynomial model predicted. But the engineers, in their pursuit of following the data too closely, came up with a very different conclusion.

The plant was designed to withstand a maximum earthquake of 8.6 magnitude and a tsunami as high as 5.7 meters. The earthquake of 2011 measured 9.0 and resulted in a 14-meter high tsunami.

**How would you summarize the mistake made by the engineers in this incident?**


### **Choices** :

- The engineers built a model that wasn't powerful enough and ended up underfitting the historical earthquake data.
- The engineers built a model that wasn't powerful enough and ended up overfitting the historical earthquake data.
- The engineers built a model that was too powerful and ended up overfitting the historical earthquake data.
- The engineers built a model that was too powerful and ended up underfitting the historical earthquake data.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When designing the model, the engineers saw a kink in the data. A linear model couldn't follow those data points closely, so they switched to a more complex, polynomial model.

The most important result of the [Gutenberg-Richter law](https://en.m.wikipedia.org/wiki/Gutenberg%E2%80%93Richter_law) is that the relationship between the magnitude of an earthquake and the logarithm of the probability that it happens is linear. The engineers ignored this.

This is a devastating example of overfitting. Here is an excerpt from [Berkeley's machine learning crash course](https://ml.berkeley.edu/blog/posts/crash-course/part-4/):

> As the name implies, overfitting is when we train a predictive model that "hugs" the training data too closely. In this case, the engineers knew the relationship should have been a straight line, but they used a more complex model than they needed to.

The third choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Fukushima: The Failure of Predictive Models](https://mpra.ub.uni-muenchen.de/69383/1/MPRA_paper_69383.pdf)
* [Machine Learning Crash Course: Part 4](https://ml.berkeley.edu/blog/posts/crash-course/part-4/)
* [Gutenberg–Richter law](https://en.m.wikipedia.org/wiki/Gutenberg%E2%80%93Richter_law)</p></details>

-----------------------

## Date - 2022-06-02


## Title - Classifying waste


### **Question** :

A group of students decided to build an application to classify household waste using smartphone pictures.

They want to start with a simple solution, so they are focusing the first version on the most commonly found types of waste: liquid, solid, organic, recyclable, and hazardous waste.

The tricky part of the application is that it needs to recognize every type of waste present on every image uploaded by users.

The students decided to use a convolutional neural network to solve this problem. The only question left is on the best way to architect it.

**Which of the following would be the best approach to design this network?**


### **Choices** :

- The output layer of the network should have a softmax activation function. The loss function should be categorical cross-entropy.
- The output layer of the network should have a sigmoid activation function. The loss function should be binary cross-entropy.
- The output layer of the network should have a softmax activation function. The loss function should be binary cross-entropy.
- The output layer of the network should have a sigmoid activation function. The loss function should be categorical cross-entropy.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The students are trying to build a [multi-label classification](https://en.wikipedia.org/wiki/Multi-label_classification) model. In multi-label classification, every image might show multiple types of waste. This is different from [multi-class classification](https://en.wikipedia.org/wiki/Multiclass_classification), where a photo would show only one kind of waste.

When building multi-label classification models, we need an output layer where every class is independent. Remember that we can have more than one active class for each input. The softmax activation function doesn't work because it uses every score to output the probabilities of each class. Softmax is the correct output for multi-class classification but not for multi-label classification problems. 

Since we shouldn't use softmax, the first and third choices are incorrect. The sigmoid function converts output scores to a value between 0 and 1, independently of all the other scores.

Multi-label classification problems borrow the same principles from binary classification problems. The difference is that we end up with multiple sigmoid outputs instead of a single one. In our example problem, we have a combination of five different binary classifiers. This is why we should use a binary cross-entropy as the loss function.

In summary, multi-class classification models should use a softmax output with the categorical cross-entropy loss function. Multi-label classification models should use a sigmoid output and the binary cross-entropy loss function.

The second choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The Wikipedia explanation of [Multi-label classification](https://en.wikipedia.org/wiki/Multi-label_classification) should give you most of what you need to understand for this type of task.
* ["Difference between multi-label classification and multi-class classification"](https://towardsdatascience.com/multi-label-image-classification-with-neural-network-keras-ddc1ab1afede) is an excellent article comparing these two types of problems.
* ["How to choose cross-entropy loss function in Keras?"](https://androidkt.com/choose-cross-entropy-loss-function-in-keras/) explains the differences between the loss functions that we discussed in this question.</p></details>

-----------------------

## Date - 2022-06-03


## Title - Everyone on the same page


### **Question** :

Willow overheard her two friends arguing about the best way to handle a few categorical features on their dataset.

One suggested Label encoding, while the other was pushing for One-Hot encoding. Both are popular encoding techniques, but Willow didn't know enough to understand the difference.

She decided to write a quick summary of both techniques to get everyone on the same page, but the discussion had her confused. She came up with two different explanations for each method, but she wasn't sure which one was correct. 

**Which of the following statements are correct about these two encoding techniques?**


### **Choices** :

- One-Hot encoding replaces each label from the categorical feature with a unique integer based on alphabetical ordering.
- One-Hot encoding creates additional features based on the number of unique values in the categorical feature.
- Label encoding replaces each label from the categorical feature with a unique integer based on alphabetical ordering.
- Label encoding creates additional features based on the number of unique values in the categorical feature.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Before analyzing this question, we need to understand what "categorical data" means.

Categorical data are variables that contain label values rather than numeric values. For example, a variable representing the weather with values "sunny," "cloudy," and "rainy" is a categorical variable.

Although some algorithms can use categorical data directly, the majority can't: they require the data to be numeric. We can use One-Hot or Label encoding to do this.

[One-Hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f) creates a new feature for each unique value of the original categorical variable.

For example, assume we have a dataset with a single feature called "weather" that could have the values "sunny," "cloudy," and "rainy." Applying One-Hot Encoding will get us a new dataset with three features, one for each value of the original "weather" column. 

A sample that had the value "cloudy" in the previous column will now have the value 0 for both "sunny" and "rainy" and the value 1 under the "cloudy" feature.

This means that the second choice is the correct explanation of how One-Hot Encoding works.

On the other hand, [Label encoding](https://www.mygreatlearning.com/blog/label-encoding-in-python/) replaces each categorical value with a consecutive number starting from 0. 

For example, Label Encoding would replace our weather feature with a new one containing the values 0 instead of "cloudy," 1 instead of "rainy," and 2 instead of "sunny."

This means that the third choice is the correct explanation of how Label Encoding works.

Therefore, the second and third choices are the correct answers to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What is One Hot Encoding? Why and When Do You Have to Use it?"](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f) is an excellent introduction to One-Hot encoding.
* ["Label Encoding in Python Explained"](https://www.mygreatlearning.com/blog/label-encoding-in-python/) is an introduction to Label encoding.
* ["One-Hot Encoding vs. Label Encoding using Scikit-Learn"](https://www.analyticsvidhya.com/blog/2020/03/one-hot-encoding-vs-label-encoding-using-scikit-learn/) covers both techniques and when to use each one.</p></details>

-----------------------

## Date - 2022-06-04


## Title - Cutting down neurons


### **Question** :

Denise had an idea she wanted to try on the neural network she built to identify handwritten digits.

Her output layer had 10 neurons, one for each digit she wanted to
recognize. She thought she could optimize the training process by cutting the number of neurons down to 4.

The goal of the model was to recognize the digit represented by an input image, and with 4 neurons, she could encode a total 16 different values, so she thought this was enough.

After training for some time, Denise found out that the network didn't perform well.

**What conclusions can you draw from Denise's experience?**


### **Choices** :

- To get better results, Denise should experiment with different optimization algorithms and learning rate values.
- Denise's network is working correctly. She can improve the results by modifying her evaluation criteria and discarding any mistakes due to the extra capacity supported by her new architecture.
- With this architecture, the first output neuron has to decide what the most significant bit of the digit represented by the image was. Unfortunately, there's no apparent relationship between the shapes that make up a digit and this information.
- Instead of replacing the 10-neuron layer, Denise should have kept it as a hidden layer and added the 4-neuron layer as the new output. The new network should be capable of finding the bitwise representation of the digit without much trouble.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In his excellent book [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap1.html#a_simple_network_to_classify_handwritten_digits), [Michael Nielsen](https://twitter.com/michael_nielsen) presents this problem and his results after trying both architectures:

> The ultimate justification is empirical: we can try out both network designs, and it turns out that, for this particular problem, the network with 10 output neurons learns to recognize digits better than the network with 4 output neurons. 

But why does the 10-neuron output network outperforms the 4-neuron output network?

> If we had 4 outputs, then the first output neuron would be trying to decide what the most significant bit of the digit was. And there's no easy way to relate that most significant bit to simple shapes (...) It's hard to imagine that there's any good historical reason the component shapes of the digit will be closely related to (say) the most significant bit in the output.

This means that the third choice is a correct explanation of what's happening to Denise with her new architecture. She won't solve the problem by exploring alternate optimization functions or experimenting with different learning rate values. Her architecture is fundamentally flawed.

The fourth choice is a potential avenue that Denise could use to improve the results. She could keep the original layer with 10 neurons to identify the correct digit but add the extra 4-neuron layer as the output to find the digit's binary representation. The network should have no issues solving this problem.

In summary, the third and fourth choices are the correct answer.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Using neural nets to recognize handwritten digits"](http://neuralnetworksanddeeplearning.com/chap1.html#a_simple_network_to_classify_handwritten_digits) is Michael Nielsen's book chapter that discusses this problem.
* ["But what is a neural network?"](https://www.youtube.com/watch?v=aircAruvnKk) is a YouTube video with one of the best explanations out there about how neural networks work.</p></details>

-----------------------

## Date - 2022-06-05


## Title - Regression x 4


### **Question** :

Here are four popular machine learning methods.

Imagine you want to build a simple binary classification model. Your goal is to predict whether a sample is positive or negative.

You could make any of these four algorithms give you the results you want with enough work. That's awesome, but you are interested in the easiest way to make this happen.

**Which of these four algorithms would you use?**


### **Choices** :

- Linear regression
- Lasso regression
- Logistic regression
- Random Forest regression


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The correct answer is Logistic regression. 

[Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) is an excellent fit for binary classification tasks. It outputs the probability of one event, in our case, the probability of a sample being positive.

All other methods are used to perform regression, in which the algorithm will predict a continuous outcome. We, however, want a categorical output, so logistic regression is the best approach.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check out ["Logistic Regression for Machine Learning"](https://machinelearningmastery.com/logistic-regression-for-machine-learning/) for an introduction to Logistic regression.
- ["Logistic Regression"](https://en.wikipedia.org/wiki/Logistic_regression) is the Wikipedia page introducing logistic regression.
- For a list and a quick introduction to regression algorithms, check out ["5 Regression Algorithms you should know – Introductory Guide!"](https://www.analyticsvidhya.com/blog/2021/05/5-regression-algorithms-you-should-know-introductory-guide/)</p></details>

-----------------------

## Date - 2022-06-06


## Title - Supervised learning workshop


### **Question** :

Lydia is going to be teaching a new machine learning class.

Among other things, she will be covering Supervised Learning techniques. Lydia knows how important this is, so she is preparing to turn the class into a giant hands-on workshop.

The University has access to many different datasets, and Lydia decides to pick a few interesting problems with enough data for students to explore.

**Which of the following problems should Lydia pick to teach supervised learning?**


### **Choices** :

- Determine whether a website displays content for a mature audience.
- Learn the best way to split a group of car buyers into different categories based on their buying patterns.
- Given the medical records from patients suffering a specific illness, learn whether we can split them into different groups for better treatment.
- Predict next year's crop yields taking into account data of the past decade.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>There are always multiple ways to approach these problems, but some are better for [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning), while others can benefit from [unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning).

To answer this question, we need to assume that the datasets from the university are labeled when necessary. Remember that supervised learning algorithms require these labels.

Let's start with the first choice. Given any website, we want a "Yes" or "No" answer depending on whether the site displays mature content. Lydia could tackle this problem with a binary classification algorithm, which is a supervised learning technique. 

Splitting a group of car buyers into different categories requires a technique that helps Lydia cluster buyers based on their buying patterns. We can't foresee these patterns beforehand, so any problem that involves finding out the best way of grouping samples is a good fit for clustering algorithms. [Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) is an unsupervised learning technique, so this option is not a good fit for supervised learning. 

The same happens with the problem related to the medical records. We don't have a predefined set of categories to split the group of patients, so this problem seems to be more amenable to clustering techniques.

Finally, predicting next year's crop yields seems a good candidate for a regression algorithm. Regression algorithms are supervised learning techniques that help us predict a continuous value—in this case, how much a crop will yield. This is another valid answer to this question.

In summary, the first and fourth choices are the correct answers.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Supervised and Unsupervised Machine Learning Algorithms"](https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/) is an excellent introduction to the differences between supervised and unsupervised learning.
* Check out ["Customer Segmentation with Machine Learning"](https://towardsdatascience.com/customer-segmentation-with-machine-learning-a0ac8c3d4d84) for more information about how to tackle problems where we need to cluster samples into groups.</p></details>

-----------------------

## Date - 2022-06-07


## Title - Making email fun


### **Question** :

Let's be honest: dealing with email is not fun.

Waking up to an inbox full of unsolicited messages is the worst way to kick off your day. Email applications do their best, but a lot of spam still gets through the cracks.

How about building your own personalized spam detection model? 

One morning, Sue decided to do it, and after a few iterations, she ended with a working model.

Time to find out how good it is!

**Which method should Sue use to evaluate her spam detection model?**


### **Choices** :

- Sue should use the model's accuracy as defined by the percentage of legitimate messages that go through with respect to the total number of received emails.
- Sue should use the model's recall as defined by the percentage of detected spam messages with respect to the total of spam messages received.
- Sue should use the Fβ-Score of the model with a high value of β.
- Sue should use the Fβ-Score of the model with a low value of β.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Spam detection is an imbalanced problem: you will always have more legitimate emails than spam emails. 

Whenever you need to work with an imbalanced dataset, accuracy will not be a good metric to decide how good your model is. You can achieve very high accuracy even with a model that does nothing useful. For example, if only 1% of the emails you receive are spam, by simply assuming none of it is spam, your model will be 99% accurate. Therefore, the first choice is not a good approach for Sue.

The recall is a helpful metric to understand how much spam you can detect, but by itself could also be deceiving. For example, Sue's model could flag every single email message as spam, which will give her a 100% recall. This, of course, it's not helpful, so the second choice is also not correct.

Using the [Fβ-Score](https://en.wikipedia.org/wiki/F-score), however, is a good choice. 

The Fβ score lets us combine precision and recall into a single metric. When using β = 1, we place equal weight on precision and recall. For values of β > 1, recall is weighted higher than precision; for values of β < 1, precision is weighted higher than recall. 

You are probably familiar with F1-Score. F1-Score is just Fβ-Score with β = 1.

Sue doesn't want to flag valid legitimate messages as spam. Doing this runs the risk of people missing important emails. Therefore, a good strategy is to prioritize a system with high precision, so using a lower value of β is the way to go. Therefore, the correct answer is the fourth choice.

Notice that by prioritizing the precision of her model, Sue will let some spam messages through. Although this is not ideal, it's a better outcome than getting the spam filter to catch legitimate emails.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What is the F-Score?](https://deepai.org/machine-learning-glossary-and-terms/f-score) is a short introduction to this metric.
* For a more in-depth analysis of the Fβ-Score, check ["A Gentle Introduction to the Fbeta-Measure for Machine Learning"](https://machinelearningmastery.com/fbeta-measure-for-machine-learning).
* Check ["Failure of Classification Accuracy for Imbalanced Class Distributions"](https://machinelearningmastery.com/failure-of-accuracy-for-imbalanced-class-distributions/) to understand why accuracy fails when working with imbalanced datasets.</p></details>

-----------------------

## Date - 2022-06-08


## Title - Non-linearities


### **Question** :

River learned an important lesson when trying to implement a neural network from scratch: 

For her network to learn anything useful, she needed to introduce non-linearities.

Whenever she didn't do it, the results were utter trash.

**Which of the following will add non-linearities to River's neural network?**


### **Choices** :

- Using Rectifier Linear Unit (ReLU) as an activation function.
- Adding convolution operations to the network.
- Using Stochastic Gradient Descent to train the network.
- Implementing the backpropagation process.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>For a neural network to learn complex patterns, we need to ensure that the network can approximate any function, not only linear ones. This is why we call it "non-linearities."

The way we do this is by using activation functions. 

An interesting fact: the [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) states that, when using non-linear activation functions, we can turn a two-layer neural network into a universal function approximator. This is an excellent illustration of how powerful neural networks are.

Some of the most popular activation functions are [sigmoid](https://en.wikipedia.org/wiki/Logistic_function), and [ReLU](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks). Therefore, the first choice is the correct answer to this question.

The second choice is incorrect; a [convolution operation is a linear operation](https://en.wikipedia.org/wiki/Convolution#Properties). You can check [this answer](https://ai.stackexchange.com/questions/19879/arent-all-discrete-convolutions-not-just-2d-linear-transforms) in Stack Exchange for an excellent explanation.

Finally, neither Stochastic Gradient Descent nor backpropagation has anything to do with the linearity of the network operations. Therefore, they aren't correct answers.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Activation function"](https://en.wikipedia.org/wiki/Activation_function) from Wikipedia to understand more about this topic.
* I find the ["Universal approximation theorem"](https://en.wikipedia.org/wiki/Universal_approximation_theorem) fascinating.</p></details>

-----------------------

## Date - 2022-06-09


## Title - Accuracy as a loss function


### **Question** :

Luna knows that the entire goal of gradient descent is to minimize the value of a function. The lower its value, the better the model will be. She has been thinking about designing a custom loss function for her use case.

She wants to use the inverse of the model's accuracy as the loss function. This way, the model will try to minimize the number of mistakes it makes by looking directly at the accuracy of the predictions.

Unfortunately, she soon discovers that this doesn't work. 

**What is the problem with this approach?**


### **Choices** :

- Accuracy is not a differentiable function, so it can't be optimized using gradient descent.
- Luna wants to optimize for high accuracy but gradient descent is a minimization algorithm; the opposite of what she needs.
- Minimizing the inverse of the accuracy is a very slow process because of the extra computations needed to compute the final value.
- Gradient descent only works with a predefined set of loss functions.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We use loss functions to optimize a model. We use accuracy to measure the performance of that model.

Usually, we can see how the accuracy of a classification model increases as the loss decreases. This is not always the case, however. The loss and the accuracy measure two different aspects of a model. Two models with the same accuracy may have different losses. 

An important insight: The loss function must be continuous, but accuracy is discrete. When training a neural network with gradient descent, we need a differentiable function because the algorithm can't optimize non-differentiable functions. One of the required characteristics for a function to be differentiable is that it must be continuous. Since accuracy isn't, we can't use it.

This makes the first choice the correct answer to this question.

The second and third choices assume that we can use accuracy as the loss function one way or the other, so they are incorrect. The fourth choice claims that gradient descent can only work with a predefined subset of functions, which is also wrong.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Loss and Loss Functions for Training Deep Learning Neural Networks"](https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/) is a great introduction to loss functions.
- Check ["How to Choose Loss Functions When Training Deep Learning Neural Networks"](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/) for a guide on how to choose and implement different loss functions.</p></details>

-----------------------

## Date - 2022-06-10


## Title - Psychologists speak another language


### **Question** :

A group of psychologists is visiting the office, and Scarlett is in charge of showing them around.

The first stop will be in the Data Science department. They are very excited about showing them the results of their latest machine learning model.

Ten minutes into the presentation, it's painfully apparent that the crew is not fully grasping what is going on. Scarlett decides to summarize her ideas using a familiar language: statistics.

In statistics, the notion of statistical error is an integral part of hypothesis testing. There are two types of errors when testing the null hypothesis: type I and type II errors. Scarlett wants to explain their results regarding the latter.

**Do you remember what the correct definition of a type II error is?**


### **Choices** :

- A type II error occurs when the null hypothesis is true and is not rejected.
- A type II error occurs when the null hypothesis is true but is rejected.
- A type II error occurs when the null hypothesis is false but is not rejected.
- A type II error occurs when the null hypothesis is false and is rejected.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>It makes sense for those who are more used to machine learning terminology to compare type I and type II errors with false positives and false negatives.

Type I errors are the same as false positives. For example, if we mark a valid email as spam, we are in the presence of a false positive. Type I errors are the rejection of a true [null hypothesis](https://www.investopedia.com/terms/n/null_hypothesis.asp) by mistake.

Type II errors are the same as false negatives. For example, if we let a spam message pass as a valid email, we are in the presence of a false negative. This is a type II error because we accept the conclusion of the email being good, even though it is incorrect. Type II errors are the acceptance of a false null hypothesis by mistake.

In other words, a type II error is when we incorrectly accept the null hypothesis even though the alternative hypothesis is true. Therefore, The third choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What Is a Null Hypothesis?"](https://www.investopedia.com/terms/n/null_hypothesis.asp) covers the basics you need to understand before going into hypothesis testing.
* Check out ["Type I and type II errors"](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors) for the definition and examples of each type of error. 
* ["Understanding Null Hypothesis Testing"](https://opentextbc.ca/researchmethods/chapter/understanding-null-hypothesis-testing/) is an excellent article about hypothesis testing.</p></details>

-----------------------

## Date - 2022-06-11


## Title - Penelope is looking for more


### **Question** :

Penelope needs to finish her homework. The final step is to make her neural network "deeper."

She tried to solve the problem using a shallow network architecture, but her professor wanted her to try a deeper network.

**What should Penelope do to turn her architecture into a deep neural network?**


### **Choices** :

- Penelope should increase the dimensionality of the input data.
- Penelope should add more layers to the network.
- Penelope should use images, text, voice, or video as the input.
- Penelope should add more neurons to the existing layers.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A deep neural network is an artificial neural network with multiple layers between the input and output layers. The more layers we add, the "deeper" the network becomes.

A deep neural network can process all sorts of data. Neither the type of input nor its dimensionality has any relationship with the depth of the network. Therefore, neither the first nor the third choices are correct.

Adding neurons alone doesn't change the depth of the network either. It does change its capacity, but a network with the same number of layers but more neurons will still have the same depth. 

The only choice for Penelope is to add more layers to the network.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Wikipedia's definition of ["Deep neural networks"](https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks) serves as a succinct summary of what deep neural networks are.
* Check ["A Layman’s Guide to Deep Neural Networks"](https://towardsdatascience.com/a-laymans-guide-to-deep-neural-networks-ddcea24847fb) for a non-mathematical introduction to deep neural networks.</p></details>

-----------------------

## Date - 2022-06-12


## Title - Low-variance model


### **Question** :

Usually, the best approach is to start experimenting with a few different algorithms to narrow down the possibilities and pick a good path forward.

That's what Sophia did. She ran her dataset through four different algorithms and noticed something peculiar.

Since she had a lot of data, she trained each algorithm with different batches separately. Only one of the models gave consistent results regardless of what portion of the data she used.

Sophia knew this was a variance issue. Low-variance models usually produce consistent results regardless of the data used to train them.

**Which of the following algorithms was the one giving consistent results?**


### **Choices** :

- Support Vector Machine (SVM)
- Decision Trees
- Logistic Regression
- k-Nearest Neighbors (KNN)


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Every machine learning algorithm deals with three types of errors: bias, variance, and irreducible error. We need to focus specifically on the variance error to answer this question.

Here is what [Jason Brownlee](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) has to say about variance: "Variance is the amount that the estimate of the target function will change if different training data was used."

In other words, variance refers to how much the answers given by the model will change if we use different training data. The model has low variance if the answers stay the same when using different portions of our training dataset.

Generally, linear models with little flexibility have low variance. For example, Linear and logistic regression are low-variance models. Nonlinear algorithms with a lot of flexibility have high variance, for example, Decision Trees, Support Vector Machines, and k-Nearest Neighbors.

Therefore, the third choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) is Jason Brownlee's article covering bias, variance, and their tradeoff.
* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).
* In case you like the simplicity of Twitter threads, here is one for you about this topic: ["Bias, variance, and their relationship with machine learning algorithms"](https://twitter.com/svpino/status/1390969728504565761).</p></details>

-----------------------

## Date - 2022-06-13


## Title - Looking for labels


### **Question** :

The company had a lot of data, but none was labeled. 

As soon as the team started planning the work, Blake's first recommendation was to look into Supervised Learning. She knew, however, that without labeled data, they weren't going anywhere.

There are many different ways to produce labels, and Blake will have to decide how to move forward.

**Which of the following techniques could Blake use to label the data?**


### **Choices** :

- Assemble a team of people that go through the data and label each sample.
- Use feedback from an existing process to automatically produce the labels.
- Use a Supervised Learning technique to infer the labels directly from the existing data.
- Use Semi-Supervised Learning to propagate labels across all of your data.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>If Blake wants to use a Supervised Learning method, she has no other option than to produce labels for the data. There are many different techniques she can use to accomplish this.

The most common way to label data is to use human labelers. Blake could assemble a team that will go through each sample assigning the appropriate label. For example, assuming the company wants to classify car pictures, the labelers could review each image and set the correct make and model of the car. Therefore, the first choice is correct.

Sometimes, we can use feedback from an existing process to create labels, also known as "direct labeling." For example, a video site recommending movies to different users can use actual clicks from its audience to determine which posters work and which don't. 

Unfortunately, direct labeling is very dependent on your use case, and it's not something you can always do. Also, notice that direct labeling doesn't necessarily capture the "true ground-truth," but only a useful approximation. Nevertheless, direct labeling is a good approach, so the second choice is correct.

The third choice argues that we could use a Supervised Learning method to infer the labels from the existing dataset, but this doesn't make sense. Supervised Learning requires the existence of labels, and that's what Blake doesn't have. If we could use the data to predict labels, we could also use it to solve the problem in the first place. This choice is incorrect.

Finally, we could use [Semi-Supervised Learning](https://machinelearningmastery.com/semi-supervised-learning-with-label-propagation/) assuming we already have a few labels. For example, if we had 10% of the labels, we could build a model to generate the other 90% of labels. However, there's no indication that Blake has any labeled data, so Semi-Supervised Learning is not an option. This choice is also incorrect.

[Active Learning](https://rapidminer.com/glossary/active-learning-machine-learning/) and [Weak Supervision](https://snorkel.ai/weak-supervision/) are also techniques to generate labels. They aren't part of this question, but it's helpful to know about them.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The ["Machine Learning Data Lifecycle in Production"](https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production) course in Coursera, part of the [Machine Learning Engineering for Production (MLOps) Specialization](https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops).
* Check out ["Semi-Supervised Learning With Label Propagation"](https://machinelearningmastery.com/semi-supervised-learning-with-label-propagation/) for an introduction to how to use a few labels with semi-supervised learning.
* ["Active Learning in Machine Learning"](https://rapidminer.com/glossary/active-learning-machine-learning/) is a short explanation of Active Learning, enough if all you need is a high-level overview.
* If you are serious about Active Learning, ["Active Learning Literature Survey"](https://burrsettles.com/pub/settles.activelearning.pdf) is the publication you want to read. 
* ["Weak Supervision: A New Programming Paradigm for Machine Learning"](http://ai.stanford.edu/blog/weak-supervision/) is a good article from Stanford introducing Weak Supervision.
* An introduction to [Weak Supervision](https://snorkel.ai/weak-supervision/) by Snorkel AI, a labeling platform. This one even includes a video.</p></details>

-----------------------

## Date - 2022-06-14


## Title - Rebecca's rotation


### **Question** :

After a very late coffee, Rebecca felt plenty of energy to crack open the book she's been dreading to read the entire week.

It was a dense read. A computer vision masterpiece that went all the into the mathematical reasoning behind every topic.

The latest chapter combined deep learning, linear algebra, and geometric transformations. Rebecca promised herself to watch a movie as soon as she finished the first problem on the topic.

It seemed straightforward: Rebecca had to rotate a two-dimensional square 90 degrees counterclockwise using matrix multiplication. She knew she had to multiply the coordinates of her square with a specific 2x2 matrix, but she didn't remember how exactly it worked.

**Which of the following is the correct matrix R to rotate a 2D square 90 degrees counterclockwise?**


### **Choices** :

- The matrix R is `[[0, -1], [1, 0]]`.
- The matrix R is `[[0, 1], [-1, 0]]`.
- The matrix R is `[[1, 0], [0, 1]]`.
- The matrix R is `[[-1, 0], [0, -1]]`.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>At its core, deep learning mostly boils down to many tensor operations chained together. These operations have a corresponding geometric interpretation, and understanding them is an excellent exercise to shed some light on how deep learning networks work.

When we talk about [rotating](https://en.wikipedia.org/wiki/Rotation_(mathematics)) an object, we can think of moving each point of that object circularly around a center. Assuming that we use a column vector to represent the coordinate of each point, we can use matrix multiplication to rotate the object.

We need a rotation matrix R to multiply with the object's coordinates and obtain the new set of rotated coordinates. The structure of this matrix R to rotate an object counterclockwise is `[[cos(θ), -sin(θ)], [sin(θ), cos(θ)]]` where θ represents the rotation angle.

Rebecca needs to rotate the 2D square 90 degrees. Remember that `cos(90) = 0` and `sin(90) = 1`, so the matrix R that Rebecca needs is `[[0, -1], [1, 0]]`, which is the first choice of this question.

Just for fun, we can go through all the other choices and determine what would be the corresponding rotation angle. 

The second choice rotates the 2D square 90 degrees clockwise—in the opposite direction that Rebecca wanted. Notice how the only difference with the correct answer is the position of the signs.

The third choice does not rotate the square—θ is 0 degrees. To see this, let's start with the matrix `[[cos(θ), -sin(θ)], [sin(θ), cos(θ)]]` and assume we multiply it by a vector `[x, y]` to get a new, rotated vector `[x', y']`:

```
x' = x * cos(θ) - y * sin(θ)
y' = x * sin(θ) + y * cos(θ)
```

Replacing the values of each component as specified in the third choice:

```
x' = x * 1 - y * 0 = x - 0 = x
y' = x * 0 + y * 1 = 0 + y = y
```

Notice how we get the exact coordinates after we apply the rotation. 

Finally, the fourth choice rotates the square 180 degrees counterclockwise. 

In summary, the first choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Rotations and reflections in two dimensions"](https://en.wikipedia.org/wiki/Rotations_and_reflections_in_two_dimensions) for an explanation of how to rotate and reflect objects in a two-dimensional space.</p></details>

-----------------------

## Date - 2022-06-15


## Title - Two similar models


### **Question** :

One of the most critical steps in your machine learning process is selecting a good baseline.

Before you start looking into more complex problems, a simple baseline gives you a solid foundation to understand where you are and what improvements you can make.

Margot and Jan wanted to use a different algorithm to build their baseline. Jan wanted to go with a simple neural network, but Margot argued for using a linear regression model.

They didn't want to pick, so they decided to try each baseline and choose the best one. But before starting, Margot and Jan agreed to research the similarities between both models so they could reuse as much as possible when building them.

They wrote down a list.

**Which of the following are true about Jan's neural network and Margot's linear regression model?**


### **Choices** :

- Both models require every input feature to be a numeric value. Margot and Jan will need to transform every non-numeric feature to feed both baselines.
- Both models require every numerical input to be between 0 and 1. Margot and Jan will need to standardize every numeric feature and squeeze them into this range.
- Both models output a vector of probabilities. Margot and Jan can build a common logic to interpret the results of their respective baselines.
- Both models output the result of a linear sum of the weighted input values. Margot and Jan should be able to explain the results from their respective baselines in a similar fashion.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>I'd go with Margot on this one. A linear regression model sounds simpler to build a baseline than a neural network. The former is easy to set up and tune, while the latter will require much more work.

Of course, this is just a gut feeling. There's nothing on this problem that suggests one way or the other.

But that's not the question. We want to understand which of the four statements is something that both models have in common.

Starting with the first choice, both neural networks and linear regression models require the input features to be numeric. Neither works with categorical features. Decision Trees, for example, don't have this limitation, but these two models do. Therefore, Margot and Jan will need to transform non-numeric features before using them.

The second choice is interesting because it's usually the case that both models work better if their input features are appropriately scaled or standardized. Training a model with a feature ranging from 0 to 10,000 and another feature ranging from 0 to 1 is never a good recipe. However, neither one of these models requires features within 0 and 1, so this is not a correct statement.

The third choice is not correct either. The output of a linear regression model is a single numerical value, not a vector of probabilities. There's also no requirement for a neural network to have such an output. 

Finally, while both linear regression and neural networks use a linear sum of weighted inputs, neural networks introduce non-linearities in the form of activation functions. This is a crucial difference. It makes neural networks much more powerful than linear regression, but at the same time, it makes it much harder to explain the results.

In summary, the correct answer to this question is the first choice.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Linear Regression v.s. Neural Networks"](https://towardsdatascience.com/linear-regression-v-s-neural-networks-cd03b29386d4) for a comparison between these two techniques.
* ["3 Reasons Why You Should Use Linear Regression Models Instead of Neural Networks"](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html) is another great article talking about their differences.
* ["Linear Regression for Machine Learning"](https://machinelearningmastery.com/linear-regression-for-machine-learning/) is a great introduction to linear regression.
* ["Intro to Neural Networks"](https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/video-lecture) is a good summary of neural networks.</p></details>

-----------------------

## Date - 2022-06-16


## Title - Samples per batch


### **Question** :

Gradient Descent is an optimization algorithm frequently used in machine learning applications.

The algorithm can vary in how many data samples we use to calculate the error: a single sample, all available samples, or a batch of samples (more than one but fewer than the entire dataset.)

Naomi read about "Mini-Batch Gradient Descent," and now she is trying to research how it works. 

**Which of the following statements is true about Mini-Batch Gradient Descent?**


### **Choices** :

- Mini-Batch Gradient Descent uses a single sample of data during every iteration.
- Mini-Batch Gradient Descent uses a batch of data (more than one sample but fewer than the entire dataset) during every iteration.
- Mini-Batch Gradient Descent uses all of the available data at once during every iteration.
- Mini-Batch Gradient Descent is an entirely different optimization algorithm, and Trevor shouldn't look at it as something related to Gradient Descent.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Here is a simplified explanation of how [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent) works: We take samples from the training dataset, run them through the model, and determine how far our results are from what we expect. We then use this difference (error) to compute how much we need to update the model weights to improve the results.

A critical decision we need to make is how many samples we use to compute the gradient of the objective function. We have three choices:
* Use a single instance of data.
* Use all of the data at once.
* Use some of the data.

Using a single sample of data is called "Stochastic Gradient Descent" or SGD. Using all the data at once is called "Batch Gradient Descent." Finally, using some of the data—more than one sample but fewer than the entire dataset—is called "Mini-Batch Gradient Descent." 

Notice that we always make a tradeoff between the accuracy of the updates and the time it takes to calculate them. While using a single instance of data is faster, the updates will be more inaccurate. On the other hand, using all of the data available while producing more accurate updates requires storing the entire dataset in memory, and it's very slow.

In summary, the second choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["An overview of gradient descent optimization algorithms"](https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants) for a deep dive into gradient descent and every one of its variants.
* ["Gradient Descent For Machine Learning"](https://machinelearningmastery.com/gradient-descent-for-machine-learning/) is another great introduction to gradient descent.
* ["A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size"](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) covers Batch and Mini-Batch Gradient Descent.</p></details>

-----------------------

## Date - 2022-06-17


## Title - Sorting tomatoes


### **Question** :

Maya's model caused a false alarm, and the tomato sorting line at the factory stopped working for the third time today.

Maya designed a computer vision model to estimate the size of tomatoes entering a sorting machine. The device has a problem handling unusually small or large tomatoes and can get damaged if one of these enters it.

Unfortunately, the factory didn't have the budget to replace the whole machine. So, they hired Maya to install a camera and implement a computer vision model to check the size of the tomatoes. If it detects a tomato that's too small or too large, the machine stops automatically and raises the alarm.

Whenever Maya's model predicts a regular-sized tomato as an outlier, the production line stops and costs the factory money. Maya suspects there is a problem with the loss function she is using.

**Which of the following loss functions should Maya use for this problem?**


### **Choices** :

- Mean Squared Error (MSE)
- Mean Absolute Error (MAE)
- Huber Loss
- Binary Cross-entropy Loss


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Maya should focus on improving how the model handles outliers, and the loss function she is using plays an important role.

Loss functions like the [Mean Absolute Error ](https://en.wikipedia.org/wiki/Mean_absolute_error)(MAE) or the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss) have a linear behavior for outliers. The [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) (MSE), on the other hand, penalizes errors quadratically, which means that significant outliers will get much higher penalties.

For example, consider Maya's model estimating the size of a 5cm tomato to be 15cm. The difference between the actual and predicted size is 10 if she uses MAE or the Huber loss, but it will become 100 if she uses MSE.

Therefore, if Maya uses MSE as the loss function, her model will optimize in a way that avoids significant outliers. Smaller errors are not critical in this case since she is only interested in very small or large tomatoes.

With this in mind, the first choice is the correct answer, while the second and the third are not. The fourth choice is the cross-entropy loss which doesn't apply to regression problems and is therefore incorrect.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Regression Metrics for Machine Learning"](https://machinelearningmastery.com/regression-metrics-for-machine-learning/) for an overview of some of the most popular metrics used for regression problems.
* ["RMSE vs MAE, which should I use?"](https://stephenallwright.com/rmse-vs-mae/) is a great summary by Stephen Allwright about the properties of these two functions and how you should think about them.
* For more information about the Huber loss, take a look at ["Huber Loss: Why Is It, Like How It Is?"](https://www.cantorsparadise.com/huber-loss-why-is-it-like-how-it-is-dcbe47936473).</p></details>

-----------------------

## Date - 2022-06-18


## Title - A dataset of car pictures


### **Question** :

This is the first team that Delilah manages.

She didn't have much experience as a manager, but she knew how important it's for a new team to nail down their process.

A couple of weeks in, the team is working on classifying different car makes and models from pictures captured with smartphones. 

As the first step, Delilah asked the team to ensure that each make and model of the cars they care about is properly represented in both the training and test sets. Delilah knows they need to avoid testing for classes that the model didn't see during training or vice versa.

**Which of the following is the correct term that refers to the process that Delilah asked her team to follow?**


### **Choices** :

- Delilah asked her team to cross-validate their dataset.
- Delilah asked her team to stratify their dataset.
- Delilah asked her team to validate their dataset.
- Delilah asked her team to bootstrap their dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling) is a method of splitting a dataset to produce splits that contain a properly balanced set of samples.

Delilah is working on a classification problem. In her case, stratified sampling will ensure that her training and test sets have approximately the same percentage of class samples as the complete set.

For example, let's assume the dataset has 10,000 pictures of Audis and 30,000 pictures of Hondas. Delilah would like to ensure that the team keeps this ratio when splitting the data into training and test sets. If the group selects 32,000 total images to train a model, Delilah would like to see 8,000 pictures of Audis (25%) and 24,000 of Hondas (75%.)

Therefore, the second choice is the correct answer to this question.

Something else to mention: there's a process called ["Stratified Cross-Validation."](https://towardsdatascience.com/what-is-stratified-cross-validation-in-machine-learning-8844f3e7ae8e) If you knew about this, you might have been tempted to select the first choice as a valid answer. Notice, however, that Delilah is not asking the team to validate a model yet. She is just focused on how to split the dataset.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Stratified sampling in Machine Learning"](https://medium.com/analytics-vidhya/stratified-sampling-in-machine-learning-f5112b5b9cfe) is a quick introduction to stratified sampling.
* Check ["What is Stratified Cross-Validation in Machine Learning?"](https://towardsdatascience.com/what-is-stratified-cross-validation-in-machine-learning-8844f3e7ae8e) for more information about stratified cross-validation.</p></details>

-----------------------

## Date - 2022-06-19


## Title - Helping Charlie with statistics


### **Question** :

There's a lot about machine learning that comes straight from statistics.

Charlie wasn't surprised when she started recognizing terms from her statistics course while reading a newly published paper. 

In statistics, the notion of statistical error is an integral part of hypothesis testing. There are two types of errors when testing the null hypothesis: type I and type II errors. The paper used both terms, and Charlie was already confused.

Let's help her out.

**Which of the following statements is correct about type I errors?**


### **Choices** :

- A type I error occurs when the null hypothesis is true and is not rejected.
- A type I error occurs when the null hypothesis is false but is not rejected.
- A type I error occurs when the null hypothesis is false and is rejected.
- A type I error occurs when the null hypothesis is true but is rejected.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>It makes sense for those who are more used to machine learning terminology to compare type I and type II errors with false positives and false negatives.

Type I errors are the same as false positives. For example, if we mark a valid email as spam, we are in the presence of a false positive. Type I errors are the rejection of a true [null hypothesis](https://www.investopedia.com/terms/n/null_hypothesis.asp) by mistake.

Type II errors are the same as false negatives. For example, if we let a spam message pass as a valid email, we are in the presence of a false negative. This is a type II error because we accept the conclusion of the email being good, even though it is incorrect. Type II errors are the acceptance of a false null hypothesis by mistake.

Based on the above description, the fourth choice is the correct answer to this question: A type I error occurs when the null hypothesis is true but is rejected.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What Is a Null Hypothesis?"](https://www.investopedia.com/terms/n/null_hypothesis.asp) covers the basics you need to understand before going into hypothesis testing.
* Check out ["Type I and type II errors"](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors) for the definition and examples of each type of error. 
* ["Understanding Null Hypothesis Testing"](https://opentextbc.ca/researchmethods/chapter/understanding-null-hypothesis-testing/) is an excellent article about hypothesis testing.</p></details>

-----------------------

## Date - 2022-06-20


## Title - Kendall's interview


### **Question** :

Kendall showed up early to her interview.

She was fresh out of her Master's, and this was going to be her first job after school.

After a few pleasantries, the interviewer started talking about high-variance models and how they typically pay a lot of attention to the training data.

Kendall nodded and smiled while imagining the question that was coming. She didn't have to wait longer.

**Which of the following algorithms can be considered high-variance models?**


### **Choices** :

- Linear Regression
- Decision Trees
- Logistic Regression
- k-Nearest Neighbors (KNN)


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Every machine learning algorithm deals with three types of errors: bias, variance, and irreducible error. We need to focus specifically on the variance error to answer this question.

Here is what [Jason Brownlee](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) has to say about variance: "Variance is the amount that the estimate of the target function will change if different training data was used."

In other words, variance refers to how much the answers given by the model will change if we use different training data. The model has high variance if the answers differ significantly when using different portions of our training dataset.

Generally, non-linear machine learning algorithms with a lot of flexibility are high variance. For example, Decision Trees and k-Nearest Neighbors are high-variance models. Linear models, on the other hand, are usually low-variance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) is Jason Brownlee's article covering bias, variance, and their tradeoff.
* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).
* In case you like the simplicity of Twitter threads, here is one for you about this topic: ["Bias, variance, and their relationship with machine learning algorithms"](https://twitter.com/svpino/status/1390969728504565761).</p></details>

-----------------------

## Date - 2022-06-21


## Title - The elbow method


### **Question** :

After using a clustering algorithm on her dataset, Veronica fell in love with how easy it was to set everything up, run it, and get quick results back. After years of experience with supervised learning methods, K-Means was a breath of fresh air.

It took her some time to go through the documentation, and while reading online, she kept bumping into the "elbow method." It was clear that she needed to understand more about it.

**Which of the following statements is true about the elbow method?**


### **Choices** :

- In cluster analysis, the elbow method is used to determine the existing biases in a dataset.
- In cluster analysis, the elbow method is used to select the outliers in a dataset.
- In cluster analysis, the elbow method is used to determine the optimal number of clusters.
- In cluster analysis, the elbow method is used to determine the features that better explain the patterns in a dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The story doesn't tell us, but I'm sure Veronica discovered something very early: No matter how many clusters she specified, the algorithm was happy to oblige. When she wanted 2 clusters, she got them. When she wanted 10, here they were.

The [elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) is a way to determine the optimal number of clusters. 

Veronica is running K-means. She could run the algorithm on her dataset for a range of values of `k`, and for each value, calculate the sum of squared errors. Then, she could plot a line chart of these errors for each value of `k`. If the line chart [looks like an arm](https://en.wikipedia.org/wiki/Elbow_method_(clustering)#/media/File:DataClustering_ElbowCriterion.JPG), then the "elbow" on the arm is the best number of clusters (`k`) that she should use.

The elbow method is a way to choose the point where diminishing returns are no longer worth the cost. It's a very popular technique when using clustering algorithms.

Therefore, the third choice is the correct answer.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Using the elbow method to determine the optimal number of clusters for k-means clustering"](https://bl.ocks.org/rpgove/0060ff3b656618e9136b) for an explanation of how to use the elbow method.
* ["Elbow method (clustering)"](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) is the Wikipedia page covering the elbow method.
* Check ["Knee of a curve"](https://en.wikipedia.org/wiki/Knee_of_a_curve) for a precise explanation of looking at the "elbow" of a curve.</p></details>

-----------------------

## Date - 2022-06-22


## Title - Claudia needed a walk


### **Question** :

After hours of work, Claudia decided to take a break.

She has been working on a model to classify music into different genres, and while her neural network is doing great during training, its accuracy struggles with the test data.

Claudia took a walk and came back with a fresh perspective. She wrote down a few strategies to approach the problem.

**Which of Claudia's strategies would you say is likely to help her solve the issue?**


### **Choices** :

- Decrease the complexity of the model by cutting down layers and reducing the number of neurons.
- Switch to a different optimization algorithm.
- Regularize the model.
- Increase the amount of training data.


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Claudia's model is overfitting. 

Her model is doing great with the training data but struggling on the test set, which is a sign that her model is not generalizing well to unseen samples.

If Claudia decreases the complexity of the model, she could prevent the neural network from "memorizing" the training samples. Complex models tend to overfit because they have too much capacity. It's easier for them to memorize the training data instead of trying to generalize. Therefore, the first choice is a valid strategy to solve the problem.

Switching the optimization algorithm is unlikely to solve this problem. Nothing in the statement gives us any information about which algorithm Claudia is currently using, and we don't have any reason to believe it's not a good one. We also know that her model is learning the training data, which wouldn't be possible if the optimization algorithm wasn't appropriate.

Claudia could also use regularization techniques to help with overfitting. Regularization discourages the model from becoming too complex or flexible, which helps prevent the model from "memorizing" the training data.

Finally, there's a chance that the training data that Claudia is using doesn't fully capture the breadth of valid samples for her problem. In other words, her training dataset might not be enough to teach the model how to predict the test data correctly. In this case, adding more data to the training set gives Claudia a better chance to build a model that generalizes better.

The first, third, and fourth choices are good strategies to solve Claudia's problem.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting With Machine Learning Algorithms"](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/) for an introduction to overfitting and underfitting in machine learning.
* ["How to Solve Underfitting and Overfitting Data Models"](https://allcloud.io/blog/how-to-solve-underfitting-and-overfitting-data-models/) covers several strategies to solve overfitting and underfitting.</p></details>

-----------------------

## Date - 2022-06-23


## Title - Research vs production


### **Question** :

A common question among people new to artificial intelligence is about the path they would like to pursue.

Many opt for an academic role, where they can spend time doing research. Others prefer getting an industry job, where they get to experience putting machine learning systems into production.

Kathy isn't sure yet, so we will counsel her by highlighting some differences between both paths.

**Select every statement that correctly highlights the differences between machine learning modeling in a research environment versus production machine learning.**


### **Choices** :

- The data used in a research environment is usually static, while the data used in a production setting is dynamic and constantly shifting.
- The design priority in a research environment is usually higher performance—either the highest accuracy or other relevant metrics. Production environments put more emphasis on costs, scalability, and explainability.
- Research is usually not concerned with fairness, but fairness is crucial when building models in a production environment.
- When building machine learning models in a research environment, most of the work goes towards monitoring and maintaining models. In production environments, most of the work centers around the initial training and validation of the model.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Kathy's story is not unique. In my experience, this is one of the most frequent questions I hear from people coming into the industry.

Although this question doesn't cover every difference, it's a good start, so let's start from the top.

Researchers like to test their work on popular datasets. This gives them a fair comparison with other existing methods and ensures its easy to reproduce their results. On the other hand, production data is constantly shifting. The datasets you used to train and test your models can quickly become obsolete. Therefore, the first choice is correct.

A lot of emphasis on academia centers around better algorithms and techniques. Can we solve this particular problem and do it more accurately? Can we do it faster? Research jobs are about inventing new methods and squeezing as much as we can from what we already know.

The focus on better techniques and algorithms leads to the main priority in many research positions: achieving better "performance." It could be about higher accuracy, a faster method, or fewer constraints. These, however, aren't usually the same concerns in the industry.

Production machine learning is generally more focused on the interpretability of results and the cost of the solution. "Higher accuracy" is not the most critical metric in many cases—it's still important but usually not at the expense of other factors. This is one of the main differences between a research position and an industry job. Therefore, the second choice is also correct.

Fairness is crucial. Yes, fairness is fundamental when putting machine learning into production, but it's equally necessary for any research environment. Therefore, the third choice is not correct.

Finally, in production environments, creating an initial model is just a small portion of work. After that, most of the time goes towards monitoring the model's results and maintaining it to combat [concept and data drift](https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift). Conversely, in academia, the goal is usually to build a model. Since data is primarily static, models don't suffer any drift. Therefore, the fourth choice is incorrect.

In summary, the first and second choices are the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The ["Machine Learning Data Lifecycle in Production"](https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production) course in Coursera, part of the [Machine Learning Engineering for Production (MLOps) Specialization](https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops).
* Check ["Why You Should Care About Data and Concept Drift"](https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift) to understand the importance of monitoring machine learning models.
* ["A Gentle Introduction to Concept Drift in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-concept-drift-machine-learning/) is an excellent introduction to concept drift.</p></details>

-----------------------

## Date - 2022-06-24


## Title - Pruning the tree


### **Question** :

Ana has been scratching her head for a while.

She is using a Decision Tree on a dataset, and although her training performance is excellent, the test performance is terrible.

After discussing with her professor, the recommendation was to prune the tree, but Ana didn't understand why this would solve the problem.

**Which statements explain why pruning the tree will solve Ana's problem? Select all that apply.**


### **Choices** :

- By pruning the Decision Tree, Ana will increase the model's bias.
- By pruning the Decision Tree, Ana will increase the model's variance.
- By pruning the Decision Tree, Ana will decrease the model's bias.
- By pruning the Decision Tree, Ana will decrease the model's variance.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A Decision Tree is an algorithm with low bias and high variance. This means that a Decision Tree makes almost no assumptions about the target function.

Because of its high variance, Decision Trees tend to overfit easily to the training dataset. Ana saw this as soon as she tried the model on her test data.

To avoid overfitting, Ana should prune the tree. We force the tree to generalize better and make assumptions by reducing the number of nodes. In other words, by pruning the tree, we increase its bias and decrease its variance.

Therefore, the first and fourth choices are the correct answer.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) is an excellent introduction to the bias and variance tradeoff.
* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).
* In case you like the simplicity of Twitter threads, here is one for you about this topic: ["Bias, variance, and their relationship with machine learning algorithms"](https://twitter.com/svpino/status/1390969728504565761).</p></details>

-----------------------

## Date - 2022-06-25


## Title - Accuracy is coming down


### **Question** :

Natalia is developing a computer vision model to classify images of birds using a CNN. 

She has a dataset of 20,000 images, split 50% into training and testing. The dataset is well balanced, containing an equal amount of bird and non-bird photos.

After some work, Natalia's model gets to around 90% accuracy, but she knows she can do better. It turns out that the model is not good at detecting birds from Madagascar, so Natalia asks her team to collect more data.

After several days, the data collection and labeling team provides 1,000 well-balanced new labeled images, which Natalia splits in half to extend the existing training and testing datasets.

The accuracy of the new model dropped to 88%.

**Which of the following is the approach that Natalia should follow to fix the problem?**


### **Choices** :

- There must be a problem with the labels on the last batch of 1,000 images from Madagascar. Natalia should work with the labeling team to correct the labels.
- The architecture of the CNN model is no longer valid for the new batch of data. Natalia should recalibrate the network to ensure she gets a better result.
- The accuracy here might not tell the story of what's happening. Natalia should evaluate the model on the original 10,000-image test dataset for a valid comparison.
- Better data always leads to a better model, so Natalia should review her code because the accuracy must improve or stay where it was.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Natalia compared the 90% accuracy computed on the 10,000 test images to the 88% accuracy on the augmented test dataset containing 10,500 images. There is a problem with this.

What if the new images from Madagascar are just much more difficult for the network? Even if the model improved, Natalia would not realize it because the additional pictures are causing the overall accuracy to drop.

Here is a hypothetical scenario of two models illustrating how the new model could improve even when returning a lower accuracy:
* Old model's accuracy on the 10,000 test images: 90% (9,000 images correctly classified.)
* Old model's accuracy on the new 500 test images: 10% (50 images correctly classified.)
* New model's accuracy on the 10,000 test images: 91% (9,100 images correctly classified.)
* New model's accuracy on the new 500 test images: 28% (140 images correctly classified.)
* New model's accuracy on the 10,500 test images: 88% (9,240 images correctly classified.)

In this hypothetical example, the new model is better on the original 10,000 test images (from 90% to 91%) and the 500 new images (from 10% to 28%.) However, the approach used by Natalia to evaluate the results made us believe the model got worse (going down from 90% to 88% accuracy.)

This is called the [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox). When comparing two experiments, we need to make sure that we evaluate the same data or at least data having the same underlying distribution. If we change the distribution, we can't compare the results anymore and may take the wrong conclusion. Therefore, the third choice is the correct answer.

The wrong labels or architecture could be reasons for the model performing worse. However, we need to check if the model is performing worse in the first place, so assuming this is the case is premature.

The fourth choice is also not correct. Adding more data doesn't automatically mean better results.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Simpson's Paradox"](https://www.britannica.com/topic/Simpsons-paradox) for a complete explanation about the paradox.
* If you prefer videos, [watch this video](https://blog.chriszacharias.com/page-weight-matters) about the Simpson's Paradox.</p></details>

-----------------------

## Date - 2022-06-26


## Title - Rolling two dice


### **Question** :

I have a bag in front of me containing a 6-sided and a 12-sided dice.

My friend pulls one out at random, rolls it, and tells me that the result is 3. 

**What is the probability that my friend pulled out the 6-sided die?**


### **Choices** :

- The probability is 1/3
- The probability is 1/2
- The probability is 2/3
- The probability is 3/4


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's assume that `A` represents the event of rolling the die and getting a `3`, `B1` represents the event of pulling out the 6-sided die, and `B2` represents the event of pulling out the 12-sided die.

We can compute the probability of my friend holding the 6-sided die using the [Bayes theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem):

```
P(B1|A) = (P(A|B1)*P(B1))/P(A)
P(B1|A) = (1/12)/P(A)

P(A) = P(A∣B1)*P(B1)+P(A∣B2)*P(B2)
P(A) = 1/12 + 1/24​
P(A) = 1/8

P(B1|A) = (1/12)/(1/8)
P(B1|A) = 2/3​
```

Thus, the answer is `2/3`.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["A Gentle Introduction to Bayes Theorem for Machine Learning"](https://machinelearningmastery.com/bayes-theorem-for-machine-learning/) is a great starting point to understand the Bayes theorem and how to use it.
* You can also check the Wikipedia page of [the Bayes theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem).</p></details>

-----------------------

## Date - 2022-06-27


## Title - The attributes of a tensor


### **Question** :

A "tensor" is one of the most basic data structures used in machine learning systems. 

Let's go back to basics and focus on the fundamental characteristics of a tensor.

**Which of the following are valid attributes that represent a tensor?**


### **Choices** :

- Its number of axes. This attribute is also called the "rank" of the tensor.
- Its cardinality. This attribute represents the numerical relationship between the axes of the tensor.
- Its shape. This attribute represents how many dimensions the tensor has along each axis.
- Its data type. This attribute represents the type of values contained in the tensor.


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Three primary attributes define a tensor:

1. Its rank, or the number of axes.
2. Its shape, or the number of dimensions per axis.
3. Its data type, or the type of data contained in it.

The rank of a tensor refers to the tensor's number of axes.

Examples:
* Rank of a matrix is 2.
* Rank of a vector is 1.
* Rank of a scalar is 0.

The shape of a tensor describes the number of dimensions along each axis.

Examples:
* `()` — scalar
* `(2,)` — vector 
* `(3, 2)` — matrix
* `(3, 2, 5)` — 3D tensor

The data type of a tensor refers to the kind of data contained in it.

Examples:
* `float32`
* `float64`
* `uint8`
* `int64`

The second choice mentions "the cardinality of a tensor" as "the numerical relationship between the axes of the tensor." This is not a correct answer. 

In summary,  the correct answer to the question is the first, third, and fourth choices.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Deep Learning with Python, Second Edition](https://amzn.to/3K3VZoy) covers the topic of tensors really well.
* Check ["A Gentle Introduction to Tensors for Machine Learning with NumPy"](https://machinelearningmastery.com/introduction-to-tensors-for-machine-learning/) for a quick introduction to tensors and practical code.</p></details>

-----------------------

## Date - 2022-06-28


## Title - Convolutions are the answer


### **Question** :

Jane is arguing with her colleague.

They are creating a neural network model to classify images uploaded to the company website. 

Jane wants to use a Convolutional Neural Network, but her colleague disagrees. Instead, he wants to keep things simple and use a fully-connected neural network. 

Jane feels that a Convolutional Neural Network is the right tool for the job, but how can she convince her colleague?

**What are the advantages of using a Convolutional Neural Network in this case?**


### **Choices** :

- Convolutional Neural Networks are usually shallower than fully-connected networks, making the training process easier and faster.
- Using a Convolutional Neural Network requires fewer parameters than a fully-connected network.
- The local structure of the image can be used more efficiently by a Convolutional Neural Network, resulting in much better features and performance.
- Convolutional Neural Networks can learn a hierarchy of visual features similar to the human brain, which results in better performance.


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A [Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN) is a much better choice when dealing with image classification problems than a fully-connected network.

The first choice is not correct. A CNN is more time and memory efficient than a fully-connected network, so we can use deeper networks with many layers, which is impossible in a fully-connected network. You should always expect CNNs to be deeper than fully-connected networks.

CNNs require fewer parameters than fully-connected networks and reuse the same set of parameters over the whole image. The number of weights in a convolutional layer depends on the kernel size and number of channels and not on the image resolution. For example, in the [AlexNet architecture](https://en.wikipedia.org/wiki/AlexNet), the five convolutional layers are responsible for only 4% of the parameters of the network. In comparison, the final three fully-connected layers contain the remaining parameters. This makes the second choice a correct answer.

One reason CNNs are very effective in dealing with pictures is that they exploit the local structure of images. Since pixels located next to each other tend to be related, convolutional kernels can capture this information, making the third option a correct answer.

Finally, a CNN can learn visual features of increasing complexity. The initial layers typically learn to recognize low-level details, like edges and colors, while deeper layers can handle more complex structures like corners and patterns. The final layers of a CNN can learn complex representations like faces or any complex objects. Therefore, the fourth choice is correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["A Beginner’s Guide to Convolutional Neural Networks (CNNs)"](https://towardsdatascience.com/a-beginners-guide-to-convolutional-neural-networks-cnns-14649dbddce8) is a great introduction to CNNs.
* If you want to get deeper into how convolutions work, check out ["A guide to convolution arithmetic for deep learning"](https://arxiv.org/pdf/1603.07285.pdf).</p></details>

-----------------------

## Date - 2022-06-29


## Title - The black sheep


### **Question** :

Here you have a list containing three popular deep learning architectures.

But there's one black sheep. One of these is not a deep learning architecture.

**Can you determine which of the following doesn't belong on this list?**


### **Choices** :

- ImageNet
- AlexNet
- GoogleNet
- VGG16


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>While [AlexNet](https://www.analyticsvidhya.com/blog/2021/03/introduction-to-the-architecture-of-alexnet/), [GoogleNet](https://arxiv.org/abs/1409.4842v1), and [VGG16](https://arxiv.org/abs/1409.1556) are deep learning architectures, [ImageNet](https://www.image-net.org/) is an image database with millions of images, so it has nothing to do with the rest.

The first choice is the correct answer.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Introduction to The Architecture of Alexnet"](https://www.analyticsvidhya.com/blog/2021/03/introduction-to-the-architecture-of-alexnet/).
* ["Going Deeper with Convolutions"](https://arxiv.org/abs/1409.4842v1) is the paper introducing GoogleNet.
* ["Very Deep Convolutional Networks for Large-Scale Image Recognition"](https://arxiv.org/abs/1409.1556) is the paper introducing VGG16.
* Check the [ImageNet](https://www.image-net.org/) website for more information about one of the most popular public datasets in the world.</p></details>

-----------------------

## Date - 2022-06-30


## Title - 120 shirt combinations


### **Question** :

Amanda wants to buy the perfect shirt.

She knows somebody at a warehouse, so she pulls a favor one Sunday afternoon, and to her amusement, they have thousands of different shirts. 

If Amanda only cared about a size that fits her, and there are four different sizes, she could buy one right after trying four shirts.

But if Amanda also cared about the color, and there are ten different colors, Amanda would have to try `4 x 10 = 40` combinations to find the perfect shirt.

What would happen if she also wanted to take the material into account? Assuming three different types, Amanda could only find the perfect shirt after trying `4 x 10 x 3 = 120` shirts.

All of a sudden, Amanda is overwhelmed.

**Which of the following ideas explains what's happening to Amanda?**


### **Choices** :

- Occam's Razor
- No Free Lunch Theorem
- Curse of Dimensionality
- Universal Approximation Theorem


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Occam's Razor is the idea that, given two solutions with similar characteristics, the simplest and most direct one is the correct answer. This idea is unrelated to Amanda's problem, so it's not the right choice.

The No Free Lunch Theorem implies that no single algorithm is universally the best-performing algorithm for all problems in machine learning. This idea is also unrelated to what's happening here.

The Universal Approximation Theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate any continuous function. Not a correct choice either.

Finally, the Curse of Dimensionality refers to various phenomena when working with data in high-dimensional spaces. It states that, as the dimensionality of the data increases, the amount of data needed to train a learning algorithm grows exponentially.

That's what's happening to Amanda: as she wants to consider more shirt attributes, it becomes more problematic to pick the perfect shirt.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check out ["Curse of Dimensionality"](https://en.wikipedia.org/wiki/Curse_of_dimensionality) in Wikipedia.
* For a deeper dive, check ["What is the Curse of Dimensionality?"](https://deepai.org/machine-learning-glossary-and-terms/curse-of-dimensionality).</p></details>

-----------------------

## Date - 2022-07-01


## Title - No high-bias models


### **Question** :

Lyla's project wasn't a walk in the park. She had a large dataset and had to deal with more than 20 relevant features.

A fundamental topic in machine learning is bias, variance, and their relationship with learning algorithms. 

High bias models typically include more assumptions about the target function, while low bias models incorporate fewer assumptions about the target function. 

Lyla knows she needs to stay away from high-bias models. 

**Which of the following algorithms should Lyla avoid?**


### **Choices** :

- Linear Regression
- Logistic Regression
- Decision Trees
- k-Nearest Neighbors


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Every machine learning algorithm deals with three types of errors: bias, variance, and irreducible error. We need to focus specifically on the bias error to answer this question.

Here is what [Jason Brownlee](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) has to say about bias: "Bias are the simplifying assumptions made by a model to make the target function easier to learn."

In other words, bias refers to the assumptions the model makes to simplify the process of finding answers. The more assumptions it makes, the more biased the model is.

Often, linear models are high-bias. They are easier to understand but make too many assumptions about the target function, preventing them from performing well on complex problems. Linear and logistic regression are two examples of high-bias models.

Nonlinear models are usually low-bias. Decision Trees and k-Nearest Neighbors are two examples.

Therefore, the first and second choices answer this question correctly.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Here is Jason Brownlee's article I mentioned before: ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/).
* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).</p></details>

-----------------------

## Date - 2022-07-02


## Title - Binary cross-entropy for the first time


### **Question** :

Yasmin heard about binary cross-entropy for the first time at work.

Despite being used to working with different machine learning models, it didn't take her long to understand how this loss function works.

After reading the math behind the scenes, there was only one question left for Yasmin to answer.

**Which neural network problems should use binary cross-entropy as their loss function?**


### **Choices** :

- Binary cross-entropy is the loss function used for multi-label classification problems.
- Binary cross-entropy is the loss function used for multi-class classification problems.
- Binary cross-entropy is the loss function used for binary classification problems.
- Binary cross-entropy is the loss function used for regression problems.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Binary cross-entropy is the loss function we use when training binary classifiers. When working on a binary classification task, we categorize every sample into two classes. For example, given images of dogs or cats, a binary classifier will decide whether a picture shows a dog or a cat.

But binary classifiers aren't the only time we use binary cross-entropy.

Binary cross-entropy is also the loss function we use in multi-label classification problems. These are problems where we categorize every sample into one or more classes. For example, organizing movies based on the type of content they show: violence, adult language, smoking, or sex, where each film could belong to one or more categories.

In multi-label classification models, the output layer returns values independent from each other. It's helpful to think of a model that outputs ten possible classes as a combination of ten different binary classifiers, and thus binary cross-entropy helps.

Neither multi-class classification nor regression problems use binary-cross entropy.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Understanding binary cross-entropy / log loss: A visual explanation"](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) is an excellent introduction to binary cross-entropy.
- Another article that helps understand binary cross-entropy is ["Binary Cross Entropy/Log Loss for Binary Classification"](https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/).</p></details>

-----------------------

## Date - 2022-07-03


## Title - Happy kitties


### **Question** :

Understanding how deep neural networks work is essential for anyone trying to become proficient at using them.

We don't know precisely how deep neural networks break down an image and distribute different patterns across the layers, but we have some general ideas.

**Imagine that we are working with pictures of happy kitties. Which of the following statements reasonably simplifies how the network works?**


### **Choices** :

- The deeper layers of a neural network compute more complex features than earlier layers.
- The earlier layers of a neural network compute more complex features than deeper layers.
- The complexity of the features computed by the neural network layers is proportionally distributed among all layers.
- The complexity of the features computed by the neural network layers has no relationship with the layer's position.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Focus on a picture of a kitty. You'll see the eyes, nose, mouth, ears, fur, and other characteristics. Notice how groups of pixels form edges, shapes, and the rest of the cat's features. 

A reasonable explanation for how a neural network works is to assume that earlier layers focus on detecting more basic features, like edges and shapes of the image. Later layers could use these earlier pieces to form more recognizable shapes like the eyes and nose of the cat.

While the network deals with pixels early on, the deeper we go into it, the more it will work with complete patterns until it reaches the output layer.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["But what is a neural network?"](https://www.youtube.com/watch?v=aircAruvnKk) is Grant Sanderson's introduction to neural networks on YouTube. Highly recommended!
* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html) is a free online book written by [Michael Nielsen](https://twitter.com/michael_nielsen).</p></details>

-----------------------

## Date - 2022-07-04


## Title - Euclidean distance


### **Question** :

Many machine learning algorithms need a way to measure the similarity between observations.

For example, when using an algorithm like [K-Means](https://en.wikipedia.org/wiki/K-means_clustering), we need a way to determine how close different samples are in our dataset before deciding whether those samples belong in the same cluster.

There are different metrics to compute the distance between observations, and one of the most popular ones is the Euclidean distance.

**From the following list, select every correct statement about the Euclidean distance.**


### **Choices** :

- The Euclidean distance is a way to compute the distance between two points in two-dimensional spaces. The Euclidean distance doesn't work in multidimensional spaces.
- The Euclidean distance between two points does not depend on which of the two points is the start and which is the destination. In other words, the distance between _p_ and _q_ is the same as between _q_ and _p_.
- The Euclidean distance between two distinct points is always positive.
- Traveling from a point _p_ to a point _q_ via a point _r_ cannot be any shorter than traveling directly from _p_ to _q_.


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We can use the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) in one or more dimensions. For example, in a line, the distance between two points is the numerical difference between their coordinates. In a plane, the distance is the [Pythagorean distance](https://en.wikipedia.org/wiki/Pythagorean_theorem). 

But can we use it in multidimensional spaces?

The answer is yes; the Euclidean distance works in [multidimensional spaces](https://hlab.stanford.edu/brian/euclidean_distance_in.html). Intuitively, this should make sense because we can use it as the metric to compute the distance between multi-feature observations in our dataset. Therefore, the first choice is incorrect.

The distance from a point _p_ to another point _q_ is the same regardless of whether we start from _p_ or _q_. This distance is always a positive value as long as _p_ and _q_ are different points. If _p_ and _q_ are the same point, the distance is 0.

In the [Euclidean plane](https://en.wikipedia.org/wiki/Euclidean_space), the distance between any two distant points is the length of the line segment joining them. So this segment joining points _p_ and _q_ can't be any shorter, regardless of whether we get from _p_ to _q_ via a third point _r_.

In summary, the last three choices are correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Euclidean Distance In 'n'-Dimensional Space"](https://hlab.stanford.edu/brian/euclidean_distance_in.html) for a summary and visualization of how the Euclidean distance works in multi-dimensional spaces.
* ["Euclidean distance"](https://en.wikipedia.org/wiki/Euclidean_distance) and ["Euclidean space"](https://en.wikipedia.org/wiki/Euclidean_space) are Wikipedia articles that will help with this topic.</p></details>

-----------------------

## Date - 2022-07-05


## Title - A classifier nobody saw coming


### **Question** :

After several experiments, it was clear that something was happening: the model wasn't returning good predictions on the test data.

The team was tired and ready to get back to the drawing board to try and find a different approach to solve the problem, but Raelynn had an idea.

She took the training dataset, removed the target variable, and mixed it with all their available test data. She then created a new binary target and set it to `1` for every test sample and `0` for every training sample.

"Run a classifier on this dataset, and let me know how good the predictions are," she asked.

**Her team seemed confused. What is Raelynn trying to do?**


### **Choices** :

- This classifier will estimate the performance of the team's model on unseen test data. It will help the team understand whether they are overfitting or underfitting the training data.
- This classifier will turn the initial problem into a more straightforward approach that will serve as a baseline for the team to continue their work and find a better solution.
- This classifier will estimate how different the training data is from the test data, potentially explaining why the team is struggling.
- Combining the training and test data is never a good idea, so this classifier's results will not be valid.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Raelynn's approach should be familiar if you have experience participating in Kaggle competitions. 

Popular validation techniques, like cross-validation, allow you to test your models on unseen data, as long as that data comes from the same distribution as your training dataset. Unfortunately, that's not always the case, and even slight differences between the training and test data will considerably affect the result of your model.

[Adversarial validation](https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation/notebook) is a technique to estimate the degree of difference between your training and test data. [The Kaggle Book](https://amzn.to/3kbanRb) introduces it as follows:

> [adversarial validation] was long rumored among Kaggle participants and transmitted from team to team until it emerged publicly thanks to a post by Zygmunt Zając on his FastML blog.

Let's think about what Raelynn did. She created a new dataset by joining the training and test data. The target of that new dataset is a binary variable differentiating the training and test samples. She can determine how easy it's to separate both datasets by running a classifier on that new data.

Adversarial validation relies on computing the [ROC-AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc), a graph showing the True Positive Rate and the False Positive Rate at different classification thresholds. The area under this curve (AUC) measures the model's performance. A perfect model will have an area of `1.0`, while a model that only makes mistakes will have an area of `0.0`.

If they run the classifier and the ROC-AUC is around `0.5`, Raelynn will know that the training and test data are not easily distinguishable, which is good because it means the data comes from the same distribution. If the ROC-AUC is too high—closer to `1.0`—the classifier can tell training and test data apart, which means they come from a different distribution.

Adversarial validation is a very clever technique. The result could help explain why the team is struggling and guide it on how to continue.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Adversarial validation"](https://blog.zakjost.com/post/adversarial_validation/) is a great introduction to adversarial validation.
* Check ["What is Adversarial Validation?"](https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation/notebook) for a discussion about this technique in Kaggle.
* [The Kaggle Book](https://amzn.to/3kbanRb) is an amazing reference for those looking to participate in Kaggle.</p></details>

-----------------------

## Date - 2022-07-06


## Title - Tensors and trolls


### **Question** :

Yesterday was Samantha's first day, and she started learning how to use a deep learning library.

Unsurprisingly, "Tensors" was the first concept she needed to understand. Samantha read everything she could about them and is now ready to write a short article with everything she learned.

But she knows that making a mistake online will bring people out of the woodwork to troll her forever.

**Samantha has to get this right. Which of the following are accurate descriptions of tensors of different ranks?**


### **Choices** :

- A vector that contains only one number is called a "scalar" or a rank-0 tensor. An example of a scalar is any numeric value like a person's age or today's temperature.
- An array of numbers is called a "vector" or a rank-1 tensor. An example of a vector is an array containing the age of every person represented in a dataset.
- An array with two dimensions is called a matrix or a rank-2 tensor. An example is an array containing the age and the sex of every person represented in a dataset.
- An array with three dimensions is a rank-3 tensor. An example is a 3D array containing the information of an image: its width, height, and the number of channels.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's get something out of the way: This question is about tensors as we use them in deep learning, not the mathematical definition of a tensor.

The rank of a tensor refers to its number of axes—or dimensions. For example:

* The rank of a scalar is 0.
* The rank of a vector is 1.
* The rank of a matrix is 2.
* The rank of a 3D tensor is 3.

A scalar, or 0D tensor, has a rank of 0 and contains a single number. These are also called "0-dimensional tensors." Therefore, the first choice is correct.

A vector, or 1D tensor, has a rank of 1 and represents an array of numbers. Therefore, the second choice is also correct.

A matrix, or 2D tensor, has a rank of 2 and represents an array of vectors. We refer to the two axes of a matrix as "rows" and "columns." Therefore, the third choice is also correct.

You can obtain higher-dimensional tensors (3D, 4D, etc.) by packing lower-dimensional tensors in an array. For example, packing a 2D tensor in an array gives you a 3D tensor. 

For example, to store a color image, we need three dimensions: one representing the image's width, another representing the height, and a final dimension for the color channels. Assuming we have three channels (red, blue, and green), you can think of having three different matrices, each containing the pixels for each one of the channels. Therefore, the fourth choice is also correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Deep Learning with Python, Second Edition](https://amzn.to/3K3VZoy) covers the topic of tensors really well.
* Check ["A Gentle Introduction to Tensors for Machine Learning with NumPy"](https://machinelearningmastery.com/introduction-to-tensors-for-machine-learning/) for a quick introduction to tensors and practical code.</p></details>

-----------------------

## Date - 2022-07-07


## Title - Start with Decision Trees


### **Question** :

When I started with machine learning, the first model I built was a Decision Tree.

As a long-time software developer, Decision Trees made perfect sense. It took me little time to understand the basics and build something useful to solve an example problem.

Since then, I always recommend newcomers start with Decision Trees. There are simpler models, but I think they strike a perfect balance between power and complexity.

**Which of the following would you categorize as an advantage of using a single Decision Tree?**


### **Choices** :

- The predictions generated by a Decision Tree are easy to interpret and explain.
- A Decision Tree is very resistant to overfitting.
- You can use a Decision Tree to solve regression and classification tasks.
- A Decision Tree doesn't require tuning to get good predictions.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>As a developer, [Decision Trees](https://en.wikipedia.org/wiki/Decision_tree_learning) have always made sense. 

I'm not talking about the process of building a good Decision Tree but about how they work to find predictions. In a short sentence, a Decision Tree is just a bunch of nested conditions that get us to the final solution.

And this property is precisely what makes them very popular: we can look at a prediction and fully understand why the model arrived at that conclusion. In other words, the predictions generated by Decision Trees are easy to interpret, an essential advantage of using Decision Trees over more obscure techniques, like neural networks or Support Vector Machines.

Decision Trees, unfortunately, are prone to overfitting if we don't take careful care of their depth. In other words, unless we ensure our tree doesn't go too deep, it will tend to fit noisy samples and output the wrong prediction. We can control overfitting in a Decision Tree by a process called "pruning."

Remember that while a single Decision Tree is prone to overfitting, using an ensemble of trees is more resistant. Here is a quote from "[To Boost or not to Boost: On the Limits of Boosted Neural Networks](https://arxiv.org/pdf/2107.13600.pdf)":

> [these experiments] confirm that training single large decision trees is prone to overfitting while boosted ensembles of decision trees are resistant to overfitting.

Many people relate Decision Trees with classification tasks, but they are also valuable for solving regression tasks. A classification task is when the predicted outcome is a discrete class, while the result of a regression task is a Real number. This flexibility makes Decision Trees very useful.

Finally, we discussed above how Decision Trees are prone to overfit if we aren't careful with their depth. This is just one of the hyperparameters that we can tune. Like with most techniques, Decision Trees require experimentation and tuning to improve the quality of their results.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["To Boost or not to Boost: On the Limits of Boosted Neural Networks"](https://arxiv.org/pdf/2107.13600.pdf), the paper cited above comparing the overfitting tendency of a single Decision Tree versus an ensemble of trees.
* ["Decision tree pruning"](https://en.wikipedia.org/wiki/Decision_tree_pruning) covers the process of pruning a tree to reduce overfitting.
* Check ["Random Forest"](https://en.wikipedia.org/wiki/Random_forest) for more information about a widely used class of Decision Tree ensembles.</p></details>

-----------------------

## Date - 2022-07-08


## Title - What model is she using?


### **Question** :

Alicia's team was having a hard time with their model.

Everything was going according to plan until the customer showed up with more data. 

The team decided to retrain the model with the new data, and to their surprise, they discovered that the test error increased significantly during evaluation.

They kept training the model with different portions of the data and testing it with their test dataset. Still, none of the models was good enough: the model was learning something different depending on the portion of the data that Alicia's team used.

**If you were to guess the model that Alicia's team is building, which of the following would be your picks?**


### **Choices** :

- Alicia is probably using a Linear Regression model.
- Alicia is probably using a Logistic Regression model.
- Alicia is probably using a k-Nearest Neighbors model.
- Alicia is probably using a Decision Tree model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The problem description has an essential clue: The testing error is high as we use different data portions to train the model. The various models can't generalize.

High-variance models usually suffer from this problem.

Here is what [Jason Brownlee](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) has to say about variance: "Variance is the amount that the estimate of the target function will change if different training data was used."

As we change the training data, a low-variance model should still return good predictions on the test data, but Alice's model doesn't. 

Often, linear models are low-variance, and nonlinear models are high-variance. This means that Alicia is probably using a k-Nearest Neighbors or a Decision Tree model since they are high-variance models.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Here is Jason Brownlee's article I mentioned before: ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/).
* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).</p></details>

-----------------------

## Date - 2022-07-09


## Title - Hot equipment


### **Question** :

Sarah is building a model to recognize pieces of equipment running too hot in a warehouse. She has a large enough dataset of thermal images, and she is using a deep learning model to build a classifier.

The problem is that the dataset is severely imbalanced, with most images showing normal temperatures. Sarah's model results aren't good enough.

**Which of the following are helpful strategies to help Sarah deal with imbalanced datasets?**


### **Choices** :

- Augment the minority class—images showing hot equipment—with synthetically-generated data.
- Reduce the training time to prevent the learning algorithm from overfitting.
- Resample the dataset by either oversampling the minority class or undersampling images showing normal temperatures.
- A deep learning model is not a good solution to solve this problem.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>From the list of choices, two strategies could help Sarah overcome the problem: augmentation and resampling.

Sarah's model is struggling because the dataset is severely imbalanced: it doesn't have too many pictures showing equipment running hot. We could alleviate this problem by adding more photos of this minority class. The first choice proposes adding synthetically-generated images, which is usually a cheaper option to generate a lot of valid data. Augmenting the dataset will allow Sarah to balance both classes, which will help her classifier.

The second strategy that could also help is resampling the dataset. Instead of training with the data as-is, Sarah could oversample the photos showing equipment running hot. For example, she could duplicate every hot image. Sarah could also undersample the normal images; for instance, she could only take every other picture.

Reducing the training time is not a helpful strategy for this problem. Also, nothing on the problem statement would suggest that deep learning is not a good approach.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Random Oversampling and Undersampling for Imbalanced Classification"](https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/) for an introduction to oversampling and undersampling.
* ["Failure of Classification Accuracy for Imbalanced Class Distributions"](https://machinelearningmastery.com/failure-of-accuracy-for-imbalanced-class-distributions/) covers the problem of using accuracy as the metric in imbalanced classification problems.
* ["Learning from imbalanced data"](https://www.jeremyjordan.me/imbalanced-data/) discusses a number of considerations and techniques for dealing with imbalanced data.</p></details>

-----------------------

## Date - 2022-07-10


## Title - Self-supervised learning


### **Question** :

Julia regularly works with supervised and unsupervised learning methods.

However, she keeps seeing new methods that use self-supervised learning techniques. Julia doesn't understand how these methods compare to supervise and unsupervised learning, so she is here to ask for your help.

**Which of the statements on self-supervised learning are true?**


### **Choices** :

- Self-supervised learning is just a fancy name for unsupervised learning.
- Similar to supervised learning, self-supervised methods can judge if their prediction is correct or not during training.
- Like unsupervised learning, self-supervised methods don't require labeled training data.
- Self-supervised methods require only a small number of labeled samples.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Self-supervised methods are considered a form of unsupervised learning. However, not all unsupervised methods are self-supervised. This reasoning allows us to exclude the first choice.

Self-supervised methods have similarities to both supervised and unsupervised learning. They don't require labeled data, which makes them similar to unsupervised learning. On the other hand, in self-supervised methods, "supervision" can be derived directly from the data. Therefore, like supervised learning, we can judge whether a prediction is true or false.

An example would be a neural network that predicts the next word given a part of a sentence. We can take the text of a book and create random samples by picking parts of sentences. We don't need any labels, but for every instance, we still know what the correct answer is. Therefore, the second and the third choice are correct.

The last choice is the definition of semi-supervised learning, which is different than self-supervised learning and is therefore incorrect.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Self-supervised learning"](https://project.inria.fr/paiss/files/2018/07/zisserman-self-supervised.pdf) is a presentation by Andrew Zisserman covering self-supervised learning for images.
* ["Self-supervised learning: The dark matter of intelligence"](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/) is an excellent post from Meta's AI Research team.</p></details>

-----------------------

## Date - 2022-07-11


## Title - Pictures and tensors


### **Question** :

I've been teaching a friend a little bit of computer vision.

Well, I'll be honest, I'm focusing directly on deep learning and how to use pre-trained models to tell cat pictures apart from dog pictures.

We have a dataset of 1,000 color pictures of size 512 x 512. Five hundred of those are images of cats, and the other half are images of dogs.

Soon after we started looking into the problem, he asked a question that I hadn't thought about in a long time.

**What's the correct shape of a tensor capable of storing all of this data at once?**


### **Choices** :

- We can store it in a tensor of shape `(1000, 512, 512)`
- We can store it in a tensor of shape `(512, 512, 3)`
- We can store it in a tensor of shape `(1000, 512, 512, 3)`
- We can store it in a tensor of shape `(500, 512, 512, 1)`


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When working with images, we need to store three components: the height, the width, and the color depth. Color images have three color channels (one for red, one for green, and one for blue), and grayscale images have a single color channel.

In other words, assuming our pictures are of size 512 x 512, we need to store the 3 RGB values for each pixel in the image, so we need a tensor of shape `(512, 512, 3)`. If we don't care about colors, we could keep every pixel of a single image in a tensor of shape `(512, 512)`. 

But even when working with grayscale images, we always add a dimension to store the color depth by convention. That's nice because we can use the same structure for grayscale and color images. If we don't care about color, we could store one picture in a `(512, 512, 1)` tensor. Since we are working with color pictures, our tensor will be `(512, 512, 3)` to accommodate the three channels.

Great! So far, we know how to store a single color picture of size 512 x 512. How about keeping 1,000 of them? We need another dimension: `(1000, 512, 512, 3)`. 

The order I chose for each dimension is intentional. By convention, we structure tensors that hold images the following way: (samples, height, width, channels). There's a different convention where channels go in front of the dimensions of the images, but most people use them at the end.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Deep Learning with Python, Second Edition](https://amzn.to/3K3VZoy) covers the topic of tensors really well.
* Check ["A Gentle Introduction to Tensors for Machine Learning with NumPy"](https://machinelearningmastery.com/introduction-to-tensors-for-machine-learning/) for a quick introduction to tensors and practical code.</p></details>

-----------------------

## Date - 2022-07-12


## Title - Betty's surprise


### **Question** :

Discovering that Gradient Descent has different names based on how many samples it uses to calculate the error was something Betty wasn't expecting.

There are three popular variations of the algorithm: Mini-Batch Gradient Descent, Stochastic Gradient Descent, and Batch Gradient Descent.

Betty found this fascinating, and although it made sense, she always found that Stochastic Gradient Descent was the hardest for her to define.

**Which of the following statements is true about this version of the algorithm?**


### **Choices** :

- Stochastic Gradient Descent determines the optimal amount of data required to compute the gradient of the cost function.
- Stochastic Gradient Descent uses a single sample of data during every iteration.
- Stochastic Gradient Descent uses all available data once during every iteration.
- Stochastic Gradient Descent uses a batch of data (more than one sample but fewer than the entire dataset) during every iteration.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Here is a simplified explanation of how Gradient Descent works: We take samples from the training dataset, run them through the model, and determine how far our results are from what we expect. We then use this "error" to compute how much we need to update the model weights to improve the results.

A critical decision we need to make is how many samples we use on every iteration to run through the model. We have three choices: 
* Use a single sample of data.
* Use all of the data at once.
* Use some of the data.

Using a single sample of data on every iteration is called "Stochastic Gradient Descent" or SGD. In other words, the algorithm uses one sample to compute the updates. 

Using all the data at once is called "Batch Gradient Descent." After processing every sample, the algorithm takes the entire dataset and computes the updates. 

Finally, using some of the data—more than one sample but fewer than the entire dataset—is called "Mini-Batch Gradient Descent." The algorithm works like Batch Gradient Descent, with the only difference that we use fewer samples.

Therefore, the second choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["An overview of gradient descent optimization algorithms"](https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants) for a deep dive into gradient descent and every one of its variants.
* ["Gradient Descent For Machine Learning"](https://machinelearningmastery.com/gradient-descent-for-machine-learning/) is another great introduction to gradient descent.
* ["A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size"](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) covers Batch and Mini-Batch Gradient Descent.</p></details>

-----------------------

## Date - 2022-07-13


## Title - Arya's confusion


### **Question** :

Arya is always confused about this.

We have multi-class classification problems when every sample in the dataset belongs to one class. We have multi-label classification problems when every instance belongs to one or more categories.

These types of problems look similar, but they are very different.

Arya never remembers the correct loss function to train a neural network to solve these problems.

**Which two of the following statements are correct?**


### **Choices** :

- Binary cross-entropy is the loss function used for multi-label classification problems.
- Binary cross-entropy is the loss function used for multi-class classification problems.
- Categorical cross-entropy is the loss function used for multi-label classification problems.
- Categorical cross-entropy is the loss function used for multi-class classification problems.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The goal of the output layer of a [multi-class classification](https://en.wikipedia.org/wiki/Multiclass_classification) model is to return the class that better represents the network's input. Softmax is ideal because it uses the output score to produce a probability for each category. 

The categorical cross-entropy loss function quantifies the difference between two probability distributions. When working on multi-class classification tasks, the categorical cross-entropy is the perfect loss function to pair with a softmax output layer.

In the case of [multi-label classification](https://en.wikipedia.org/wiki/Multi-label_classification) models, our output layer should return values independent from each other. Sigmoid is perfect because it converts the output scores to a value between 0 and 1.

Multi-label classification problems borrow the same principles from binary classification problems. The difference is that we end up with multiple sigmoid outputs instead of one. It's helpful to think of a model that outputs ten possible classes as a combination of ten different binary classifiers. Remember the loss function we use to train binary classification models? Binary cross-entropy. 

In summary, multi-class classification models should use a softmax output with the categorical cross-entropy loss function. Multi-label classification models should use a sigmoid output and the binary cross-entropy loss function.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Binary crossentropy"](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/binary-crossentropy) for an explanation of how Binary Cross-Entropy works.
* Check ["Categorical crossentropy"](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy) for an explanation of how Categorical Cross-Entropy works.</p></details>

-----------------------

## Date - 2022-07-14


## Title - Only one hidden layer


### **Question** :

Sarah wants to add a few slides about neural networks as part of a new course she's been working on for the past month.

Most of her students don't have a machine learning background, so Sarah wants to ensure she uses accurate and simple language during her presentation to avoid confusing them.

She plans to show an example of how a neural network with a single hidden layer works, but she is having difficulty choosing the right way to refer to this model.

**Which of the following is the correct way to talk about a neural network with a single hidden layer?**


### **Choices** :

- A deep neural network
- A recurrent neural network
- A hidden neural network
- A shallow neural network


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We refer to neural networks that use a few hidden layers between the input and output as "shallow neural networks." There's no specific number for how many hidden layers is too many, but one single layer makes Sarah's model a shallow network.

A deep neural network consists of multiple layers between the input and output layers. The more layers we add, the "deeper" the network becomes.

The term "hidden neural network" doesn't exist, and while recurrent neural networks exist, Sarah's network is not one of them.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["But what is a neural network?"](https://www.youtube.com/watch?v=aircAruvnKk) is Grant Sanderson's introduction to neural networks on YouTube. Highly recommended!
* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html) is a free online book written by [Michael Nielsen](https://twitter.com/michael_nielsen).</p></details>

-----------------------

## Date - 2022-07-15


## Title - Eden's schedule


### **Question** :

Eden is building a deep learning model, but she is not having a lot of success getting it to converge.

One idea she wants to explore is to use a learning rate scheduler. Several of her colleagues recommended it, but she still isn't sure how this could help.

Eden decided that the first step for her is to understand how learning rate schedulers work.

**Which of the following is a correct definition of what a learning rate scheduler does?**


### **Choices** :

- The learning rate scheduler will help the optimization algorithm get past a flat region by continuing its previous movement.
- The learning rate scheduler will help the optimization algorithm accelerate in one direction based on past updates.
- The learning rate scheduler will save a copy of the network weights according to the value of the learning rate.
- The learning rate scheduler will adjust the learning rate during training according to a pre-defined schedule.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When training a neural network, setting the hyperparameters of the optimizer is essential for getting good results. One of the most critical parameters is the [learning rate](https://en.wikipedia.org/wiki/Learning_rate). Setting the learning rate too high or too low will cause problems during training.

A simple way to think about the learning rate is as follows: if we set it too low, the training process will be slow; it will take a long time for the network to converge. Conversely, if we use a learning rate that's too high, the process will oscillate around the minimum without converging. 

A popular technique to find a good balance is to use a learning rate scheduler. This predefined schedule adjusts the learning rate between epochs or iterations as the training progresses.

The most common scenario is to start with a high learning rate and decrease it over time. In the beginning, we take significant steps towards the minimum but move more carefully as we hone in on it. 

The first two choices of this question refer to [momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum). A learning rate scheduler's goal is neither rolling by flat regions nor accelerating in one direction based on previous updates. The third choice is also incorrect since a learning rate scheduler has nothing to do with saving the learning rate.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Learning Rate Scheduling"](https://d2l.ai/chapter_optimization/lr-scheduler.html) is a great introduction to learning rate schedulers.
- ["How to Choose a Learning Rate Scheduler for Neural Networks"](https://neptune.ai/blog/how-to-choose-a-learning-rate-scheduler) is an article from [Neptune AI](https://neptune.ai/), focusing on some practical ideas on how to use schedulers.</p></details>

-----------------------

## Date - 2022-07-16


## Title - Evaluating object detection


### **Question** :

Rose started a new job, and her first task was to develop an object detection model that detects cars in drone footage.

Rose wanted to build a generic model that she could use for different applications. 

It would help if she had some solid evaluation of the model's performance to compare different versions and choose the best one.

**Which evaluation metrics should Rouse use to evaluate her model?**


### **Choices** :

- Recall
- ROC Curve
- Precision-Recall Curve
- F1 score


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The recall is a helpful metric for object detection, but it won't tell Rose the whole picture without looking at the precision. Rose could build a useless model with high recall but abysmal precision. Therefore, she can't use recall as the definitive metric.

There's a problem with [ROC Curves](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) for object detection problems: there's no concept of True Negatives, and the ROC curve needs it to display the False Positive Rate on one of the axes. On object detection, the number of bounding boxes that won't contain a relevant object is too large, and that's why we avoid any metrics that depend on True Negatives.

Instead, Rose can compute a [Precision-Recall Curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html). This curve is similar to the ROC curve, but instead of using the False Positive Rate, it uses the model's precision that doesn't depend on the True Negatives. 

Finally, computing the [F1-score](https://en.wikipedia.org/wiki/F-score) is also an option because it considers both the precision and recall of the model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Classification: ROC Curve and AUC"](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) for an explanation of how to create and interpret a ROC curve.
* For more information about Precision-Recall curves, check [Scikit-Learn's documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html).</p></details>

-----------------------

## Date - 2022-07-17


## Title - Illegitimate traffic


### **Question** :

Rachel's team built a decision tree model to determine illegitimate requests to their application. 

While working on a prototype with fake data, everything worked as expected. But as soon as they tried to use real traffic, it was clear that the problem was more complex than expected: most requests were legitimate, and the decision tree had difficulty finding the problematic ones.

Rachel still thought a decision tree was the right approach, so she decided to find a solution.

**What's the appropriate next step to solving this problem?**


### **Choices** :

- Rachel should balance the dataset before training the decision tree. After doing this, she can fit the model again.
- Rachel should fit the decision tree and then scale the model's results appropriately, following the weight of each class.
- Rachel should fit the decision tree on each class separately—illegitimate and legitimate requests. Then, she should combine the results proportionally to the weight of each class.
- Rachel cannot make a decision tree work with an imbalanced dataset. She should pick a different model.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's imagine there's no problem with 99% of the traffic. If Rachel's model classifies every request as legitimate, it will have 99% accuracy. Unfortunately, this is useless, so Rachel needs to find a different solution.

If Rachel balances the dataset before fitting the decision tree, she will have a better chance. There are many different strategies to balance a dataset, and they all aim to amplify the impact of the minority class so the model can better predict it.

The second choice is not a solution. It doesn't make sense to scale the model results following the weight of the classes. Something similar happens with the third choice: fitting the decision tree on each separate class doesn't make sense either.

Finally, the fourth choice argues that decision trees don't work with imbalanced datasets, which is not true. Besides balancing the dataset, Rachel could use each class' weight to penalize the model more harshly when it misses illegitimate requests.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Random Oversampling and Undersampling for Imbalanced Classification"](https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/) for an introduction to oversampling and undersampling.
* ["Failure of Classification Accuracy for Imbalanced Class Distributions"](https://machinelearningmastery.com/failure-of-accuracy-for-imbalanced-class-distributions/) covers the problem of using accuracy as the metric in imbalanced classification problems.
* ["Learning from imbalanced data"](https://www.jeremyjordan.me/imbalanced-data/) discusses a number of considerations and techniques for dealing with imbalanced data.</p></details>

-----------------------

## Date - 2022-07-18


## Title - Increasing dropout


### **Question** :

Quinn's approach to learning something new is to run as many experiments as she needs to grasp how things work.

Today, it was the turn of Dropout. 

Quinn wants to know how it works, and one of her experiments is to increasingly modify the Dropout rate in a neural network and see how it affects the model's accuracy on the test data.

**Which of the following do you think is the result that Quinn will observe after running this experiment?**


### **Choices** :

- As the Dropout rate increases, the accuracy will go down until it hits a low value.
- As the Dropout rate increases, the accuracy will go down, and at some point, it will begin to increase.
- As the Dropout rate increases, the accuracy will go up until it hits its maximum potential.
- As the Dropout rate increases, the accuracy will go up, and at some point, it will begin to decrease.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Dropout is a regularization method that works well and is vital for reducing overfitting.

Sometimes, the nodes in a neural network create strong dependencies on other nodes, which may lead to overfitting. An example is when a few nodes on a layer do most of the work, and the network ignores all the other nodes. Despite having many nodes on the layer, you only have a small percentage of those nodes contributing to predictions. We call this phenomenon "[co-adaptation](https://machinelearning.wtf/terms/co-adaptation/)," and we can tackle it using Dropout.

During training, Dropout randomly removes a percentage of the nodes, forcing the network to learn in a balanced way. Now every node is on its own and can't rely on other nodes to do their work. They have to work harder by themselves. 

As Quinn increases the Dropout rate, she will reduce overfitting and see the model's accuracy improve. At some point, however, the rate will be so high that the model will start underfitting, and the accuracy will come down. This drop happens because removing too many of the neurons during training will effectively reduce the model's capacity too much. In other words, while some regularization is good, too much of it will prevent the model from learning.

Therefore, the fourth choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For more information about co-adaptation and how to use Dropout, check ["Improving neural networks by preventing
co-adaptation of feature detectors"](https://arxiv.org/pdf/1207.0580.pdf).
* ["A Gentle Introduction to Dropout for Regularizing Deep Neural Networks"](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) is an excellent introduction to Dropout.</p></details>

-----------------------

## Date - 2022-07-19


## Title - Convolutions are better


### **Question** :

When Nora finished the chapter about neural networks, she was ready to step it up with a more complex problem.

The next stop was computer vision using convolutional neural networks (CNN.) These worked differently than fully connected networks and were better suited for processing image data.

Before diving in, Nora wanted to summarize the advantages of using a CNN.

**Which of the following are characteristics of convolutional neural networks?**


### **Choices** :

- Convolutional neural networks are scale invariant.
- Convolutional neural networks are rotation invariant.
- Convolutional neural networks are translation invariant.
- Convolutional neural networks do not need human preprocessing or supervision to detect relevant features in an image.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To answer this question, we need to consider that Nora is focusing on out-of-the-box [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) and how they compare to fully-connected networks. [Some implementations](https://arxiv.org/abs/1411.6369) improve the capabilities of CNNs in different ways, but here we are focusing on their fundamental characteristics.

Convolutional neural networks are [neither scale nor rotation invariant](https://pyimagesearch.com/2021/05/14/are-cnns-invariant-to-translation-rotation-and-scaling/); you need to include rotated images at different scales as part of your training dataset to teach the network how to recognize them. They are, however, translation invariant: they can recognize the same patterns independently of where they show in an image. This characteristic differentiates them from fully-connected networks.

Finally, convolutional neural networks can learn relevant features of an image without needing human supervision. Back in the day, when trying to get a neural network to work with image data, we had to preprocess the images and manually curate data points to help the network learn what we considered important. This was time-intensive, didn't scale, and made the training process brittle and slow. Therefore, the fourth choice is also correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Are CNNs invariant to translation, rotation, and scaling?"](https://pyimagesearch.com/2021/05/14/are-cnns-invariant-to-translation-rotation-and-scaling/) goes into more detail about whether convolutional neural networks are translation, rotation, and scale invariant.
- Check ["How Do Convolutional Layers Work in Deep Learning Neural Networks?"](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/) for an introduction to how convolutional layers work.</p></details>

-----------------------

## Date - 2022-07-20


## Title - Crossing the bridge


### **Question** :

To cross the bridge, Rylee had to solve a final puzzle.

She had four pieces in front of her, each representing a layer used in Convolutional Neural Networks (CNN). Rylee had to put them in the correct order, and only then will the doors to the bridge reveal.

There's only one chance and no time to waste.

**Which of the following is the correct order in which these four layers appear in a CNN?**


### **Choices** :

- Input Layer, Pooling Layer, Convolutional Layer, Non-linearity.
- Input Layer, Convolutional Layer, Pooling Layer, Non-linearity.
- Input Layer, Convolutional Layer, Non-linearity, Pooling Layer.
- Input Layer, Pooling Layer, Non-linearity, Convolutional Layer.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Look at the structure of a CNN, and you'll see a pattern repeated multiple times: Convolution, non-linearity, pooling. ReLU and Max Pooling are popular choices for non-linearity and pooling layers.

Here is an illustration of the [architecture of VGG16](https://medium.com/mlearning-ai/an-overview-of-vgg16-and-nin-models-96e4bf398484). Notice the pattern:

![Architecture of VGG16](https://user-images.githubusercontent.com/1126730/174477391-272e2265-415a-49ef-b3c5-8573e96b153a.png)

The convolutional layer creates feature maps by applying learned filters to the input image. The pooling layer downsamples the feature maps and makes the model robust against location differences of the features detected. They work together, which is why they appear in this order.

Introducing non-linearities is crucial if we want our model to learn complex functions. The non-linearity is applied directly after the convolution layer to produce the final feature map.
 
If Rylee wants to stay alive, she must order the pieces as indicated by the third choice.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Convolutional Neural Networks — A Beginner’s Guide"](https://towardsdatascience.com/convolution-neural-networks-a-beginners-guide-implementing-a-mnist-hand-written-digit-8aa60330d022) is a great introduction to Convolutional Neural Networks.
* Check ["A Gentle Introduction to Pooling Layers for Convolutional Neural Networks"](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/) for more information about how the combination of convolutions and pooling layers work.</p></details>

-----------------------

## Date - 2022-07-21


## Title - Roman numerals


### **Question** :

Tracy is taking part in the [Data-Centric AI Competition](https://https-deeplearning-ai.github.io/data-centric-comp/) by [Andrew Ng](https://twitter.com/andrewyng). The competition's goal is to train a model to classify Roman numerals. However, unlike most other challenges, in this one, the participants are only allowed to change the data and not the model.

Every competitor has access to a balanced dataset with examples of Roman literals, but Tracy decided to use a Generative Adversarial Network (GAN) to generate new samples.

She implemented a GAN and trained it on her dataset. Both loss functions converged nicely, and she started sampling from the model. However, something was off: the GAN only generated images for two of the numerals but never produced any others.

**Why is Tracy's network not producing the expected results?**


### **Choices** :

- The GAN is overfitting. That's why the loss functions converged, but the results in practice are not good.
- The dataset isn't perfectly balanced. The network probably ignored any underrepresented classes.
- Training GANs requires vast amounts of training data. Tracy didn't have a large enough dataset.
- The discriminator network is stuck in a local minimum allowing the generator to cheat.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Tracy's idea is good. We can use [Generative Adversarial Networks](https://en.wikipedia.org/wiki/Generative_adversarial_network) to create new data samples and augment the original dataset. However, a lot of work is involved in training a GAN correctly. 

The first hypothesis is that the GAN is suffering from overfitting. However, the network seems to work well for some numerals but not others. That is an unlikely behavior for a model that's overfitting.

Neither the second nor the third choices are correct either. First, the description of this problem states that Tracy's dataset is well balanced. Even if there are minor differences, it's unlikely for the network to struggle to generate some of the numerals. Second, the amount of samples doesn't seem to be a problem for the classes produced by the network, so it shouldn't be causing the issue either.

Finally, the fourth choice seems to hit the nail: we are in the presence of one of the most common problems when training a GAN: "[Mode collapse.](https://developers.google.com/machine-learning/gan/problems#mode-collapse)"

Mode collapse happens when the discriminator network gets stuck in a local minimum, and the generator learns to fool it by consistently producing a few samples repeatedly. At this point, the generator stops producing images for most of the numerals, and we end up with results from a few classes.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Andrej Karpathy](https://twitter.com/karpathy) built a [Generative Adversarial Network tool](https://cs.stanford.edu/people/karpathy/gan/) that you can use to play around with different hyperparameters when training a GAN.
* For a deep dive into Generative Adversarial Networks, check Neuromatch's Academy [Introduction to GANs](https://deeplearning.neuromatch.io/tutorials/W2D4_GenerativeModels/student/W2D4_Tutorial2.html).</p></details>

-----------------------

## Date - 2022-07-22


## Title - Agatha Christie's novels


### **Question** :

Maeve wanted to win the hackathon, so she went with a fun and impressive application.

She downloaded the text of three dozen Agatha Christie novels and used it to train a model to impersonate the author and generate short stories using her style.

The most exciting part was that Maeve's model wrote the stories one character at a time!

**Which of the following models do you think Maeve used to build her solution?**


### **Choices** :

- A Feed-forward Neural Network
- A Convolutional Neural Network
- A Recurrent Neural Network
- A Text Generation Neural Network


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Maeve used a [Recurrent Neural Network](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNN) to build her model. This choice gave Maeve an essential advantage.

Maeve's model receives a large chunk of text, one character at a time, and outputs the probability distribution of the next character in the sequence. This approach may not sound impressive until you think carefully about what it does when generating a word like "HELLO": the first time she feeds the model the letter "L," its output should be another "L," but the second time, the output switches to an "O."

This ability reveals a critical characteristic of Recurrent Neural Networks: they don't solely rely on their input to make predictions. Instead, they have a memory to keep context and use it to generate their output. In other words, the RNN's output is influenced not only by its input but also by the history of inputs Maeve used in the past.

Keeping context around and using it for text generation is crucial for Maeve's application.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["The Unreasonable Effectiveness of Recurrent Neural Networks"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy for the motivation behind Recurrent Neural Networks.
* ["An Introduction To Recurrent Neural Networks And The Math That Powers Them"](https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/) is an excellent introduction to Recurrent Neural Networks.
* Neuromatch's Academy ["Time series for Language"](https://deeplearning.neuromatch.io/tutorials/W2D5_TimeSeriesAndNaturalLanguageProcessing/student/W2D5_Tutorial2.html) also introduces Recurrent Neural Networks.</p></details>

-----------------------

## Date - 2022-07-23


## Title - Different twins


### **Question** :

Mary and Jane were twins.

Despite their striking physical similarities, they were very different people.

Mary was stubborn. She had a specific mental model for everything, and no matter the situation, she would follow her model regardless of the consequences. As Jane frequently said, Mary was always biased in her process.

On the other hand, Jane was all over the place, sometimes, even to a fault. She could not generalize and gave too much importance to small and irrelevant details. Her variability drove Mary crazy.

**If Mary and Jane were machine learning models, which of the following would be correct?**


### **Choices** :

- Mary will be more likely to overfit. Jane will be more likely to underfit.
- Mary will be more likely to underfit. Jane will be more likely to overfit.
- Mary will be more likely to underfit. Jane will be similarly likely to overfit and underfit.
- Mary will be similarly likely to overfit and underfit. Jane will be more likely to overfit.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Neither Mary nor Jane are machine learning models, but this makes for an interesting question, so let's go with it.

We know that Mary has a high bias. She pays little attention to the situation that she is in and instead always follows a specific mental model. Her inability to adapt to scenarios that aren't part of her mental models makes her decisions suboptimal.

Because of the high bias, Mary will be more likely to underfit. She isn't capable of adapting to new situations, so she wouldn't take advantage of new opportunities and will always get mediocre results.

On the other hand, Jane has the opposite problem: She is high variance. Jane pays a lot of attention to every detail, no matter how unimportant. Her lack of mental models will take her down rabbit holes that won't be productive. Because of this, Jane will be more likely to overfit.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Understanding the Bias-Variance Tradeoff"](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229) is a great introduction to bias, variance, and their tradeoff.
* ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) is an excellent introduction to the bias and variance tradeoff.
* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).</p></details>

-----------------------

## Date - 2022-07-24


## Title - Show me another function


### **Question** :

During her interview, Maggie has to build a binary classifier to distinguish images of shoes from pictures of watches.

The problem was simple, and since there wasn't a lot of data, Maggie used a pre-trained ResNet50 model, modified its output, and trained the final two layers on the data they had. Before anyone in the room noticed, the model had finished with excellent performance.

Impressed with Maggie's speed, the interviewer started looking at the code. Since this was a binary classification problem, Maggie used a Sigmoid activation function on the last layer, catching the interviewer's attention.

"I understand why you used Sigmoid for this problem,"— he said with a smirk. "Can you modify the code using a different activation function, please?"

**Which of the following activation functions could Maggie use in the last layer to get similarly good results? Select all that apply.**


### **Choices** :

- ReLU
- Leaky ReLU
- Softmax
- Tanh


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Maggie is building a binary classifier, which means she wants the output layer to produce a result that she can easily interpret to determine whether a picture is a photo of a shoe or an image of a watch.

Neither the first nor the second choices are correct. [ReLU](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks) returns its input if positive or zero otherwise, while Leaky ReLU is based on ReLU but with a slight slope for negative values. 

Let's assume the input to the output layer using ReLU or Leaky ReLU is a positive value. Both activation functions will leave that value untouched. They will act as a passthrough for any positive values coming from the previous layer and turn any negative values into zero—Leaky ReLU will turn negative values into a tiny positive result. This is not going to help Maggie classify the images.

[Softmax](https://en.wikipedia.org/wiki/Softmax_function) converts a vector of numbers into a vector of probabilities. Maggie cares about two different classes: "shoe" and "watch." She can use softmax to output a vector with two values and decide the correct classification based on which one returns the largest value. For example, if the first value in the output vector is larger, Maggie can assume the image is a shoe. If the second value is larger, the picture is a watch.

Remember that Maggie initially used a [sigmoid](https://en.wikipedia.org/wiki/Logistic_function) activation function. Softmax is an extension of sigmoid to cover cases where we care about more than two classes. [Softmax reduces to sigmoid](https://en.wikipedia.org/w/index.php?title=Logistic_regression&oldid=755697139#As_a_.22log-linear.22_model) when we use it in a binary classification context, so Maggie should get the same result in both cases.

Finally, Maggie could also use a tanh activation function in the output layer. She will need to modify the encoding of the target classes as -1 and 1—instead of 0 and 1, which is what we commonly do—and use a different loss function. [Here is an experiment](https://jamesmccaffrey.wordpress.com/2020/11/02/neural-network-binary-classification-with-tanh-output-activation/) where the author trains a network using mean square error and an output layer with a tanh activation function.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For a comparison between sigmoid and softmax, check ["The Differences between Sigmoid and Softmax Activation Functions."](https://medium.com/arteos-ai/the-differences-between-sigmoid-and-softmax-activation-function-12adee8cf322)
* ["A Gentle Introduction to the Rectified Linear Unit (ReLU)"](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks) is a great introduction to ReLU.
* Check out ["Neural Network Binary Classification With Tanh Output Activation"](https://jamesmccaffrey.wordpress.com/2020/11/02/neural-network-binary-classification-with-tanh-output-activation/) for the source code of a binary classifier using a tanh activation function.</p></details>

-----------------------

## Date - 2022-07-25


## Title - Transforming Remi's model


### **Question** :

Everyone convinced Remi to rewrite her Long Short-Term Memory (LSTM) model and start using Transformers.

But after Remi spent some time researching, she concluded that Transformers wouldn't necessarily improve the quality of her model results.

**Assuming both models will provide the same results, what's a valid reason for Remi to start using Transformers instead of LSTM?**


### **Choices** :

- Transformers are much more computationally efficient than RNN-based networks.
- Transformers have the potential to understand the relationship between an element and the element that follow it in the sequence.
- Transformers have the potential to process sequences one element at a time.
- Transformers have the potential to understand the relationship between sequential elements that are far from each other.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Unlike Transformers, [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) models are sequential and process elements in order. Transformers process sequences as a whole rather than element by element. This characteristic allows us to parallelize Transformers, which enables us to process much more data when compared with an LSTM.

The above explanation helps answer the first and third choices of this question. Transformers are much more computationally efficient, and they don't process elements sequentially but rather process sequences as a whole.

Finally, although Transformers can understand the relationship of two elements next to each other in a sequence, an LSTM can also do the same, so the second choice is not a reason to migrate to Transformers. Unlike LSTM models, the power of the Transformer architecture is understanding the relationship between elements that are far from each other.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762) is the famous paper where the Google Brain team introduced the Transformer architecture.
* Check ["Long Short-Term Memory"](https://en.wikipedia.org/wiki/Long_short-term_memory) for an introduction to LSTM models.</p></details>

-----------------------

## Date - 2022-07-26


## Title - Launching in different countries


### **Question** :

Maddison has never been part of such an extensive application. 

Her company hired her to build an incredibly ambitious project, and a few years after they launched it, usage took off in the United States. With more and more daily interest, Maddison's company is ready to expand to other countries.

Maddison knows it won't be a trivial task. At its core, the system uses a deep learning model to describe pictures taken by users, and Maddison is afraid of what will happen when people from all over the world start uploading data. 

**What can Maddison do to mitigate potential biases before expanding to different countries?**


### **Choices** :

- Maddison should improve their feature engineering process to create better features that help the model produce good results for users in different countries.
- Maddison should collect equal proportions of data from users living in the countries they plan to release.
- Maddison should improve their labeling process to ensure a diverse pool of people handling labels.
- Maddison should improve their feature selection process to ensure the model only uses relevant features for each user's country.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Maddison is right: guaranteeing the application runs with the same success across different countries will require work. Specifically, they will need to ensure the training data reflects every user's diversity and cultural context.

Here are a few wedding photographs labeled by a classifier trained on the Open Images dataset. This example comes from [Google's AI blog](http://ai.googleblog.com/2018/09/introducing-inclusive-images-competition.html). Notice the predictions recorded below each image:

![Wedding photographs labeled by a classifier trained on the Open Images dataset](https://user-images.githubusercontent.com/1126730/167641166-a55ff720-1a29-4d75-b60b-71e3f3da4f81.png)

The classifier can't handle wedding traditions from different parts of the world, and that's what Maddison needs to avoid. 

Maddison should focus on the data they use to train their model to ensure fairness and proper representation, but it is neither about better feature engineering nor feature selection.

If the training dataset they are using doesn't correctly represent every user, no amount of fiddling with features will reduce the bias. Remember,  ƒ(🗑️)=🗑️ —garbage in, garbage out.

From the choices on this question, Maddison should focus her time on two different areas: collecting representative data and ensuring that data is correctly labeled. Notice that doing only one of these is not enough; you need both to ensure you have a less biased dataset.

For example, adding the last image to the dataset is not enough because most people in the United States will not recognize it as a webbing image, either. Labelers should be familiar with the particular country's culture to classify pictures correctly.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Introducing the Inclusive Images Competition"](http://ai.googleblog.com/2018/09/introducing-inclusive-images-competition.html) Google's blog post introducing the ["Inclusive Images Challenge"](https://www.kaggle.com/c/inclusive-images-challenge) competition on Kaggle.
* The ["Machine Learning Data Lifecycle in Production"](https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production) course in Coursera, part of the [Machine Learning Engineering for Production (MLOps) Specialization](https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops).
* ["Fairness"](https://en.wikipedia.org/wiki/Fairness_(machine_learning)) is a Wikipedia article covering some important fairness concepts.</p></details>

-----------------------

## Date - 2022-07-27


## Title - Rotten bananas


### **Question** :

The supermarket wanted to create an automatic process to detect rotten bananas so they could send a rep and pull them out of the shelves.

They had been collecting pictures for a long time, but unfortunately, most were images of good-looking bananas. 

Camila had an idea. What if they used an autoencoder to build a model capable of detecting rotten bananas? The dataset they had was probably everything they needed for this.

**Regardless of whether this is a good idea, which of the following statements are true about autoencoders?**


### **Choices** :

- Autoencoders are supervised learning methods that use neural networks for the task of representation learning.
- Autoencoders are good at detecting unique characteristics among the different samples in the dataset and reproducing them in the output.
- Autoencoders have a bottleneck, which we use to constrain the amount of information that traverses from the input to the network's output.
- Autoencoders are typically only capable of reconstructing data similar to the class of observations we used during training.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>I like to think of autoencoders as data compression algorithms built using neural networks. A network encodes (compresses) the original input into an intermediate representation, and another network reverses the process to get the same information back.

The encoding process generalizes the input data. Its job is to represent the dataset as compactly as possible, so the decoder can do a decent job reproducing the original data. This encoding and decoding process is lossy, which means we will lose some details from the initial input.

Autoencoders are not supervised learning methods because they don't need explicit labels to train. Instead, we can consider them self-supervised because they generate labels directly from the training data. Therefore, the first choice is incorrect.

The second choice is also incorrect: any unique characteristics in the input data will never make it past the autoencoder's bottleneck. The encoder is very picky about the features it encodes, so it'll never select anything that's not representative of the whole dataset. This is a great property that we can use to detect anomalies in the data!

The third choice is correct. Here is a visualization of an autoencoder from my favorite ["Introduction to autoencoders"](https://www.jeremyjordan.me/autoencoders/) article:

![Autoencoder](https://user-images.githubusercontent.com/1126730/168314400-94634324-b449-4279-87b1-efc0e41bc550.png)

Notice the bottleneck in the middle. This is a fancy name for a hidden layer that forces the network to learn a compressed representation of the original input. Any characteristic that doesn't help explain the dataset correctly will never make it past the bottleneck. 

For example, imagine we train an autoencoder with thousands of pictures of bananas. The autoencoder will learn to reproduce the original picture. What would happen if we showed the autoencoder a rotten banana instead? Since the autoencoder didn't learn how to represent rotten bananas, the decoder will never be able to reproduce them correctly. The result will look like a regular banana because that's all the information the decoder had.

Finally, the fourth choice is also correct. Autoencoders can only reconstruct data similar to the data they were trained on.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* My favorite introduction to autoencoders is the ["Introduction to autoencoders"](https://www.jeremyjordan.me/autoencoders/) article by Jeremy Jordan.
* If you want to play with some code, check the ["Convolutional autoencoder for image denoising"](https://keras.io/examples/vision/autoencoder/) Keras tutorial. You should be able to run it using Google Colab.</p></details>

-----------------------

## Date - 2022-07-28


## Title - Visiting new places


### **Question** :

Mackenzie loved visiting new places and working remotely for weeks at a time.

She had a strategy to discover every secret corner of a new city: Some days, she would pick a random path. Other days, she would only use the streets she knew well enough. Over time, Mackenzie would learn about every corner, road, and hidden passage the city would have to offer.

Mackenzie, a reinforcement learning engineer, described her approach as a "0.6 epsilon greedy policy."

**Which of the following correctly summarizes Mackenzie's approach during her travels?**


### **Choices** :

- Mackenzie follows a random path 60% of the time and only takes streets she already knows 40% of the time.
- Mackenzie follows a random path 40% of the time and takes streets she already knows 60% of the time.
- Mackenzie emphasizes exploiting her knowledge about the city more than exploring the unknown.
- Mackenzie usually follows a greedy path and otherwise takes random streets.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In reinforcement learning, an ["epsilon greedy policy"](https://developers.google.com/machine-learning/glossary#epsilon-greedy-policy) refers to a strategy that either follows a random policy with epsilon probability or a greedy policy otherwise.  

A [greedy policy](https://developers.google.com/machine-learning/glossary#greedy-policy) always chooses the action with the highest expected return. In Mackenzie's case, this would be taking the streets that she already knows. She explores and discovers new places and roads when following a [random policy](https://developers.google.com/machine-learning/glossary#random_policy).

Mackenzie describes her approach as a "0.6 epsilon greedy policy" because she follows random streets about 60% of the time. In other words, Mackenzie spends most of her time exploring rather than exploiting what she already knows.

As an additional note, in reinforcement learning, after accumulating knowledge from exploiting, the algorithm reduces the value of epsilon to shift from following a random to a greedy policy.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Exploration vs. exploitation in reinforcement learning"](https://www.manifold.ai/exploration-vs-exploitation-in-reinforcement-learning) is a great introduction to the exploration vs. exploitation tradeoff.
* Check Google's machine learning glossary definition of ["Epsilon greedy policy"](https://developers.google.com/machine-learning/glossary#epsilon-greedy-policy).</p></details>

-----------------------

## Date - 2022-07-29


## Title - Tom hates losing


### **Question** :

During the COVID19 lockdown, Tom started playing backgammon with his friends. He liked the game, but he wasn't good at it.

Tom hates losing, so he decided to train a reinforcement learning algorithm to help him play the game and beat his friends. He chose the Monte Carlo Tree Search method and implemented it using deep learning. After all, this approach helped [AlphaGo Zero](https://en.wikipedia.org/wiki/AlphaGo_Zero) become the best Go player, so surely it would help him beat his friends.

Soon after Tom started, he found a concept he didn't understand. He needed to train a _value network_ despite having a _policy network_ telling him what move to do next. Why does he need yet another network?

**Which of the following statements about the _value network_ are useful?**


### **Choices** :

- The value network takes the state of the game and outputs the best possible moves the player can make.
- The value network takes the state of the game and outputs an estimation of how good that state is for the player.
- The value network is used to reduce the depth of the search when traversing all possible variants of how the game could develop.
- The value network is used to reduce the breadth of the search when traversing all possible variants of how the game could develop.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The classical algorithmic approach to solving a game is traversing all possible states and seeing which move leads to the most favorable outcome. For many games, including backgammon, checking all possibilities is not feasible, so we need to rely on approximations.

[Monte Carlo Tree Search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search) is a popular method that has been applied successfully to many games. Combining it with deep neural networks makes it even more powerful. For this, we need to train two neural networks.

The first one is called the policy network. Given the state of the game, this network outputs a probability distribution over all possible game moves, and it tells us which ones are likely the best. We can train this network with experts in a supervised fashion, then refine it using reinforcement learning. Notice that this question's first choice incorrectly mistakes this network with the value network.

The second neural network we need to train is the value network. It takes the state of the game and outputs an approximation of how favorable that state is for the player. That's what the second choice of this question describes, so it's a correct answer.

When finding the best strategy, the policy network tells us which moves are best and reduces the search's breadth in the game tree. The value network tells us if it makes sense to continue exploring a branch given its value. Therefore, the value network helps us reduce the search's depth, making the third choice of this question a correct answer as well.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- The ["Monte Carlo Tree Search"](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search) page in Wikipedia has a good overview of this heuristic search method.
- The [Reinforcement Learning](https://amzn.to/3yk2bon) book by Sutton and Barto is an excellent book on reinforcement learning covering all the topics from this question.
- ["Reinforcement Learning For Games by Neuromatch"](https://deeplearning.neuromatch.io/tutorials/W3D5_ReinforcementLearningForGames/chapter_title.html) is an excellent tutorial on applying Monte Carlo Tree Search with deep learning to games.</p></details>

-----------------------

## Date - 2022-07-30


## Title - Securing your bank account


### **Question** :

Jacob is working at a big bank, and they assigned him a critical task: Revamp their mobile application's security using face-scanning authentication.

Modern phones offer face identification functionality, but the bank wanted to use additional biometric markers to ensure higher security. In addition to taking a picture of the user's face, Jacob had to create a 3D model.

The problem was that many of the bank's customers used phones with a single front camera and no 3D capabilities.

**What method can Jacob use to create a 3D model of the user's face?**


### **Choices** :

- Jacob can use a deep learning-based depth estimation algorithm using a single front-facing photo from users.
- Jacob can estimate a 3D model by asking users to take two photos: one front-facing and another after moving the phone to the left.
- Jacob can estimate a 3D model by asking users to take two photos: one front-facing and another after moving the phone closer to their faces.
- Jacob can estimate a 3D model by asking users to take two photos: one front-facing in portrait mode and another in landscape mode.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Face identification alone doesn't provide enough security: anyone could bypass the authentication screen by using a static photo of the user's face. That's why having a 3D model of the user's face is critical.

That's why the first choice of this question is not a good idea. While [monocular depth estimation methods using deep learning](https://paperswithcode.com/task/monocular-depth-estimation) deliver fantastic results, using a single photo won't offer the security of a 3D model.

Using multiple photos of the user's face from different positions will give us information that we can use to estimate a 3D model. We can use the [parallax effect](https://en.wikipedia.org/wiki/Parallax) to calculate the distance to every pixel and create a 3D model of the face using triangulation. The intuition behind this method is that points far away from the camera will move less between photos compared to closer points. Such methods are known as [stereo vision](https://en.wikipedia.org/wiki/Computer_stereo_vision). 

Finally, although the fourth choice of this question argues about using multiple photos, we can only use the parallax effect when the pictures are from different positions. Rotating the phone will not be enough because the image of the user's face will look mostly identical in both shots.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["How Do Neural Networks See Depth in Single Images?"](https://openaccess.thecvf.com/content_ICCV_2019/papers/van_Dijk_How_Do_Neural_Networks_See_Depth_in_Single_Images_ICCV_2019_paper.pdf) is a paper that covers how we can estimate depth from a single image.
- Check the ["Triangulation"](https://en.wikipedia.org/wiki/Triangulation_(computer_vision)) Wikipedia page for an explanation of stereo estimation when using two or more images.</p></details>

-----------------------

## Date - 2022-07-31


## Title - Parallelizing training


### **Question** :

Claire's company has been accumulating data at a breakneck pace. 

Her team maintains one of their machine learning models, and every few weeks, they retrain it with all of the available data.

Unfortunately, the training process is starting to take too much time. Not only is the team spending a fortune on computing power, but any experimentation is becoming a problem.

Claire wants to look into whether parallelizing the training process is doable.

**How can Claire parallelize training?**


### **Choices** :

- Claire can parallelize the process by training multiple model copies on the same computer running various GPUs. Each copy will train on a portion of the training dataset, and each device will synchronize its updates at the end of each iteration.
- Claire can parallelize the process by training multiple model copies on different computers. Each model will train on a portion of the training dataset, and each computer will synchronize its updates at the end of each iteration.
- Claire can parallelize the process by training multiple models on different computers. Each copy will use the entire training dataset, and Claire will keep the best performant model at the end.
- Unfortunately, Claire can't parallelize the training process of her model because training is an atomic operation. It would be different if Claire needed to train an ensemble model.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Claire has multiple ways to parallelize—or distribute—the training process. We can divide these into two groups:

* Data Parallelism: We train multiple copies of the model on a different subset of the data.
* Model Parallelism: We train different segments of a model on the entire dataset.

The first two choices are instances of data parallelism: Claire can train multiple copies of the model on separate workers, each on a different slice of the dataset. Notice that these two choices differ from what we consider a "worker."

The first choice suggests that Claire can use multiple GPUs in the same computer to distribute the training process. The second choice argues that Claire can do the same using various computers. Both options are correct. Check TensorFlow's [Mirrored Strategy](https://www.tensorflow.org/guide/distributed_training#mirroredstrategy) and [Multi-Worker Mirrored Strategy](https://www.tensorflow.org/guide/distributed_training#multiworkermirroredstrategy), and PyTorch's [Distributed Data Parallel](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html).

The third choice is not a valid approach to parallelizing the model. If Claire trains the same model on the same data on separate computers, she will get the same results, albeit minor differences due to the stochasticity of the training process.

Finally, the fourth choice is incorrect because Claire does have options to parallelize training.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["How to Distribute Deep Learning Model Training?"](https://arshren.medium.com/how-to-distribute-deep-learning-model-training-693a1898918f) is a great introduction to training parallelization.
* For information about how to parallelize training using TensorFlow, check ["Distributed training with TensorFlow"](https://www.tensorflow.org/guide/distributed_training).
* For information about how to parallelize training using PyTorch, check ["Getting started with Distributed Data Parallel"](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html).
* The ["Machine Learning Data Lifecycle in Production"](https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production) course in Coursera, part of the [Machine Learning Engineering for Production (MLOps) Specialization](https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops) covers parallelization.</p></details>

-----------------------

## Date - 2022-08-01


## Title - What happens to the loss?


### **Question** :

Aubrey is not happy with the train and validation loss of her model.

Everything seems to be working fine, except her validation loss is lower than the training loss.

Aubrey's experience is the opposite. She is used to seeing models crush the training set and get a much lower loss, so this chart seems suspicious to her:

![Train and validation loss](https://user-images.githubusercontent.com/1126730/172624732-e6f2d9ea-5a75-447e-87da-2f06e2df0d8a.png)

**Which of the following could be valid reasons for the validation loss to be lower than the training loss?**


### **Choices** :

- Aubrey's model includes Dropout layers.
- Aubrey is using some data augmentation techniques before training.
- Aubrey is using a validation set that's too easy for the model.
- Aubrey is leaking training samples into the validation set.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Many people look at a plot showing a validation loss lower than a training loss and immediately assume there's a problem. The fact is that this is not necessarily something to worry about, but first, we need to make sure of that.

The simplest explanation is that Aubrey might have a validation set that's very easy for the model—at least easier than the training set. This could happen, for example, because of the way Aubrey sampled the data to create the validation set—picking simple instances—or because there's a leak—which would be a problem. If she is working with an imbalanced dataset but is not stratifying the data, she might be in this situation. The same problem could happen if the validation set doesn't have enough samples.

The third and fourth choices are potential reasons based on the above explanation. Aubrey should make sure she doesn't have a leak and work to improve her validation set.

But her validation set might be perfectly correct, and the explanation for the lower loss might be elsewhere.

Dropout is a regularization technique that makes learning the train set harder for the model. Remember that Dropout only applies during training and not validation. Assuming Aubrey is using Dropout layers in her model, she is pushing the training loss higher and contributing to the gap. Therefore, the first choice is a potential reason for the validation loss to be lower.

Finally, data augmentation could also contribute to this problem. Data augmentation has a regularization effect, and we usually augment data during training but not during validation. In other words, we train models with complex examples and evaluate them with more straightforward samples. Therefore, the second choice is also a correct answer.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Why is my validation loss lower than my training loss?"](https://pyimagesearch.com/2019/10/14/why-is-my-validation-loss-lower-than-my-training-loss/) covers the answer to this question.
* ["How to use Learning Curves to Diagnose Machine Learning Model Performance"](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/) is a great introduction to learning curves and how to use them to understand how your model is working.</p></details>

-----------------------

## Date - 2022-08-02


## Title - Twitter loves gradient boosting


### **Question** :

When Stella joined Twitter, she found that many people in the machine learning community were fans of gradient-boosting algorithms.

Stella wasn't expecting this. She came from a program emphasizing deep learning techniques, so seeing so much talk about gradient boosting was shocking. Apparently, these algorithms were instrumental in real-life applications.

Stella asked around, and [Bojan](https://twitter.com/tunguz) replied. He gave her a list of some of the main characteristics of gradient boosting algorithms.

**Which of the following do you think made it into Bojan's list?**


### **Choices** :

- Gradient boosting combines individual learners barely better than random guessing to produce excellent results.
- Gradient boosting's results are less interpretable than those from a decision tree.
- Gradient boosting is flexible and can optimize any differentiable loss function.
- Gradient boosting is very computationally and memory efficient.


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Twitter is right: [gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) is a helpful ensembling technique for building machine learning systems. Remember that ensembling is where we combine a group of models to produce a new model that yields better results than any one of the initial individual models.

This introduction speaks directly about the first choice. Gradient boosting ensembles a group of weak learners. We call them "weak" because they have some predictive capacity, usually slightly better than random guessing. The strength of the technique is not in any of these individual learners but in their combination. Therefore, the first choice is correct.

Using decision trees as the weak learners for gradient boosting is popular. Unfortunately, in ensembling these learners, we tradeoff some of the interpretability we'd get from an individual decision tree. It's much harder to interpret gradient boosting results than a tree's results. Therefore, the second choice was also part of Bojan's list.

As the name suggests, gradient boosting uses the loss function's gradient to minimize the loss of the model. It works similarly to gradient descent, supporting any differentiable loss function. That means we can use well-known loss functions like squared errors, logarithmic loss, or even define a custom loss function. The paper ["Greedy Function Approximation: A Gradient Boosting Machine"](https://jerryfriedman.su.domains/ftp/trebst.pdf) goes into more detail about the generalization that makes this possible.

Finally, the fourth choice is not correct. Depending on the problem, gradient boosting can quickly scale to thousands of weak learners, which is expensive in terms of computational and memory resources.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* If this is the first time you hear about gradient boosting, start by reading ["A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning"](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/).
* Check out ["Gradient Boosting In Classification: Not a Black Box Anymore!"](https://blog.paperspace.com/gradient-boosting-for-classification/) for a full walkthrough of how gradient boosting works with one example.
* ["Greedy Function Approximation: A Gradient Boosting Machine"](https://jerryfriedman.su.domains/ftp/trebst.pdf) is the paper mentioned in the explanation of this question.</p></details>

-----------------------

## Date - 2022-08-03


## Title - Navigating the recession


### **Question** :

The looming recession wasn't good news for the newspaper, and to add insult to injury, their machine learning model predicting demand started deviating from the actual results they were seeing.

Marisol had to take over. Her mission was simple: investigate what was happening with the model. They needed to fix it as soon as possible if they wanted a chance to survive.

After a few weeks, Marisol's report came back. The model wasn't ready to handle the changing economic conditions, and there were a few features whose values had drifted. She offered a few solutions for the team to consider.

**Which of the following could be part of Marisol's proposed solutions?**


### **Choices** :

- Keep the existing model, but modify its input in production by replacing the drifting features with their training mean or mode.
- Remove the drifting features from the training dataset and retrain the model.
- Add a few additional features to the training dataset, hoping the model can better adapt to the changing economic conditions. Retrain the model.
- Collect additional data, label it, and retrain the model.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The world doesn't stand still. As soon as you deploy your machine learning model to production, changes in the data—regardless of how small they are—will affect the quality of its predictions.

In a sudden economic downturn, our newspaper found itself in a middle of a bad situation. They struggled to survive, and degraded predictions made the issue even more pressing: they were flying blind.

Marisol understood the model was suffering from the consequences of drift in the data, and she even identified the few features significantly impacting the results. The newspaper could alleviate this problem in many different ways, so let's analyze each possibility.

One possible solution is to continue using the same model but replace the drifting features with a more traditional value. For example, assuming one of the features has changed significantly from its training mean, they could use that mean value instead of the skewed value.

The advantage of this scenario is that they don't need to retrain their model. The disadvantage is that there's no guarantee that this will work. Most likely, the model's predictions will still suffer, but there's a chance this degradation is less severe than allowing the drift to hit the model.

Remember that the first option may not be a reasonable solution depending on how severe the drift is and the correlation between the drifting feature and the target label. However, since we don't have any more specific information, this is something worth trying.

Something similar could happen if we follow the second strategy: removing a feature altogether and retraining the model could help alleviate the drift but is not guaranteed. Again, we'll likely see a drop in performance, and the only question is whether that decrease is less severe than using the skewed features. 

Sometimes we can add additional features to the dataset that help address the drift. For example, the unemployment rate could help smooth predictions during changing economic conditions. This is not always possible but worth exploring.

Finally, the go-to solution is to collect more data and retrain the model to adapt to the new situation. This is usually how most companies try to address data and concept drift, and it works in most cases, although it's far from a silver bullet. For example, there are situations where collecting updated ground-truth data is not possible without waiting for a long time, so reacting to drifts is a significant challenge.

In summary, every one of the choices is a possibility for the newspaper to explore. They all have different tradeoffs, and the team must decide which works better for them.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* This question was inspired by this excellent article: ["Drift in Machine Learning."](https://towardsdatascience.com/drift-in-machine-learning-e49df46803a) It covers the nuances of how to deal with data drift and comes with several good examples.
* ["Data Drift vs. Concept Drift: What Are the Main Differences?"](https://deepchecks.com/data-drift-vs-concept-drift-what-are-the-main-differences/) is a great introduction to data and concept drift.
* ["Why You Should Care About Data and Concept Drift" ](https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift) is a great article from [Evidently AI](https://evidentlyai.com/) focusing on the importance of monitoring your models.</p></details>

-----------------------

## Date - 2022-08-04


## Title - Avery's internship


### **Question** :

It took her some time, but finally, Avery landed an internship position at one of her dream companies.

The fun didn't last long. On the first day, she joined a team of other interns who had been dealing with a training problem with their neural network.

Soon after starting the process, the training loss flattened out and remained unchanged until the end. The team tried to increase the number of iterations to no avail.

**Which of the following could be valid reasons for the training loss to remain flat?**


### **Choices** :

- The team is using a learning rate that's too low.
- The team is using too much regularization.
- The team is working with an imbalanced dataset, with one class overwhelmingly dominating the rest.
- The team is using a lousy weight initialization strategy.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>I can't think of a better start to an internship than dealing with a model that refuses to learn. That's what's happening to their network: the training loss represents the model's ability to understand the training data. Getting a flat curve means the model can't learn that data appropriately.

Let's first consider the effect of the learning rate on this problem. The learning rate is a setting that lets you control how fast we move in the direction that the optimization algorithm wants to take us. Think of a ball rolling down a hill. A higher learning rate means the ball will move at a more significant speed, while a lower learning rate means there will be a lot of friction, and the ball will move slower.

Finding the optimal solution is like getting the ball right inside a hole at the bottom of the hill. If our learning rate is too high, the ball might roll over the hole and continue moving! If our learning rate is too low, we might never get to the hole in the first place. Keep in mind that the closer we get to the hole, the more our "training loss" will show progress.

Avery's team is looking at a flat training loss. This means they aren't seeing progress toward the solution, which is consistent with using a learning rate that's too low. Their hypothetical ball has too much friction and is not moving fast enough—or maybe at all—towards the solution, so it makes sense for the training loss to remain flat. Therefore, the first choice is correct.

Sometimes, we have a model that fits the training data too well. That might sound like something good, but it isn't. Models tend to "memorize" the training data and do very poorly on any future samples when this happens. A way to combat this problem is by using a concept that we call ["regularization."](https://theaisummer.com/regularization/)

Regularization controls the model complexity. Think of it as saving the model from itself. The more aggressive the regularization is, the harder for the model to memorize the training data and the more it will need to focus on learning from it. But what happens if we set the regularization too high? It could be challenging for the model to learn anything, so Avery's team would see a flat learning curve. Therefore, the second choice is also a possible explanation.

Working with imbalanced datasets is challenging because models tend to prioritize the majority class. Imagine you are building a model that tells dogs apart from cats, and 99% of your images are dogs. A simple model that always predicts a dog will be correct 99% of the time, but this would be useless. Regardless, if the model is learning anything, the training loss should decrease, so the third choice is not a reasonable explanation for Avery's problem.

Finally, we have to consider whether initializing a network's weights could not lead the model to learn. Here is an excerpt from ["Gradient Descent,"](http://www.cs.umd.edu/~djacobs/CMSC426/GradientDescent.pdf) a publication from the Computer Science Department of the University of Maryland:

> When a problem is nonconvex, it can have many local minima. And depending on where we initialize gradient descent, we might wind up in any of those local minima since they are all fixed points.

Let's get back to our ball rolling downhill and imagine that along the way, there are small holes that look like a solution, but they aren't. Initializing the network is like placing our ball in a specific position on the hill: If we start far from the optimal solution—the hole we want to reach—there will be a more significant probability of not getting there.

Moreover, we risk placing the ball where we will get stuck in one of those holes along the way: The network will think it finishes and stop learning. Therefore, the initialization can influence whether the network learns and the fourth choice is also correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Gradient Descent"](http://www.cs.umd.edu/~djacobs/CMSC426/GradientDescent.pdf) is a deep dive into gradient descent and its variants from the Computer Science Department of the University of Maryland.
* ["Understand the Impact of Learning Rate on Neural Network Performance"](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/) it's an excellent summary of the learning rate's impact when training neural networks.
* You can check out ["Regularization techniques for training deep neural networks"](https://theaisummer.com/regularization/) to learn more about regularization.
* The best in-depth explanation I've read about weight initialization is ["Initializing neural networks"](https://www.deeplearning.ai/ai-notes/initialization/), an interactive post from DeepLearning AI.</p></details>

-----------------------

## Date - 2022-08-05


## Title - The cheap, local zoo


### **Question** :

The local zoo hired a team to build a feature for their mobile application.

They wanted people to take a picture of an animal and have the model classify its species.

The zoo was government-funded and didn't have much money, so they hired the least expensive team they could find. That was obvious from the very first meeting.

After presenting the problem, the project lead admitted he wasn't sure about the best way to measure the model's success. He gave the zoo four possibilities and waited for them to answer.

**Which of the following metrics do you think are good ways to evaluate the model's performance?**


### **Choices** :

- The team could use the overall accuracy of the model.
- The team could use the F1-Score of the model.
- The team could use the precision of the model.
- The team could use the recall of the model.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Before we look at every possible choice, I want to draw your attention to something important: the question is about which metrics are good ways to evaluate the model, not the "best" metric.

The zoo needs a classification model. Are any of these metrics not valid for a classification model?

The team can use any of these metrics to evaluate the model since each is useful in different ways. Even better, they can use a combination of these metrics to understand various aspects of the final solution.

For example, accuracy is usually an excellent metric to describe the performance of a classification system. Accuracy doesn't work very well when dealing with imbalanced problems—where one class dominates the other—but we have no indication this will be the case. An advantage of using accuracy is that it is a very well-understood term, even among non-technical people.

[F1-Score](https://en.wikipedia.org/wiki/F-score) is another good metric to evaluate a classification model. It's a balance between the precision of the model and its recall. F1-Score works much better than the accuracy when the balance between classes is an essential factor. However, F1-Score is not as easy to interpret as the accuracy of a model.

[Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) are also beneficial to understanding the model's performance. For example, the team could monitor the precision of the model in classifying specific species or focus on its recall for others.

In summary, all four metrics are helpful for this particular problem.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check ["Evaluating Deep Learning Models: The Confusion Matrix, Accuracy, Precision, and Recall"](https://blog.paperspace.com/deep-learning-metrics-precision-recall-accuracy/) for a great introduction to how to evaluate a model.
- ["Accuracy vs. F1-Score"](https://medium.com/analytics-vidhya/accuracy-vs-f1-score-6258237beca2) contrasts these two metrics, just as ["F1 Score vs. Accuracy: Which Should You Use?"](https://www.statology.org/f1-score-vs-accuracy/) does.</p></details>

-----------------------

## Date - 2022-08-06


## Title - Infinite number of layers


### **Question** :

As she ran some experiments, Sadie noticed an interesting pattern with her classification model.

Sadie was building a model using a feed-forward neural network. She wanted to find the appropriate capacity for the model, so she started experimenting by adding new hidden layers and checking the final accuracy of the network.

**Which of the following describes what Sadie found as she increased the number of hidden layers in the network?**


### **Choices** :

- The network's accuracy on the training data decreased while the accuracy on the test data increased.
- The network's accuracy on the training data increased while the accuracy on the test data decreased.
- Both the network's accuracy on the training and test data increased.
- Both the network's accuracy on the training and test data decreased.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Sadie kept increasing the network's capacity by adding more hidden layers. Unfortunately, all of this additional power comes at a cost.

In other areas of computing, throwing more resources at a problem doesn't necessarily have negative consequences; we may waste the extra capacity, but it wouldn't influence the results. That is not the case with machine learning models.

How much capacity we have in a neural network is an essential factor in its ability to produce good results. The model will underfit if we don't have enough and overfit if we have too much. We need the network's capacity to be just right to get good results.

As Sadie adds more hidden layers, her neural network will start overfitting. All the extra power will lead the model to memorize the training data instead of trying to generalize. Think about that for a second: what would be the model results on the training data if the model memorized it?
 
The model's accuracy on the training data will improve. The more power we give the network, the better the results. Remember, the model is not learning much; it's just memorizing the data.

And how about the test data?

The model hasn't seen the test data, so regardless of how much capacity we add, it can't simply memorize it. Our only hope for the model to do well on the test data is if it generalizes appropriately, and in this case, we know the model is overfitting. As Sadie increases the network's capacity, the results on data that the model hasn't seen will start degrading quickly.

In summary, as Sadie increases the neural network's capacity, the model will start overfitting, increasing the accuracy of the predictions on the training data and decreasing the model's accuracy on the test data.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting With Machine Learning Algorithms"](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/) for an introduction to overfitting and underfitting in machine learning.
* ["How to Solve Underfitting and Overfitting Data Models"](https://allcloud.io/blog/how-to-solve-underfitting-and-overfitting-data-models/) covers several strategies to solve overfitting and underfitting.</p></details>

-----------------------

## Date - 2022-08-07


## Title - Time for tuning


### **Question** :

The team already had a machine learning model with decent results, but Pilar was sure they could do better.

They enrolled in a tabular competition, finally settling on a good solution after a few days. It was now time to try and squeeze as much as possible from their model.

Over the years, Pilar has used several techniques to tune her models' hyperparameters. She wanted to bring some of them in front of the team so they could all pick the best approach.

**Which of the following are hyperparameter tuning strategies that the team could use?**


### **Choices** :

- The team could use a Random search to sample the search space randomly.
- The team could use a Grid search to search through the hyperparameters exhaustively.
- The team could use a cross-validation scheme to search for the best hyperparameters.
- The team could manually search for the best combination of hyperparameters.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We use the term "hyperparameter" to refer to the settings we can use to control the learning process. We fix these "knobs" and "levers" before training a model. In contrast, we use "parameters" to refer to variables internal to the model whose values we estimate (learn) during the learning process using data.

A good way of thinking about this:

* Parameters: We learn their values during training. We do not set their values manually.
* Hyperparameters: The settings we fix before the learning process. We cannot learn these values during training.

Each model has different hyperparameters. For example, you can control the depth of a decision tree or the step size during the optimization process of a neural network.

[Hyperparameter tuning](https://en.wikipedia.org/wiki/Hyperparameter_optimization) is finding the optimal parameters for a learning algorithm on a particular problem. There are many different techniques, but the simplest one is to do it manually. The team could pick the few hyperparameters they need to optimize and try different combinations of values. They will keep the best-performing model at the end of the process. Therefore, the fourth choice is correct.

Manually tuning hyperparameters is not fun, and it becomes inefficient with too many hyperparameters. When working on complex problems, a better strategy is to use a search algorithm like [Grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search) or [Random search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Random_search).

Grids search is an algorithm that searches every possible combination of hyperparameter values. As you might imagine, grid search is not a feasible technique if the search space is too large (too many hyperparameters with an extensive range of potential values.) We don't have any information about Pilar's problem, so this choice is also correct.

Random search doesn't have the same problem with high-dimensional spaces. This technique searches randomly for the best combination of hyperparameters. It is one of the most popular hyperparameter tuning techniques, and Pilar will probably consider it.

Finally, a [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) scheme is a fundamental piece of the overall strategy, but it's not a way to search for optimal values. We need to validate the model results as we explore the search space and select different combinations of hyperparameters. Here is where the cross-validation scheme comes into play. By itself, cross-validation does nothing to find the optimal values.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Overview of hyperparameter tuning"](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview) is a great introduction to hyperparameters and the process to find their optimal value.
* ["7 Hyperparameter Optimization Techniques Every Data Scientist Should Know"](https://towardsdatascience.com/7-hyperparameter-optimization-techniques-every-data-scientist-should-know-12cdebe713da) is a quick overview of some of the most popular hyperparameter tuning techniques.
* [The Kaggle Book](https://amzn.to/3kbanRb) is a fantastic reference for those looking to participate in Kaggle.</p></details>

-----------------------

## Date - 2022-08-08


## Title - Nothing seems to work


### **Question** :

Mila started doing what others recommended: she split her dataset and trained a model on the split with most of the data. Unfortunately, her binary classification model was barely better than random chance.

She augmented the dataset with synthetic examples to no avail. She experimented with a different learning rate and added momentum, but the model struggled to learn the training data.

Mila knew her model was underfitting, but she didn't know what else to do.

**Which of the following techniques can Mila use to prevent underfitting?**


### **Choices** :

- Mila should unconstraint her model by removing any regularization.
- Mila should increase the network's capacity by using more layers and neurons.
- Mila should start using Batch normalization layers.
- Mila should start using Dropout.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We know that Mila's model is underfitting; it can't capture the relationship between her data and the target variable.

Usually, a way to work around underfitting is to ensure the model has enough capacity. In other words, we want to increase the complexity of the model and remove any constraints that would prevent it from thoroughly learning.

Looking at the available choices, the first two are sensible options that Mila should explore.

The first choice will remove regularization constraints from the model. Regularization is a technique we can use to prevent a model from overfitting. Think of it as "breaks" we add to the model to prevent it from "memorizing" the data. Too much regularization could cause the model to start underfitting.

The second choice will make the model more complex. By adding depth and width to the network, Mila will increase its capacity, giving the model a better chance to accommodate the training data appropriately.

Unfortunately, there's no guarantee either one of these two options will solve Mila's problem, but they are an excellent place to start.

The third choice is incorrect. [Batch normalization](https://www.analyticsvidhya.com/blog/2021/03/introduction-to-batch-normalization/) is a form of regularization, so it probably won't help Mila overcome her problem. [Dropout](https://programmathically.com/dropout-regularization-in-neural-networks-how-it-works-and-when-to-use-it/) is also a form of regularization, so it won't help Mila either.

In summary, the first and second choices are the correct answers to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["How to Control Neural Network Model Capacity With Nodes and Layers"](https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers/) is an excellent article about the effects of changing the capacity of a neural network.
* ["A Gentle Introduction to Batch Normalization for Deep Neural Networks"](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/) covers Batch normalization from top to bottom.
* ["Dropout Regularization in Neural Networks: How it Works and When to Use It"](https://programmathically.com/dropout-regularization-in-neural-networks-how-it-works-and-when-to-use-it/) will give you everything you need to know about Dropout.</p></details>

-----------------------

## Date - 2022-08-09


## Title - Pranking Howard


### **Question** :

Jim wants to prank Howard. They have had an on-and-off relationship for years, and Jim is looking to score some cheap points.

Howard is the company's data scientist, and he's been working on a machine learning model. Jim is not an expert, but he knows enough to realize that Howard is working with decision trees.

Jim wants to do something subtle that increases the complexity of the tree. He wants the model to start overfitting.

**Which of the following ideas is the one most likely to make Howard's tree more complex?**


### **Choices** :

- Jim should include more samples in Howard's dataset. The larger the dataset, the more complex the resulting model will be.
- Jim should remove some of the data that Howard is using. The smaller the dataset, the more complex the resulting model will be.
- Jim should modify the code and increase the maximum depth of the tree. The larger this value, the more complex the model will be.
- Jim should modify the code and decrease the maximum depth of the tree. The smaller this value, the more complex the model will be.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Jim is mean, but I trust him, and I'm sure Howard deserves it.

Jim wants Howard's model to start overfitting. He is considering two different levers to accomplish it: modifying the amount of data in the dataset or modifying the maximum depth of the tree.

You could argue that increasing the amount of data could potentially introduce patterns that increase the complexity of a decision tree—through samples that bring something new to the table. This, however, is not a given, so Jim should not do this.

Removing data is also not a good strategy. Jim will end up with the same complexity or have a less complex tree that will start underfitting, which is the opposite of what he wants.

Messing with the maximum depth, however, is a solid move. A [decision tree](https://en.wikipedia.org/wiki/Decision_tree_learning) represents particular observations about a sample in its branches and the conclusions about that item's target value as the tree's leaves. If you draw the tree, its maximum depth is a way to explain how tall the tree is.

Complexity increases proportionally with the depth of the tree. If Jim increases the maximum depth in the code, the next time Howard trains the model, the tree could try to accommodate more patterns than it needs to generalize appropriately. In other words, the larger the depth of a tree, the more likely Howard's model is to overfit. The third choice is the correct answer.

By the way, decreasing the maximum depth of the tree will probably result in underfitting. A less complex tree might not have enough capacity to learn the necessary patterns in the dataset.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Decision tree learning"](https://en.wikipedia.org/wiki/Decision_tree_learning) is the Wikipedia page where you can read about decision trees.
- ["How to tune a Decision Tree?"](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680) is a great article talking about different ways to tune a decision tree, including the effects of setting the maximum depth hyperparameter.</p></details>

-----------------------

## Date - 2022-08-10


## Title - Still time to compete


### **Question** :

Time was running out on Addison as she sat in front of her computer. 

The task was to get a neural network to train correctly, and Addison has made meticulous progress. There was only one problem left, and fortunately, she knew how to fix it: 

The neural network used stochastic gradient descent, causing the objective function to fluctuate heavily. On top of that, it was overshooting the global minima, and Addison didn't have time to experiment with the learning rate. Instead, she wanted to use Batch Gradient Descent.

**Which of the following statements is true about how this variant of the algorithm uses the dataset?**


### **Choices** :

- Batch Gradient Descent uses a single sample of data to compute the gradient of the cost function.
- Batch Gradient Descent uses a batch of data (more than one sample but fewer than the entire dataset) to compute the gradient of the cost function.
- Batch Gradient Descent uses the entire dataset to compute the gradient of the cost function.
- Batch Gradient Descent determines the optimal amount of data required to compute the gradient of the cost function.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Here is a simplified explanation of how [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent) works: We take samples from the training dataset, run them through the model, and determine how far our results are from what we expect. We then use this difference (error) to compute how much we need to update the model weights to improve the results.

A critical decision we need to make is how many samples we use to compute the gradient of the objective function. We have three choices:
* Use a single instance of data.
* Use all of the data at once.
* Use some of the data.

Using a single sample of data is called "Stochastic Gradient Descent" or SGD. Using all the data at once is called "Batch Gradient Descent." Finally, using some of the data—more than one sample but fewer than the entire dataset—is called "Mini-Batch Gradient Descent." 

Notice that we always make a tradeoff between the accuracy of the updates and the time it takes to calculate them. While using a single sample is faster, the updates will be more inaccurate, hence the fluctuations that Addison observed. On the other hand, using all the available data while producing more accurate updates requires storing the entire dataset in memory, which is very slow.

In summary, the third choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["An overview of gradient descent optimization algorithms"](https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants) for a deep dive into gradient descent and every one of its variants.
* ["Gradient Descent For Machine Learning"](https://machinelearningmastery.com/gradient-descent-for-machine-learning/) is another great introduction to gradient descent.
* ["A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size"](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) covers Batch and Mini-Batch Gradient Descent.</p></details>

-----------------------

## Date - 2022-08-11


## Title - A photography collection


### **Question** :

Audrey's love for photography is legendary.

She has spent most of her life taking beautiful pictures, and after so many years, she has built an impressive collection with hundreds of photos.

Audrey decided to take her art to the next level and start taking advantage of some of her deep learning knowledge. Her goal is to train a model on her collection of images to identify interesting patterns.

Audrey's never done this before, and she isn't sure how to use tensors to load her entire collection of images at once in memory.

**How many dimensions should Audrey use to load all of her photos at once in a tensor?**


### **Choices** :

- Audrey should use a 1-dimensional tensor.
- Audrey should use a 2-dimensional tensor.
- Audrey should use a 3-dimensional tensor.
- Audrey should use a 4-dimensional tensor.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When working with images, we need to store three different "components": the height, the width, and the color depth. Color images have three color channels (one for red, one for green, and one for blue), and grayscale images have a single color channel.

How many dimensions do we need to store an entire image if we don't care about colors? Assuming Audrey's pictures are of size 512 x 512, she can keep every pixel of a single image in a tensor of shape `(512, 512)`. 

But even with grayscale images, we always add a dimension to store the color depth by convention. That's nice because we can use the same structure for grayscale and color images. Audrey could keep one picture in a `(512, 512, 1)` tensor if she doesn't care about color. If she does, the tensor will be `(512, 512, 3)`.

Audrey needs three dimensions to store one of her photos, but she wants to load her entire collection. To do this, she needs another dimension. Assuming Audrey has 1,000 pictures, her tensor will look like `(1000, 512, 512, 3)`. 

When loading images in a tensor, the order for each dimension is intentional: (samples, height, width, channels). There's a different convention where channels go in front of the dimensions of the images, but most people use them at the end.

Therefore the fourth option is the correct answer to this question: Audrey needs a 4-dimensional tensor to store her collection of images.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Deep Learning with Python, Second Edition](https://amzn.to/3K3VZoy) covers the topic of tensors really well.
* Check ["A Gentle Introduction to Tensors for Machine Learning with NumPy"](https://machinelearningmastery.com/introduction-to-tensors-for-machine-learning/) for a quick introduction to tensors and practical code.</p></details>

-----------------------

## Date - 2022-08-12


## Title - Balancing costs


### **Question** :

Mariam has been working on predicting whether mechanical components need maintenance. She built a computer vision model, went through a successful pilot program, and her company is ready to start using the model in one of their warehouses.

You might remember Mariam. We already talked about her journey.

She ran pictures of 100 different components during the pilot program, and the model made four mistakes. The company, however, doesn't see them as equally important:

* Every false-negative mistake will cost the company $5,000.
* Every false-positive mistake will cost the company $100.

**Which one of the following metrics should Mariam prioritize when evaluating her model?**


### **Choices** :

- Mariam should prioritize getting a model with high accuracy.
- Mariam should prioritize getting a model with high precision.
- Mariam should prioritize getting a model with high recall.
- Mariam should prioritize getting a model with high f1 score.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>There's a critical detail here: false negatives are fifty times more expensive than false positives. Here is how the company looks at this situation:
* Every false-negative mistake means the company won't replace a component before it breaks, so downtime will occur when the part stops working. Every time this happens, the cost will be $5,000.
* Every false-positive mistake means that the company will have to send a technician to replace a component that is working correctly. It'll waste time, so every time this happens, the cost will be $100.

Mariam needs a model that prevents false negatives. In other words, the model should be good at detecting components that need maintenance, even if it will flag a few functional ones incorrectly.

Here is an idea to answer this question. We can compute the cost of two different scenarios and look at the value of each metric to decide which one points to the best model.

These are the two scenarios, each one with the model making five mistakes:
1. 100 components, 4 true positives, 91 true negatives, 3 false positives, and 2 false negatives. The cost of this scenario is $300 + $10,000 = $10,300.
2. 100 components, 5 true positives, 90 true negatives, 2 false positives, 3 false negatives. The cost of this scenario is $200 + $15,000 = $15,200.

The second scenario is much worse for the company, so we need to look at each one of the metrics and see which one would tell us that.

In both cases, the model's accuracy is 95%—5 mistakes in 100 samples. If Mariam prioritizes the model's accuracy, she won't be able to tell these two scenarios apart. Therefore, the first choice is incorrect.

Let's compute the [precision](https://en.wikipedia.org/wiki/Precision_and_recall#Precision) of each scenario:

```
precision = TP/(TP+FP)

precision1 = 4/(4+3)
precision1 = 4/7
precision1 = 0.57

precision2 = 5/(5+2)
precision2 = 5/7
precision2 = 0.71
```
If Mariam prioritizes the model's precision, she would incorrectly decide that the second scenario is better, which is not the case. Therefore, the second choice is incorrect.

Let's compute the [recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall) of each scenario:

```
recall = TP/(TP+FN)

recall1 = 4/(4+2)
recall1 = 4/6
recall1 = 0.66

recall2 = 5/(5+3)
recall2 = 5/8
recall2 = 0.62
```

The recall tells Mariam that the first scenario is better than the second, which is correct. Prioritizing recall is a good strategy.

And what about the [f1-score](https://en.wikipedia.org/wiki/F-score)? This metric is interesting because it looks for a balance of precision and recall. Intuitively, we know that always prioritizing recall without worrying about precision will not work. For example, imagine 10,000 components where a single one is not working. A model with 100% recall could predict that every one of the components needs maintenance, costing the company $100 for each of the 9,999 mistakes. This is not good, but would the f1-score address that problem?

Let's do the math for the two scenarios we have:

```
f1-score = 2*TP/(2*TP+FP+FN)

f1-score1 = 2*4/(2*4+3+2)
f1-score1 = 8/13
f1-score1 = 0.61

f1-score2 = 2*5/(2*5+2+3)
f1-score2 = 10/15
f1-score2 = 0.66
```

Here, the f1-score incorrectly points to the second scenario as the best outcome. The issue is that the f1-score balances the precision and recall equally, but we need a model biased towards higher recall. We can accomplish this using β > 1 in the [fβ-score](https://machinelearningmastery.com/fbeta-measure-for-machine-learning), but that's not a valid choice for this question. 

In this scenario, Mariam should prioritize the recall of her model, making the third choice the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check out ["Precision and recall"](https://en.wikipedia.org/wiki/Precision_and_recall) for a better understanding of these two metrics.
* ["Evaluating Deep Learning Models: The Confusion Matrix, Accuracy, Precision, and Recall"](https://blog.paperspace.com/deep-learning-metrics-precision-recall-accuracy/) is a great article that puts all of these concepts together.
- Check ["A Gentle Introduction to the Fbeta-Measure for Machine Learning"](https://machinelearningmastery.com/fbeta-measure-for-machine-learning) for an introduction to the fβ-score.</p></details>

-----------------------

## Date - 2022-08-13


## Title - Slow training


### **Question** :

The results were good, but the training process was painfully slow and inefficient. The team ran out of credits because they had to use some of the most expensive hardware to train their neural network.

It was Ivy who proposed the right solution.

Instead of using Batch Gradient Descent, she proposed Stochastic Gradient Descent. With a single change, they would speed up training and relieve pressure on the instances' memory. 
 
**Which of the following statements is true about Stochastic Gradient Descent?**


### **Choices** :

- Stochastic Gradient Descent uses a single sample of data to compute the gradient of the cost function.
- Stochastic Gradient Descent uses a batch of data (more than one sample but fewer than the entire dataset) to compute the gradient of the cost function.
- Stochastic Gradient Descent uses the entire dataset to compute the gradient of the cost function.
- Stochastic Gradient Descent determines the optimal amount of data required to compute the gradient of the cost function.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Here is a simplified explanation of how Gradient Descent works: We take samples from the training dataset, run them through the model, and determine how far our results are from what we expect. We then use this difference (error) to compute how much we need to update the model weights to improve the results.

A critical decision we need to make is how many samples we use to compute the gradient of the objective function. We have three choices:
* Use a single instance of data.
* Use all of the data at once.
* Use some of the data.

Using a single sample of data is called "Stochastic Gradient Descent" or SGD. Using all the data at once is called "Batch Gradient Descent." Finally, using some of the data—more than one sample but fewer than the entire dataset—is called "Mini-Batch Gradient Descent." 

Notice that we always make a tradeoff between the accuracy of the updates and the time it takes to calculate them. Ivy moved to SGD, and the process became much faster, but she will start seeing fluctuations in her loss function.

In summary, the first choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["An overview of gradient descent optimization algorithms"](https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants) for a deep dive into gradient descent and every one of its variants.
* ["Gradient Descent For Machine Learning"](https://machinelearningmastery.com/gradient-descent-for-machine-learning/) is another great introduction to gradient descent.
* ["A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size"](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) covers Batch and Mini-Batch Gradient Descent.</p></details>

-----------------------

## Date - 2022-08-14


## Title - Data splits


### **Question** :

Eleanor had just started her machine learning course. Over the first few lessons, the class covered working with data and preparing it to train a model.

Unfortunately, the course was somewhat prescriptive and rushed. Although it covered the need to split the data into train, validation, and test sets, it didn't spend any time explaining the reasons.

Eleanor knows that making progress without understanding the why behind every decision is not good.

**Select every choice from the following statements that explain why we split a dataset before training a model?**


### **Choices** :

- Having independent datasets can prevent the model from underfitting.
- Having independent datasets can prevent the model from overfitting.
- Having independent datasets can make the training process faster.
- Having independent datasets can accurately evaluate the model's performance.


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's talk about school for a minute.

Imagine teaching a math class, and it's time to evaluate your students. You decide to leave them 100 exercises as their homework. These problems cover the content they need to master for acing the exam.

How can you design an exam that effectively identifies those who learned the material?

Let's assume you pick 20 of the same homework exercises and use them in your test. This strategy might result in some false positives: students who memorize the solutions to their homework may get a high score, although they don't necessarily know how to reason. In machine learning, we call this "overfitting."

To ensure students don't overfit to their training exercises, you don't want to use the same homework to test their knowledge. Instead, you want to find new problems that evaluate the same material but are different enough to force the students to show their skills.

We want to do the same when training machine learning models. If we only evaluate our work in the same data we use to train the model, we might overfit and have a model that isn't capable of generalizing to different data. In other words, the model may "memorize" the training data and learn to return excellent predictions when tested.

If we split the dataset and leave a portion of it to evaluate how much the model learned, we will ensure that overfit won't happen. Therefore, the second and fourth choices are correct: we can accurately assess the model's performance and avoid overfitting.

Getting back to the previous analogy, those students that can't solve the homework in the first place are underfitting. Underfitting happens when the model cannot learn the training data, so we don't need separate splits to detect underfitting. The training data is enough, so the first choice is not correct.

Finally, we don't split the data to improve the training process speed; we do it to evaluate its performance accurately.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Overfitting and Underfitting With Machine Learning Algorithms"](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/) is an excellent article about overfitting, underfitting, and their differences.
* Check out ["Overfitting vs. Underfitting in Machine Learning: Everything You Need to Know"](https://neptune.ai/blog/overfitting-vs-underfitting-in-machine-learning) for another take on the same topic.
* ["Train, Validation, Test Split for Machine Learning"](https://blog.roboflow.com/train-test-split/) goes into detail about the importance of each split.</p></details>

-----------------------

## Date - 2022-08-15


## Title - The shoe reseller


### **Question** :

Nova worked for a shoe reseller in the computer vision department. 

She led the team that built their deep learning model to recognize shoes from pictures. They used a pre-trained ResNet50 as the foundation and fine-tuned it with a large dataset of shoe images.

Nova's model can classify 1,000 different shoe styles, and it's pretty good!

During Fashion Week, right after the model goes into production, a famous manufacturer releases a new style of their popular shoe brand.

**What would happen if we showed a picture of the new shoes to Nora's model? Select everything that could occur.**


### **Choices** :

- Assuming Nora's team trained the model properly, it should classify the new style correctly.
- The model will return an incorrect style with low confidence.
- The model will return an incorrect style with high confidence.
- The model will return an error because it doesn't recognize the new style.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Nora and her team trained their model on a dataset with 1,000 different classes—each class representing a particular shoe style. This is a classic supervised learning approach: you train a model to classify a sample into a group of pre-defined categories.

Unfortunately, their classification model can't correctly classify a style that the team didn't use during the training process, so the first choice is not correct.

The fourth choice is incorrect: the model will not return an error. [Classification models don't know what they don't know](https://twitter.com/svpino/status/1531968815063769089), so Nora's model will produce a prediction albeit an incorrect one.

We are left with two choices: Would the prediction's confidence be low or high? Intuitively, it would make sense to get a low confidence result. At the end of the day, the model is trying to classify a shoe style it hasn't seen before. Unfortunately, this is not the way it works.

The confidence that we get back from a classification model does not reflect how confident the model is in that prediction concerning the potential universe of possible results. That's how we think, but the "universe" of a model is limited to the categories we used to train it. In Nora's case, they need to look at a 95% confidence result for the other 999 categories, not as "the model it's 95% certain the image belongs to this particular style."

Therefore, we can't predict how the model's confidence will fluctuate for a particular style—either supported or unsupported. We may get a low or high confidence result, but the prediction will be incorrect in every case. Therefore, the second and third choices are both valid.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Out-of-distribution"](https://deepchecks.com/glossary/out-of-distribution/) for a quick introduction to the problem of trying to classify samples that differ from the training data.
* A Twitter thread covering why ["Classification models don't know what they don't know"](https://twitter.com/svpino/status/1531968815063769089).</p></details>

-----------------------

## Date - 2022-08-16


## Title - Cheating on the exam


### **Question** :

Cheating is not smart, but students do it every day.

The professor wasn't paying attention when somebody handed Aurora a piece of paper asking for help. They wanted to know how to configure the output layer of a neural network for different types of problems.

Aurora wanted to help, and although she wasn't entirely sure, she sent them back the notes below.

**Which of the following notes did Aurora get right?**


### **Choices** :

- For a regression problem, use a single neuron and no activation function.
- For a binary classification problem, use a single neuron and a sigmoid activation function.
- For a multi-class classification problem, use as many neurons as you have classes and a softmax activation function.
- For a multi-label classification problem, use as many neurons as you have classes and a sigmoid activation function.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Please, don't cheat in your exams. With that out of the way, every note Aurora sent was correct.

When we work on [regression](https://en.wikipedia.org/wiki/Regression_analysis) problems, we want the model to output a single, continuous value. For example, imagine we want to return the predicted price of a house or the predicted highest temperature for the weekend. We need a single neuron (value) to accomplish this, and we don't need any activation function to alter that value. 

For [binary classification](https://en.wikipedia.org/wiki/Binary_classification) problems, we can set up a one-neuron output layer, but we need a sigmoid activation function this time. We need two different results from binary classification models: one positive and one negative. We can do this using one value constrained between 0 and 1, and that's how a sigmoid function will help. We can then threshold the result and decide, for example, that any results equal to or greater than 0.5 is positive or negative otherwise.

In [multi-class classification](https://en.wikipedia.org/wiki/Multiclass_classification) tasks, we are looking to predict the correct class out of several choices. We can set up an output layer with one neuron for each potential class to accomplish this. After running a sample through the network, the largest output value will correspond to the predicted class. A softmax activation function will transform the output values into a probability distribution of every potential class.

Finally, when building [multi-label classification](https://en.wikipedia.org/wiki/Multi-label_classification) models, we need an output layer where every class is independent because we can have more than one active class per sample. We can accomplish this by using one neuron per class, but this time using a sigmoid activation function instead of a softmax function. The sigmoid will individually squeeze every neuron value between 0 and 1, so we can later pick every class over the threshold as the predicted labels for a sample.

Remember that the optimizer we use to train the networks is an essential difference between multi-class and multi-label classification problems. It's not enough with a different output activation function.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Difference between multi-label classification and multi-class classification"](https://towardsdatascience.com/multi-label-image-classification-with-neural-network-keras-ddc1ab1afede) is an excellent article comparing these two types of problems.
* Read the section "Activation for Output Layers" from ["How to Choose an Activation Function for Deep Learning"](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/) for a summary of different activation functions for the output layer of neural networks.</p></details>

-----------------------

## Date - 2022-08-17


## Title - A classification model


### **Question** :

Natalie has been a software developer for quite some time. Although her job keeps her motivated, she wants to take her career to the next level.

A critical step she wants to take is introducing machine learning into her work. She started learning some of the fundamentals and is now ready to apply what she's learned.

She researched one of her company's problems and learned she needed to build a supervised learning classification model. She had enough labeled data, so it seemed like a good fit.

**Based on this, which of the following better describes what Natalie needs to accomplish?**


### **Choices** :

- She needs to train a model that returns a numerical prediction for each sample of data.
- She needs to train a model that clusters the data into different groups based on their characteristics.
- She needs to train a model to predict the class of every sample of data out of a predefined list of classes.
- She needs to train a model that returns the optimal policy that maximizes the potential outcomes of her problem.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Natalie's problem requires her to predict the class of every sample of data out of a predefined list of classes. That's the goal of machine learning classification models.

The first choice refers to a [regression](https://en.wikipedia.org/wiki/Regression_analysis) model. Here we want the model to output a single, continuous value. For example, imagine we want to return the predicted price of a house or the predicted highest temperature for the weekend. 

The second choice refers to a [clustering](https://en.wikipedia.org/wiki/Cluster_analysis) model. These unsupervised learning techniques are helpful when we don't have labels for our data and want the algorithm to group every sample into dynamically generated groups.

The fourth choice is a loose description of a [reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning) approach, where we want an agent to learn the optimal policy that maximizes a reward function.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["4 Types of Classification Tasks in Machine Learning"](https://machinelearningmastery.com/types-of-classification-in-machine-learning/) is an excellent introduction to classification models in machine learning.
* For an introduction to classification models, check ["Classification in Machine Learning: What it is and Classification Models"](https://www.simplilearn.com/tutorials/machine-learning-tutorial/classification-in-machine-learning)</p></details>

-----------------------

## Date - 2022-08-18


## Title - Maximum depth


### **Question** :

One of the essential hyperparameters used in decision trees is their maximum depth.

But unfortunately, that's not the only thing the classroom had to remember for their test. There were many more hyperparameters with funny names and responsibilities, and they barely had time to prepare for the exam.

As soon as Isabella looked at the first section of the test, her heart dropped. It was all about decision trees, and she spent most of her time focusing on neural networks.

The first question didn't seem too tricky, however.

**Which of the following is the correct definition of the maximum depth of a decision tree?**


### **Choices** :

- The maximum depth of a decision tree is the length of the longest path from the tree's root to a leaf.
- The maximum depth of a decision tree is the length of the shortest path from the tree's root to a leaf.
- The maximum depth of a decision tree is the length of the longest path from the tree's root to an interior node.
- The maximum depth of a decision tree is the length of the shortest path from the tree's root to an interior node.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In a few words, a [decision tree](https://en.wikipedia.org/wiki/Decision_tree_learning) represents particular observations about a sample in its branches and the conclusions about that item's target value as the tree's leaves. If you draw the tree, its maximum depth is a way to explain how tall the tree is.

It makes sense that the maximum depth is the length of the longest path from the root to the bottom of the tree, but what exactly is that bottom? Is it a leaf or an internal node?

Your decision tree would not be complete if you only considered internal nodes. Remember, leaves are where the conclusions of the target value live. In other words, a leaf is where the final answer lives. You can't talk about the maximum depth of a tree without considering the leaves, so that's the correct answer to this question.

One more important thing to notice is that everything else being equal, a more straightforward decision tree will always generalize better than a more complex one. The maximum depth of the tree directly affects its complexity.

In summary, the first choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Decision tree learning"](https://en.wikipedia.org/wiki/Decision_tree_learning) is the Wikipedia page where you can read about decision trees.
- ["How to tune a Decision Tree?"](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680) is a great article talking about different ways to tune a decision tree, including the effects of setting the maximum depth hyperparameter.</p></details>

-----------------------

## Date - 2022-08-19


## Title - Hiring a team


### **Question** :

The company was getting ready to build an extensive system with a machine learning model at its core, so it was time to start hiring.

Jordan was one of the first. She came right off her Ph.D. to lead the creation of an automated data pipeline for the system. There was only one small problem: she didn't have industry experience, and her research work was very different.

Jordan needs to build a team, but it's hard to identify the appropriate candidates without fully understanding what goes into a data pipeline.

**Which of the following are some of the data processing steps that go into a data pipeline?**


### **Choices** :

- Data analysis is one of the steps of a data pipeline.
- Data ingestion is one of the steps of a data pipeline.
- Data preparation is one of the steps of a data pipeline.
- Data validation is one of the steps of a data pipeline.


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A critical part of any production-ready machine learning system is the data pipeline. Its goal is to collect and prepare the data we can use later to train, validate and use the model. 

A data pipeline is a fully automated process. There shouldn't be any steps that require human intervention.

Analyzing the data is not part of a data pipeline. During the data analysis phase, the team will define the data required by the system. This is a manual step that happens before the data pipeline is ready. During the data analysis phase, the team will make the necessary decisions to build the data pipeline. Therefore, the first option is incorrect.

Data ingestion is one of the steps of a data pipeline. During this step, we ingest the data from the different data sources and transfer it to where we will store it and later process it. For example, the company may need data currently stored in spreadsheets, databases, and a few real-time sensors. The data ingestion step will get the necessary data from each place and keep them in the final location for the rest of the process.

Data preparation is another essential step of a data pipeline, where we format and prepare the data before using it. For example, the company might want to standardize the format of dates collected from different sources or perform specific transformations on numerical data. 

Finally, data validation is another step of a data pipeline. We can validate that the necessary data we are ingesting is present and follow the appropriate schema during this step. For example, we might need to collect a minimum number of samples daily and raise an exception during the data validation step if we don't meet that condition.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The ["Machine Learning Data Lifecycle in Production"](https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production) course in Coursera, part of the [Machine Learning Engineering for Production (MLOps) Specialization](https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops).
* ["What is a Data Pipeline"](https://hazelcast.com/glossary/data-pipeline/) is a short article explaining the high-level idea of data pipelines.</p></details>

-----------------------

## Date - 2022-08-20


## Title - Abigail's chart


### **Question** :

Abigail firmly believes in charts. She always takes a few minutes to plot and support her story with compelling visuals.

She was ready to share her model with the rest of her team. This was her first machine learning experience, and she was proud of how much progress she had made in the last few weeks.

Abigail plotted her model's training and validation loss over the number of iterations (epochs). The cart looked like this:

![Learning Curves](https://user-images.githubusercontent.com/1126730/169589116-99acdd87-aedb-4e40-9f6e-3e1b353bf55d.jpg)

**Which of the following is the correct conclusion from looking at Abigail's plot?**


### **Choices** :

- The training loss decreases continuously, indicating that the model is doing well.
- The validation loss decreases to a point and starts increasing again, indicating the model is overfitting.
- The training and validation loss suddenly slowed down, indicating a problem with the learning process.
- The validation loss doesn't follow the training loss closely, indicating the model is underfitting.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Analyzing [learning curves](https://en.wikipedia.org/wiki/Learning_curve_(machine_learning)) is one of the fundamental skills you should build in your career. There are different learning curves, but we will focus on Abigail's chart.

First, notice that the chart shows the loss—or error—as we increase the number of training iterations. A good mental model is to look at this the following way: "as we keep training, how much better the model gets?" Since we are displaying the loss, larger values are worse, so having both lines decrease is a good sign.

We have two lines in the chart: one representing the loss we get during training, the other representing the loss during the validation process. How these lines look concerning each other is essential. Most of the time, one of the lines alone wouldn't give you a complete picture of the situation.

Let's start with the first choice that argues that a training loss that's continually decreasing is good. Indeed this could be a good sign in isolation: the more we train our model, the fewer mistakes we make on the training dataset, but this is not enough to draw any conclusions.

Always be suspicious of an always-decreasing training loss; it's a sign that your model might be memorizing the data. Usually, you want a model that learns up to a point, and then the loss stays flat. But how do you know when during the process that should happen? The relationship between the training loss and the validation loss shows how they start diverging at about 100 iterations. This point is the key.

Think about it this way: Abigail's model "continues learning" the training data but stops learning the validation data at about 100 iterations. The model is overfitting: it's memorizing the training data, which is not helping with the validation data. Therefore, the first choice is incorrect, and the second is correct.

The third choice is incorrect as well. You'll always see a similar slow down in the loss functions. The best case—not usual, but technically possible—is to reach 0, where there's nowhere else to go. 

Finally, the fourth choice is also incorrect. First, the training and validation loss do not necessarily need to follow each other. Second, this model shows overfitting—memorizing the training data—and not underfitting—lack of model complexity to learn the data correctly. You can usually identify an underfitting model when its training loss is flat or doesn't decrease much.

In summary, the second choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["How to use Learning Curves to Diagnose Machine Learning Model Performance"](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/) is the article that inspired this question and from where I borrowed Abigail's chart.
* Wikipedia's introduction to [Learning curves](https://en.wikipedia.org/wiki/Learning_curve_(machine_learning)).</p></details>

-----------------------

## Date - 2022-08-21


## Title - Low-bias models


### **Question** :

High bias models typically include more assumptions about the target function, while low bias models incorporate fewer assumptions about the target function. 

High bias models are simpler to interpret and usually run fast, but they don't scale well to complex problems. 

Juliette has already tried a few high-bias algorithms on her dataset and pushed them as far as possible. She is now ready to try something different.

**Which of the following algorithms are low-bias models that Juliette can try on her problem?**


### **Choices** :

- Linear Regression
- Decision Trees
- Logistic Regression
- k-Nearest Neighbors (KNN)


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Every machine learning algorithm deals with three types of errors: bias, variance, and irreducible error. We need to focus specifically on the bias error to answer this question.

Here is what [Jason Brownlee](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) has to say about bias: "Bias are the simplifying assumptions made by a model to make the target function easier to learn."

In other words, "bias" refers to the assumptions the model makes to simplify the process of finding answers. The fewer assumptions it makes, the less biased the model is.

Nonlinear models are usually low-bias. They don't make too many assumptions about the target function, making them an excellent option for tackling complex problems. Decision Trees and k-Nearest Neighbors are examples of low-bias models.

On the other hand, linear models are usually high-bias. They are easier to understand but make too many assumptions about the target function, preventing them from performing well on complex problems. Linear and logistic regression are two examples of high-bias models.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Here is Jason Brownlee's article I mentioned before: ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/).
* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).</p></details>

-----------------------

## Date - 2022-08-22


## Title - Three elevators


### **Question** :

Iris's company moved downtown. After closing on a large round of funding, they leased one of the tallest buildings in the city. 

The views were amazing, but the building was busy, and getting an elevator during peak hours was always a chore.

Iris's floor had three elevators unequally spaced along a wall. On a dull afternoon, while waiting for their ride to come, Iris asked her friends where they would stand and wait for the elevator. She wanted to minimize the distance they had to walk when one of the elevators arrived.

**Which of the following will minimize the distance they have to walk? Keep in mind the elevators are not spaced equally along the wall.**


### **Choices** :

- They should always stand in any of the elevators at both extremes of the wall.
- They should always stand in front of the middle elevator, regardless of its location along the wall.
- They should always stand at the mid-point between the two elevators located at the extreme.
- They should only stand in front of the middle elevator if the distance to the other two elevators is the same. If not, they can stand in any of the extreme elevators.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>If Iris stands in front of the middle elevator, she will minimize the distance she has to walk. This holds for any distribution of the elevators along the wall—they don't need to be equally spaced.

Let's break this down into the three possible situations with an example where we have elevators A, B, and C located along a wall (paraphrasing the answer on [this Stack Exchange page](https://stats.stackexchange.com/questions/355538/why-does-minimizing-the-mae-lead-to-forecasting-the-median-and-not-the-mean)):
1. If Iris waits in front of elevator A and elevator B arrives, she will need to walk from A to B. If elevator C arrives, she must walk from A to C, passing B on her way. Here is the pattern: A→B or A→B→C.
2. If Iris waits in front of elevator C and elevator A arrives, she will need to walk from C to A, passing B on her way. If elevator B comes, she must walk from C to B. Here is the pattern: C→B→A, or C→B.
3. If Iris waits in front of elevator B and elevator A arrives, she will need to walk from B to A. If elevator C arrives, she must walk from B to C. She will never have to pass any other elevator. Here is the pattern: B→A or B→C.

Do you see how standing in front of any of the elevators at both extremes of the wall will make you walk the same distance in multiple cases? A→B in the first case, and C→B in the second one.

["Visualizing the Median as the Minimum-Deviation Location"](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.610.3019&rep=rep1&type=pdf) explains this as follows:

> First, consider waiting at a median location. Then, consider "moving" to a waiting position away from this median point. If one does so, one moves away from more elevators than one moves towards, thereby increasing the sum of the distances to the elevators.

A more exciting insight: we can minimize the distance by standing at the median point between elevators for any number of elevators, not only three. Notice that this median point is not necessarily in front of an elevator.

Now that we know the correct answer is the second choice imagine if we wanted to minimize the total distance they have to walk from the door they enter the lobby where the elevators are located. It's no longer about the distance between the elevators, but we also need to consider the distance from the door to the elevator that arrives. [The answer may surprise you](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.610.3019&rep=rep1&type=pdf).</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Take a look at ["Visualizing the Median as the Minimum-Deviation Location"](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.610.3019&rep=rep1&type=pdf) for an in-depth analysis of this problem.
* ["Why does minimizing the MAE lead to forecasting the median and not the mean?"](https://stats.stackexchange.com/questions/355538/why-does-minimizing-the-mae-lead-to-forecasting-the-median-and-not-the-mean) is the question that led me down this rabbit hole.</p></details>

-----------------------

## Date - 2022-08-23


## Title - Bloody snake


### **Question** :

It's 2030, and you are playing a massive multi-player game in the Metaverse.

Don't ask how you got here. You don't have time for these questions. You need to focus on staying alive.

A massive snake with three heads and bloody eyes comes charging after you. It's not looking good! 

You have enough breath to read a clue written on the walls at the last second: "bootstrap it, and you'll live," it says.

**Where would you hide? Only one of these places will save your life:**


### **Choices** :

- Behind a tree
- In the forest
- In a cave
- In a hole


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>I hope this was a fun question.

As you would expect, the answer is in the clue written on the walls. "Bootstrap it, and you'll live" doesn't say much until you see the four hiding spots: A tree, a forest, a cave, and a hole.

[Bootstrap aggregating](https://en.wikipedia.org/wiki/Bootstrap_aggregating), also called "bagging," is a popular machine learning ensembling technique. We usually use bagging with decision trees, and one of the hiding spots is a tree, so that could be the answer! In reality, there's an even better answer.

[Random Forest](https://en.wikipedia.org/wiki/Random_forest) is an algorithm that consists of many individual decision trees. It uses bootstrap aggregating to combine these trees to reach a solution much better than the one provided by any of the individual trees.

"Bootstrapping" is more closely related to Random Forest than Decision Trees, so you'll stay alive if you hide in the forest.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Bagging and Random Forest Ensemble Algorithms for Machine Learning"](https://towardsdatascience.com/understanding-random-forest-58381e0602d2) is a great introduction to bagging and Random Forest.
* Check out ["Understanding Random Forest"](https://towardsdatascience.com/understanding-random-forest-58381e0602d2) to understand how it works and why it's effective.</p></details>

-----------------------

## Date - 2022-08-24


## Title - The political reporter


### **Question** :

Aria has been working with a source for the past four weeks to get information about hospitalized patients with COVID-19.

In her short career as a political reporter, she has never dealt with technical data, so imagine her surprise when she realized the information her source sent her was not in plain English.

The hospital has been using a machine learning model to predict whether patients sick with COVID-19 will end up in the hospital. The report came with a bunch of numbers, and it measured the quality of the model using two weird terms: "precision" and "recall."

Aria has to write her article, but first, she needs to figure out how to interpret the numbers.

**Which of the following statements accurately represent what precision and recall are?**


### **Choices** :

- Precision: Among all the patients classified by the model as potential hospitalizations, the fraction that ended up in the hospital.
- Precision: Among all the patients that ended up in the hospital, the fraction that the model correctly classified.
- Recall: Among all the patients classified by the model as potential hospitalizations, the fraction that ended up in the hospital.
- Recall: Among all the patients that ended up in the hospital, the fraction that the model correctly classified.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In machine learning, especially when building classification models, [precision](https://en.wikipedia.org/wiki/Precision_and_recall#Precision) and [recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall) are two of the most critical metrics we need to understand.

Let's start with the definition from Wikipedia:

Precision is the fraction of relevant instances among the retrieved instances, while recall is the fraction of relevant instances that were retrieved. 

Using this definition, we can break down what precision is by looking at two components:

1. Relevant instances: These are all patients the model predicted as positive and actually ended up in the hospital. We call these "true positive" patients.
2. Retrieved instances: These are all the patients the model predicted would end up in the hospital. 

The precision is the number of relevant instances divided by retrieved instances. We want to express how "precise" the model was: out of every patient the model predicted would end up in the hospital, how many were actually hospitalized. Looking at the available choices, the first is the correct definition of precision.

Let's do the same with recall. We need two components:

1. Relevant instances: These are all patients the model predicted as positive and actually ended up in the hospital. We call these "true positive" patients.
2. Positive instances: The number of patients that ended up in the hospital.

The recall is the number of relevant instances divided by the number of positive instances. In other words, we want to express the capacity of the model to find positive patients: out of every patient that ended up in the hospital, how many did the model retrieve. Looking at the choices, the fourth is the correct definition of recall.

In summary, the first and fourth choices are the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check out ["Precision and recall"](https://en.wikipedia.org/wiki/Precision_and_recall) for a better understanding of these two metrics.
* ["Precision vs Recall"](https://medium.com/@shrutisaxena0617/precision-vs-recall-386cf9f89488) is a great article covering the tradeoffs of precision and recall using a real-life example.</p></details>

-----------------------

## Date - 2022-08-25


## Title - Hailey's labeling approach


### **Question** :

Hailey had an excellent idea!

Despite having access to a massive dataset of x-ray pictures, she didn't have too many labels. The company couldn't afford to have doctors spending their time labeling samples, so she was in a catch-22 situation: to save the doctor's time, she wanted to build a supervised learning model, but she couldn't do that without having the doctors spend more time labeling data.

Hailey found a way to solve the problem: she built a model using the labels she had. Then started processing the unlabeled samples and setting aside those for which her model wasn't confident.

After each round, Hailey asked doctors only to spend time labeling those low-confidence samples, after which she built a new model and repeated the process. 

This approach saved the company a massive amount of money. 

**How would you classify Hailey's approach?**


### **Choices** :

- Hailey's approach is a Weak Supervision technique.
- Hailey's approach is a Reinforcement Learning technique.
- Hailey's approach is an Unsupervised Learning technique.
- Hailey's approach is a Semi-supervised Learning technique.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Hailey encountered a widespread problem we often face when we want to build machine learning models for companies. Sometimes, we have access to plenty of data, but labeling the dataset is time-consuming or hard to collect without a prohibitive cost.

Hailey did something clever: she only asked doctors to spend time labeling images that had the most value to her model. She built a good model with a fraction of the labels that otherwise would have been necessary.

We call this technique ["Active Learning"](https://essays.bnomial.com/active-learning), a semi-supervised learning technique. Hailey followed a [pool-based sampling](https://towardsdatascience.com/active-learning-in-machine-learning-525e61be16e5) approach, where she queried the pool of unlabeled samples and used the confidence of her model to decide which instances to label next.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Active Learning"](https://essays.bnomial.com/active-learning) is a short introduction to active learning and how the process works.
* If you are serious about Active Learning, ["Active Learning Literature Survey"](https://burrsettles.com/pub/settles.activelearning.pdf) is the publication you want to read.</p></details>

-----------------------

## Date - 2022-08-26


## Title - The weird puzzle room


### **Question** :

Vivian took her kids to an escape room. They had to solve a bunch of puzzles to get out.

But this room wasn't like every other room. 

They found the final puzzle behind a painting hanging from the wall. Four numbered little doors. The key hid behind one of them and a losing score behind the other three.

Above the doors, they found the question: "What's the optimal depth of a decision tree?"

**What door will let Vivian's family out of the room?**


### **Choices** :

- The optimal value for a decision tree's depth is setting it to the number of training samples minus one.
- The optimal value for a decision tree's depth is setting it to the logarithm of the number of training samples.
- The optimal value for a decision tree's depth is setting it as low as we possibly can.
- The optimal value for a decision tree's depth is setting it as high as we possibly can.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We need to revisit a few concepts and establish constraints to answer this question. How do we know a decision tree has an optimal depth?

First, the depth of a decision tree is the length of the longest path from a root to a leaf. There's a tradeoff to keep in mind related to this value. Deeper trees will lead to more complex models prone to overfitting. Shallower trees will have the opposite problem: less complex models prone to underfitting. Finding the appropriate depth is critical to getting good results.

To determine the optimal depth, we then need to establish how we will measure the performance of the decision tree. Since the goal is to build a model that produces good predictions, I'll assume that our goal is to find the optimal depth to get the best predictions possible on a test dataset.

Let's start with the first choice that argues that we should set the depth at the number of training samples minus one. This is the maximum theoretical depth of a decision tree, but it's not the optimal value. If we make the tree this deep, we will undoubtedly overfit the training data and perform poorly on any unseen data.

The second choice is also incorrect. Although this option is probably better than setting the depth to the number of samples, it will still lead to overfitting. For example, for a dataset with 1024 training samples, we will need a decision tree with 10 levels and a tree of depth 10, which requires a total of `2^10 = 1024` nodes. This choice will lead to a tree with as many nodes as samples, which will cause the tree to overfit.

We can analyze the third and fourth choices together. Are we better off setting the depth of a decision tree as low or as high as possible? Let's pick two hypothetical decision trees, each producing the same results, but one deeper than the other. Which of these two models would you use?

Less complex classifiers will usually generalize better than more complex ones. We should remove any non-critical or redundant sections of a decision tree, which generally leads to a much better model. [Decision tree pruning](https://en.wikipedia.org/wiki/Decision_tree_pruning) is one technique used to accomplish this.

If you have two decision trees with the same predictive power in your test dataset, always pick the simpler one. Therefore, we should always set the depth as low as possible without affecting the model's predictive capabilities.

In summary, the third choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["How to tune a Decision Tree?"](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680) is a great article talking about different ways to tune a decision tree, including the effects of setting the maximum depth hyperparameter.
- ["Decision tree pruning"](https://en.wikipedia.org/wiki/Decision_tree_pruning) is a Wikipedia article covering pruning and its effects.
- A great article to understand the relationship between the bias-variance tradeoff, overfitting, and under-fitting is ["How To Find Decision Tree Depth via Cross-Validation"](https://towardsdatascience.com/how-to-find-decision-tree-depth-via-cross-validation-2bf143f0f3d6).</p></details>

-----------------------

## Date - 2022-08-27


## Title - Predictive maintenance


### **Question** :

Mariam built a machine learning model that looks at pictures of mechanical components and predicts whether they need maintenance.

After successfully testing the model in a pilot project, the company plans to start using it in one of its warehouses. Before they are ready, Miriam needs to summarize the performance of the pilot project. 

She ran pictures of 100 different components through the model and found the following:

* The model predicted that 7 components needed maintenance. After manually inspecting them, only 5 required work, but the other 2 were okay. 
* The model predicted that 93 components were working as expected but missed 2 that needed maintenance.

**Which of the following is the correct summary of accuracy, precision, and recall for Miriam's model?**


### **Choices** :

- The model's accuracy is 93%, the precision is 29%, and the recall is 71%.
- The model's accuracy is 98%, the precision is 71%, and the recall is 29%.
- The model's accuracy is 90%, the precision is 98%, and the recall is 29%.
- The model's accuracy is 96%, the precision is 71%, and the recall is 71%.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The easiest way to answer this question is to put the information we know in a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). Whenever you build a classification model, the confusion matrix will help tremendously. Let's see how.

Here is the confusion matrix with Miriam's results. Notice the small annotations next to each value:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/168438082-c56db22f-935b-4e27-9d01-ca8f881a8ad1.png)

* Miriam's model found 5 true positives (TP). These are the components that the model predicted that needed maintenance that actually needed it.
* There are 2 false positives (FP). These are the other 2 components the model thought erroneously needed maintenance.
* There are 91 true negatives (TN). These are the components the model classified as okay and were indeed fine.
* There are 2 false negatives (FN). These are the 2 components that the model missed as needing maintenance.

We can compute the three metrics we need using this information. Let's start with accuracy:

```
accuracy = (TP+TN)/(TP+TN+FP+FN)
accuracy = (5+91)/(5+91+2+2)
accuracy = 96/100
accuracy = 0.96
```

The [precision](https://en.wikipedia.org/wiki/Precision_and_recall#Precision) of the model is the fraction of components that truly need maintenance among the components that the model predicts need it. We can compute the precision this way:

```
precision = TP/(TP + FP)
precision = 5/(5 + 2)
precision = 5/7
precision = 0.71
```

The [recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall) of the model is the fraction of components that truly need maintenance among all the existing components that need maintenance. We can compute the recall this way:

```
recall = TP/(TP + FN)
recall = 5/(5 + 2)
recall = 5/7
recall = 0.71
```

As you can see, the fourth choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Here is the Wikipedia page explaining what a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) is. Most of the information you need is here.
* Check out ["Precision and recall"](https://en.wikipedia.org/wiki/Precision_and_recall) for a better understanding of these two metrics.
* ["Evaluating Deep Learning Models: The Confusion Matrix, Accuracy, Precision, and Recall"](https://blog.paperspace.com/deep-learning-metrics-precision-recall-accuracy/) is a great article that puts all of these concepts together.</p></details>

-----------------------

## Date - 2022-08-28


## Title - The first exam


### **Question** :

Ayla is grading the first exam of her machine learning class.

Almost 50 students submitted a document explaining the neural network architecture they used to solve a problem and a complete analysis of their results.

Ayla opens the first paper and sees the following chart, showing the model's training and validation loss over the number of iterations (epochs):

![Learning Curves](https://user-images.githubusercontent.com/1126730/169652336-7646dbf8-c63d-4fac-92cf-c13f2881acaf.jpg)

**Which of the following is the correct feedback Ayla should give this student?**


### **Choices** :

- The training loss decreases continuously, indicating that the model is doing well.
- The separation between the training and the validation curves indicates the training set is too complex for the model to learn.
- The training and validation loss continue to decrease until the end of the training process, indicating the model is overfitting.
- The training and validation loss continue to decrease until the end of the training process, indicating the model is underfitting.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Analyzing [learning curves](https://en.wikipedia.org/wiki/Learning_curve_(machine_learning)) is one of the fundamental skills you should build in your career. There are different learning curves, but we will focus on Ayla's chart here.

First, notice that the chart shows the loss—or error—as we increase the number of training iterations. A good mental model is to look at this the following way: "as we keep training, how much better the model gets?" Since we are displaying the loss, larger values are worse, so having both lines decrease is a good sign.

We have two lines in the chart: one representing the loss we get during training, the other representing the loss during the validation process. How these lines look concerning each other is essential. Most of the time, one of the lines alone wouldn't give you a complete picture of the situation.

Let's start with the first choice that argues that a training loss that's continually decreasing means that Ayla's model is doing a good job. While this could be a good sign, it's not a sufficient explanation to draw any conclusions about the quality of the model. For example, the model could be memorizing the dataset (overfitting), and while the training loss is decreasing, we will end up with a flawed model. Therefore, the first choice is incorrect.

The chart shows a gap between the training and the validation loss. Even when the student didn't train the model for too long (about 50 epochs), the gap indicates the model can achieve a better loss in the validation dataset. This, however, doesn't mean the training dataset is too complex for the model to learn. The chart shows how the training loss continually decreases until the end of the process, so the second choice is also incorrect.

The critical insight is that the training and validation losses decrease during the entire process. What would you say if you were to guess what would happen right after the 50th epoch? Looking at the chart, it seems clear that both the training and validation losses have the potential to keep decreasing, and therefore the student stopped the training process too early.

What does this mean? Is the model overfitting or underfitting?

The model is not maximizing its potential. If we let it train for longer, the model will likely continue learning. This is a characteristic of underfitting models: they aren't taking full advantage of their data. Therefore, the third choice is incorrect, and the fourth is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["How to use Learning Curves to Diagnose Machine Learning Model Performance"](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/) is the article that inspired this question and from where I borrowed Ayla's chart.
* Wikipedia's introduction to [Learning curves](https://en.wikipedia.org/wiki/Learning_curve_(machine_learning)).</p></details>

-----------------------

## Date - 2022-08-29


## Title - Distance measures


### **Question** :

Distance measures are critical in many machine learning algorithms: they summarize the relative difference between two objects. K-Nearest Neighbors is an example of an algorithm that uses a distance measure at its core.

Every distance measure doesn't work for every problem. Instead, we must select the appropriate function depending on the nature of the data.

**Here is a list of four distance measures and a quick summary of how they work. Select every one of them that's correct.**


### **Choices** :

- The Euclidean distance between two vectors is the square root of the sum of the squared differences between them.
- The Manhattan distance between two vectors is the sum of their absolute differences.
- The Hamming distance is a generalization of the Euclidean and Manhattan distances that we can tune depending on which distance measure we need.
- The Minkowski distance computes the distance between two binary vectors.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>While the Euclidean and Manhattan distances summary is correct, the Hamming and Minkowski summaries aren't.

The Hamming distance computes the distance between two binary vectors. For example, you can calculate the distance between objects using a one-hot encoded feature.

The Minkowski distance is a generalization of the Euclidean and Manhattan distances. Both of these work with real-value vectors, but the Euclidean distance is the shortest path between objects, while the Manhattan distance is the rectilinear distance between them. Using the Minkowski distance, we can control which approach to use depending on the data.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["4 Distance Measures for Machine Learning"](https://machinelearningmastery.com/distance-measures-for-machine-learning/) for a complete explanation of these four distance measures.
* ["Five Common Distance Measures in Data Science With Formulas and Examples"](https://regenerativetoday.com/five-common-distance-measures-in-data-science-with-formulas-and-examples/) is a deeper dive into these distance measures.</p></details>

-----------------------

## Date - 2022-08-30


## Title - Softmax mockery


### **Question** :

Despite years of research, Loretta never paid too much attention to the softmax function and its properties. She never needed it, so imagine her surprise when Alan pushed her about it.

Alan was cooky. He didn't know much but liked to show off with every bit of trivia that crossed his path. More than anything, he enjoyed pissing off Loretta at every opportunity he had.

And here is Alan, bugging Loretta about the softmax function and ready to mock her if she fails.

**Which of the following are correct statements about the softmax function?**


### **Choices** :

- The softmax function is a soft or smooth approximation to the max function.
- The softmax function is a soft or smooth approximation to the argmax function.
- The softmax function converts a vector of `n` values into a probability distribution of `n` possible outcomes.
- Softmax is a differentiable function.


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The name "softmax" is a misnomer. The softmax function is a smooth—soft—approximation of the argmax function, not the max function. To avoid this confusion, some of the literature uses "softargmax" instead, but the machine learning world ran with "softmax" and never looked back.

The softmax function turns a vector of `n` values into another vector of `n` probabilities that sum to 1. These probabilities are proportional to the relative scale of each value in the vector. For example, when we use softmax as the activation function of a neural network's output layer, we get a normalized probability distribution that we can use to interpret the result of multi-class classification models.

Finally, contrary to the argmax function, the softmax function is differentiable.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["The Softmax function and its derivative"](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/) for a complete explanation of softmax.
* ["What is the Softmax Function?"](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer) is another great explanation of how the function works.</p></details>

-----------------------

## Date - 2022-08-31


## Title - Vintage shoes


### **Question** :

Sam owns a small store that sells vintage shoes. 

Business is going great, and one of her main issues is determining the price of new inventory as it comes in. Sam would like to create a machine learning model that automatically selects the best price using the historical performance.

Sam was a data scientist in a previous life, so she was surprised when she found some documentation recommending a new loss function: The Root Mean Squared Log Error (RMSLE.) This function seemed better than Root Mean Squared Error (RMSE) for her use case.

**Which of the following are some of the differences between RMSLE and RMSE?**


### **Choices** :

- RMSLE penalizes coming under the actual value much more than coming above the actual value. On the other hand, RMSE penalizes both cases in the same way.
- RMSLE is not sensitive to outliers as RMSE is. Whenever we have outliers, the result of RMSE can explode while RMSLE will scale down the outliers.
- RMSLE measures the error by only focusing on the predicted value. On the other hand, RMSE uses the difference between the predicted and the actual value to compute the error.
- RMSLE focuses on the relative error between two values. When using RMSLE, the scale of the error is not significant. On the other hand, the result of RMSE increases in magnitude if the error scale increases.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Compared to [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation), [RMSLE](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle)) is a relatively new metric. Here is the formula:

![RMSLE Formula](https://user-images.githubusercontent.com/1126730/168863453-215ebc00-201b-40f2-8cea-fcd998df3cd7.png)

If you compare this formula with the RMSE formula, you'll notice they are almost the same, with the difference of RMSLE computing the log of both the predicted and actual values. This small difference comes with some essential properties.

First, RMSLE penalizes underestimates much more than overestimates. To understand the reason, take a look at the following charts that I took from ["What's the Difference Between RMSE and RMSLE?"](https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a):

![RMSLE vs RMSE](https://user-images.githubusercontent.com/1126730/168865119-e9947726-a6c3-4a19-9b90-e8993bb3c105.png)

Notice how RMSLE penalizes negative values much more heavily than positive ones, while RMSE equally penalizes both cases. Therefore, the first choice is correct.

The second choice is also correct. The log that RMSLE uses squashes outliers, while RMSE amplifies their effect. If you want your model to remain unaffected by outliers, RMSLE is a good candidate.

The third choice is incorrect because both RMSE and RMSLE use the predicted and actual values. Predicted values alone don't tell us anything; comparing them with the actual value is critical.

Finally, the fourth choice is also correct. For example, imagine a case where the model predicts 30 when the actual value was 40 and another case where the prediction was 300 when the actual value was 400. The RMSE of the second case is ten times more than the RMSE of the first case. However, the RMSLE will score both cases the same. The RMSLE measures the relative error, while the RMSE measures the absolute error.

In summary, the first, second, and fourth choices are correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What's the Difference Between RMSE and RMSLE?"](https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a) covers really well every one of the differences between these two functions.
* Another article that covers the differences between some of the most popular functions is ["Evaluation Metrics for Regression models"](https://akhilendra.com/evaluation-metrics-regression-mae-mse-rmse-rmsle/).</p></details>

-----------------------

## Date - 2022-09-01


## Title - Bounding boxes


### **Question** :

Magdalena is working on a model to detect people in footage from security cameras.

She wants to measure the performance of her object detection method, but there is a problem: The bounding boxes predicted by her model never match precisely the bounding boxes in the ground truth data. 

Magdalena needs a metric that tells her how well the two bounding boxes overlap. Based on this metric, she can define a threshold and decide if the predicted bounding box is correct or not.

**Which metric can Magdalena use?**


### **Choices** :

- The percentage of the ground truth bounding box that's covered by the predicted bounding box.
- The percentage of the predicted bounding box that's covered by the ground truth bounding box.
- The intersection area of the predicted and ground truth bounding boxes divided by their union.
- The intersection area of the predicted and ground truth bounding boxes multiplied by their union.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>After a quick look at the choices, the first two options sound like plausible solutions. However, they have fundamental flaws that may allow a bad model to score high on these metrics.

Let's take the percentage of the ground-truth bounding box covered by the prediction. If our model learns to predict huge bounding boxes covering almost the whole image, we will always get a score close to 100%. 

If we take the percentage of the predicted bounding box covered by the ground truth, we have the opposite problem: a small bounding box somewhere in the bounds of the ground truth will give us a 100% score. Therefore the first two choices are incorrect.

An excellent way to deal with these problematic cases is to take the so-called intersection over union measure (IoU for short, also called the [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index)). We compute the intersection area of the bounding boxes and divide it by their union. The union will increase if the predicted bounding box is too big, driving the score down. The small intersection area will lower the score if the bounding box is too small. 

Finally, the last choice is also incorrect. Multiplying the intersection by the union will not give us any relevant or helpful results.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check ["Intersection over Union (IoU) for object detection"](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/) for an entire explanation of this metric.</p></details>

-----------------------

## Date - 2022-09-02


## Title - Stock prices


### **Question** :

George had an exciting meeting with his new client.

A fund manager wants George's team to design a machine learning model that evaluates the price of stocks. The model should use publicly available information to predict the true share price of each company. 

That's all the fund manager needs. He will take it from there and look deeper into under-priced stocks (where the true price is higher than the current market price.) 

The model will act as a pre-filter to sort out potential companies, and the fund manager will do a more detailed analysis before deciding whether to invest in the stock. The fund manager doesn't care if there are occasional wrong predictions, even by a large margin, but he doesn't want to have too many false positives because detailed research will cost a lot of time.

George is trying to decide which loss function the team should use for the model.

**Which of the following loss functions do you think are suitable for this problem?**


### **Choices** :

- Mean Squared Error (MSE)
- Mean Absolute Error (MAE)
- Binary Cross-entropy Loss
- Huber Loss


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We don't have too much information about the problem, but the client has clear priorities regarding the errors they are willing to tolerate. Significant outliers are not a big problem because the fund manager will spot them quickly and reject them. However, they expect excellent overall performance to avoid wasting time researching unpromising stocks.

With this in mind, the [Mean Absolute Error](https://en.wikipedia.org/wiki/Mean_absolute_error) (MAE) and the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss) seem like better options than the [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) (MSE.) The MSE penalizes high errors of the model much more than the others. This means that the model will try to learn to avoid big mistakes in the prediction, which will likely come at the cost of worse overall performance. We don't want this.

On the other hand, the MAE and the Huber loss are more robust to outliers, and they will not dominate the training process. Therefore, the second and fourth choices are the correct answers.

The cross-entropy loss doesn't apply to regression problems, so the third choice is also incorrect.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Regression Metrics for Machine Learning"](https://machinelearningmastery.com/regression-metrics-for-machine-learning/) for an overview of some of the most popular metrics used for regression problems.
* ["RMSE vs MAE, which should I use?"](https://stephenallwright.com/rmse-vs-mae/) is a great summary by Stephen Allwright about the properties of these two functions and how you should think about them.
* For more information about the Huber loss, take a look at ["Huber Loss: Why Is It, Like How It Is?"](https://www.cantorsparadise.com/huber-loss-why-is-it-like-how-it-is-dcbe47936473).</p></details>

-----------------------

## Date - 2022-09-03


## Title - Recognizing overfitting


### **Question** :

Cora doesn't know yet what overfitting is, but most of her class doesn't talk about anything else, so she expects to find out about it at any moment.

She started training models recently and would love to know how to recognize when one of her models overfit.

**If you were to summarize it for Cora, in which of the following situations is a model overfitting?**


### **Choices** :

- The training loss on the model stays constant during the entire training process.
- The model's performance on the training set is good but low on the validation set.
- The model's performance on the training and validation sets is low.
- The model takes an extremely long time to converge.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>If the training loss doesn't move during the training process, something is wrong, but we can't say that the model is overfitting. Overfitting is also not related to the speed of convergence. There may be multiple reasons a model takes a long time to converge, but we can't claim that a model is not converging fast enough because it's overfitting.

When a model is overfitting, we should see a good performance on the training set alone. A good model should capture valuable patterns in the data and discard any noise that doesn't help with predictions, leading to good performance in both the training and validation sets. The third choice argues for low performance on both training and validation sets, which will be a way to detect underfitting.

In summary, the second option is the correct answer: Cora should expect good performance on the training set but low validation performance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting With Machine Learning Algorithms"](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/) for an introduction to overfitting and underfitting in machine learning.
* ["How to Solve Underfitting and Overfitting Data Models"](https://allcloud.io/blog/how-to-solve-underfitting-and-overfitting-data-models/) covers several strategies to solve overfitting and underfitting.</p></details>

-----------------------

## Date - 2022-09-04


## Title - Initializing neural networks


### **Question** :

High-level libraries are usually enough to solve complex problems without worrying about low-level details.

But Gabriella didn't have that luxury. Her research got her very deep into how neural networks work. 

As part of the project, she wants to write a blog post about the influence of different initialization schemes on how neural networks learn. She wants to cover the problems that could happen when we use the wrong initialization.

**Which of the following problems could we face if we don't initialize the weights correctly?**


### **Choices** :

- The network might suffer from vanishing gradients, which could cause a slow down in training.
- The network might suffer from exploding gradients, preventing it from learning correctly.
- Every network neuron might learn the same features during training.
- Every neuron of the network might learn different features during training.


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Correctly initializing a neural network can have a significant impact on convergence.

We may suffer from the vanishing gradient problem if we use small values to initialize the network's weights and the exploding gradient problem if we use large weight values instead. The gradients will become smaller or larger as we move from the output layer toward the input layer during backpropagation. ["Initializing neural networks"](https://www.deeplearning.ai/ai-notes/initialization/) illustrates a specific example with a 9-layer neural network using the identity function as the activation on every layer. 

The third choice is correct: if we initialize the network with zeros, every neuron will learn the same features. Here is an excerpt from ["Initializing neural networks"](https://www.deeplearning.ai/ai-notes/initialization/) explaining the consequences of using the same weight values:

> Thus, both hidden units will have identical influence on the cost, which will lead to identical gradients. Thus, both neurons will evolve symmetrically throughout training, effectively preventing different neurons from learning different things.

The final choice, however, is not a problem. We want every neuron to learn different features.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Initializing neural networks"](https://www.deeplearning.ai/ai-notes/initialization/) is an excellent summary of the importance of weight initialization.
* Check ["Weight Initialization Techniques in Neural Networks"](https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78) to learn about different initialization schemes.</p></details>

-----------------------

## Date - 2022-09-05


## Title - Zombies taking over


### **Question** :

Every TV in the country didn't talk about anything else: zombies were finally taking over the world.

But there was still hope.

Adeline was part of the team dedicated to finding a cure to transform the zombies back. She wasn't a machine learning expert but knew how to build logistic regression classifiers. Her job was to categorize zombies based on data captured by field teams.

**Assuming that there are five categories of zombies and that Adeline is only working with logistic regression models, how many models does she need to build to classify zombies correctly?**


### **Choices** :

- Adeline can classify every zombie correctly by building a single model.
- Adeline can classify every zombie correctly by building 5 models.
- Adeline can classify every zombie correctly by building 24 models.
- Adeline cannot classify every zombie using logistic regression. She needs a multi-class classification model instead.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Adeline can solve the problem using logistic regression alone.

[Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) estimates the probability of an event occurring by outputting a single value bounded between 0 and 1. The closer the result is to zero, the less likely the event will occur, while the closer is to 1, the more likely it is. This structure makes logistic regression ideal for tackling binary or 2-class classification problems. Notice that we can formulate a binary classification task as a 2-class classification task and vice versa.

Adeline needs to classify zombies into five different categories. She can use the One-vs-All method (also called "One-vs-Rest.") She will need to train five models:

* Model 1: Identifying Class 1 vs. [Class 2, Class 3, Class 4, Class 5]
* Model 2: Identifying Class 2 vs. [Class 1, Class 3, Class 4, Class 5]
* Model 3: Identifying Class 3 vs. [Class 1, Class 2, Class 4, Class 5]
* Model 4: Identifying Class 4 vs. [Class 1, Class 2, Class 3, Class 5]
* Model 5: Identifying Class 5 vs. [Class 1, Class 2, Class 3, Class 4]

After running the data through every one of these five models, the correct category would be the prediction from the model with the highest confidence.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check out ["Logistic Regression for Machine Learning"](https://machinelearningmastery.com/logistic-regression-for-machine-learning/) for an introduction to Logistic regression.
- ["Essential Data Science Tips: How to Use One-Vs-Rest and One-Vs-One for Multi-Class Classification"](https://www.kdnuggets.com/2020/08/one-vs-rest-one-multi-class-classification.html) is a great reference to understand how to use the One-vs-All method for multi-class classification.</p></details>

-----------------------

## Date - 2022-09-06


## Title - Cheating with a model


### **Question** :

Skylar's model had the best performance in the entire class, but there was a problem: the results were too good to be correct. 

The professor decided to review her process step by step. Skylar stood in front of the class and wrote down her process:

1. Skylar loaded the entire dataset in memory.
2. Replaced one column's missing values with the mean of the column and scaled another column using Mix-Max Scaling.
3. She then split the dataset into a train and a test set.
4. And finally, she trained her model.

As soon as she finished, the professor knew what the issue was. 

**Which of the following is the reason for the model's inflated performance?**


### **Choices** :

- Skylar loaded the entire dataset in memory. She should have loaded it in batches.
- Skylar transformed her data before splitting the dataset. She should have split the data before transforming it.
- Skylar used the mean of the column to impute missing values. She should have used the median instead.
- Skylar used Min-Max Scaling to transform one of the columns. She should have applied log transformation instead.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>**Rule of thumb:** When your model's results seem too good to be true, they probably are.

The problem with Skylar's model is a data leak: she transformed her dataset before splitting it. When computing the mean of a column or applying [Min-Max Scaling](https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization)), Skylar used the test data, which she isn't supposed to have. 

It's subtle but essential. 

When doing any work with your data, ensure you only look at your train set. Test data is akin to production data: you don't have it during training time and can't make any decisions based on it.

In this example, Skylar imputed missing values using the mean of the entire column, including those samples that later became part of the test set. She also used Min-Max Scaling, which uses the minimum and maximum values of the column. She leaked information from the soon-to-be test dataset into her training process in both cases.

This leak caused Skylar's model to perform much better than others. She cheated without realizing it.

The solution is to split the dataset before doing any transformations. Skylar should compute the training data's mean, minimum, and maximum values and use them to impute and scale the test set.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check [Data Leakage in Machine Learning](https://machinelearningmastery.com/data-leakage-machine-learning/) for an introduction to data leaks and how to prevent them. 
* The [Wikipedia page on leakages in machine learning](https://en.wikipedia.org/wiki/Leakage_(machine_learning)) covers the topic very well.</p></details>

-----------------------

## Date - 2022-09-07


## Title - Normalizing inputs to a layer


### **Question** :

What if we normalize the input to the layers within a neural network?

Liliana understood the idea of data normalization, but this new approach seemed bold and different.

Fortunately, she found plenty of documentation about this technique, called Batch Normalization. During the training process, Batch Normalization normalizes the input to each layer within the network.

**Liliana wants to write some notes before calling it a day. Which of the following are correct statements about Batch Normalization?**


### **Choices** :

- Batch Normalization rescales the data to a mean of zero and a standard deviation of one.
- Batch Normalization rescales the data between zero and one.
- Batch Normalization makes the network less sensitive to the choice of weight initialization.
- Batch Normalization requires extra computations, slightly increasing the network's total training time.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Batch Normalization is a popular technique used to train deep neural networks. It normalizes the input to a layer during every training iteration using a mini-batch of data. It smooths and simplifies the optimization function leading to a more stable and faster training.

Batch Normalization works by scaling its input—the previous layer's output—to a mean of zero and a standard deviation of one per mini-batch. Thus the first choice is correct and the second incorrect.

Although correctly initializing a network can significantly impact convergence, the stability offered by Batch Normalization makes training deep neural networks less sensitive to a specific weight initialization scheme. Since Batch Normalization normalizes values, it reduces the likelihood of running into vanishing or exploding gradients.

Finally, Batch Normalization does require extra computations, making individual iterations slower. However, it will dramatically reduce the number of iterations needed to achieve convergence, making the training process much faster. Therefore, the fourth choice is incorrect.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["A Gentle Introduction to Batch Normalization for Deep Neural Networks"](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/) is a good introduction to Batch Normalization.
* Another great article to understand the impact of Batch Normalization is ["Why is Batch Normalization useful in Deep Neural Networks?"](https://towardsdatascience.com/batch-normalisation-in-deep-neural-network-ce65dd9e8dbf)</p></details>

-----------------------

## Date - 2022-09-08


## Title - Leave one out


### **Question** :

Rosie discovered that her model had worse performance when using a different test set.

She started the project by setting aside a portion of her data and built a model that, so far, impressed everyone with a strong performance. But the bubble burst when she trained and tested the model with a different portion of her dataset.

Rosie's team recommended k-Fold cross-validation, but Rosie wanted to take the process to the extreme: she decided to create as many folds as samples in the dataset.

**Which of the following statements correctly describes what Rosie should expect to happen:**


### **Choices** :

- Rosie's method will be computationally cheaper than using fewer folds and result in a more reliable estimate of model performance.
- Rosie's method will be computationally more expensive than using fewer folds but will result in a more reliable estimate of model performance.
- Rosie's method will be computationally cheaper than using fewer folds but result in a less reliable estimate of model performance.
- Rosie's method will be computationally more expensive than using fewer folds and result in a less reliable estimate of model performance.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Rosie's approach has a name: [Leave-one-out cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation), a case of cross-validation where the number of folds equals the number of samples in the dataset. 

To use leave-one-out cross-validation, we build one model for each sample in the dataset. We train each model using all data except one instance we later use to evaluate its performance. Finally, we compute the overall performance by averaging the result of each model.

Assuming Rosie uses leave-one-out cross-validation on a dataset with 10,000 samples, she will need to train 10,000 models. Compare this with 10-Fold cross-validation, where she will only need to build ten models. Leave-one-out cross-validation is significantly more expensive than a process using fewer folds.

On the other hand, leave-one-out cross-validation will give Rosie a more robust estimate of model performance. Each sample has an opportunity to represent the entire dataset and contribute to the final evaluation, and this will result in a reliable and unbiased estimate of model performance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["LOOCV for Evaluating Machine Learning Algorithms"](https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/) is an excellent introduction to leave-one-out cross-validation.
* Check ["A Quick Intro to Leave-One-Out Cross-Validation (LOOCV)"](https://www.statology.org/leave-one-out-cross-validation/) for a brief introduction to leave-one-out cross-validation.</p></details>

-----------------------

## Date - 2022-09-09


## Title - Universal approximators


### **Question** :

A theoretical question that Daisy always liked to discuss was the ability of machine learning algorithms to approximate any function.

She called them "universal approximators." To classify, they had to approximate any measurable or continuous function up to any desired accuracy.

**Which of the following would you consider universal approximators?**


### **Choices** :

- Neural Networks
- Support Vector Machines using a Gaussian kernel
- Linear Regression
- Boosted Decision Trees


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Except for linear regression, all the other three choices are universal approximators.

Thanks to the [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem), neural networks are a well-known member of this category. The theorem states that, when using non-linear activation functions, we can turn a two-layer neural network into a universal function approximator.

The authors of ["A Note on the Universal Approximation Capability of Support Vector Machines"](https://www.researchgate.net/publication/2531186_A_Note_on_the_Universal_Approximation_Capability_of_Support_Vector_Machines) show that support vector machines with standard kernels can also approximate any measurable or continuous function.

Finally, the ["Universal approximation"](https://kenndanielso.github.io/mlrefined/blog_posts/12_Nonlinear_intro/12_5_Universal_approximation.html) chapter of [_Machine Learning Refined_](https://amzn.to/3OXcHZF) covers trees as one of the standard types of universal approximations.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["A Note on the Universal Approximation Capability of Support Vector Machines"](https://www.researchgate.net/publication/2531186_A_Note_on_the_Universal_Approximation_Capability_of_Support_Vector_Machines) is a paper showing that SVMs with standard kernels can approximate any measurable or continuous function.
* Chapter 12.5 ["Universal approximation"](https://kenndanielso.github.io/mlrefined/blog_posts/12_Nonlinear_intro/12_5_Universal_approximation.html) of [_Machine Learning Refined_](https://amzn.to/3OXcHZF) covers kernels, neural networks, and trees as universal approximations.</p></details>

-----------------------

## Date - 2022-09-10


## Title - Missing answers


### **Question** :

Amaya's company surveyed a large group of people. After collecting and digitizing all of the data, she noticed that many participants didn't answer one particular question.

It was clear to Amaya that not disclosing the answer to the question was as important as the answer itself, so she wanted to consider this when preparing the data.

Amaya is ready to build a machine learning model but must first deal with the missing answers.

**Which of the following should be the best strategy that Amaya should pursue?**


### **Choices** :

- Amaya should remove the column that contains the missing values.
- Amaya should add a new column to the dataset to flag rows with missing values and then replace those values with a reasonable answer.
- Amaya should replace the missing values with a reasonable answer.
- Amaya should predict the missing values using a separate machine learning model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Amaya knows she must do something with the missing answers before training a machine learning model, and [imputing](https://en.wikipedia.org/wiki/Imputation_(statistics)) these values seem like a good solution. For example, she could replace the missing values with their mean, median, or mode.

But this approach has a problem: Amaya thinks having many participants not disclose the answer to the question is as important as the answer itself. If she replaces the missing values, she will lose that information.

A great approach to avoid losing that information is to add a new binary column to the dataset. This column will flag every participant that didn't answer the question. Assuming avoiding to answer is not a random event, this new column will help with the predictive performance of Amaya's model.

Therefore, the best strategy for Amaya is to add this new column right before imputing missing values.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Handling Missing Values"](https://www.kaggle.com/dansbecker/handling-missing-values) is a Kaggle notebook that goes over a few strategies to handle missing values including adding an extra column to flag these rows.
- Check ["Imputation of missing values"](https://scikit-learn.org/stable/modules/impute.html#imputation-of-missing-values) for some information about how Scikit-Learn handles missing values.
- ["How to Handle Missing Data with Python"](https://machinelearningmastery.com/handle-missing-data-python/) is an excellent post discussing different strategies in Python.</p></details>

-----------------------

## Date - 2022-09-11


## Title - Inattentive drivers


### **Question** :

Summer started working for Tesla. 

Her first assignment is to build a model that will use the camera in the cabin to detect whether drivers are paying attention or are too tired.

The system's ultimate goal is to provide audio and visual alerts to help drivers stay safe. Tesla hopes this feature will significantly decrease the number of accidents.

**Before the work starts, Summer wants to decide which of the following will be the best way to evaluate the model:**


### **Choices** :

- Summer should use the recall of the model, while keeping an eye on its precision.
- Summer should use the accuracy of the model.
- Summer should use the precision of the model, while keeping an eye on its recall.
- Summer should use the training loss of the model.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>There will be many more situations where Summer's system will find that the driver is correctly paying attention than not. If she uses the accuracy of her model as the evaluation metric, she could end up with a struggling model that still shows high accuracy. Remember that [accuracy is not a good metric when facing an imbalanced problem](https://articles.bnomial.com/when-accuracy-doesnt-help). You can achieve very high accuracy even with a model that does nothing useful.

Let's assume Summer's model misses most cases where the driver is tired and not paying attention. Think of a model that detects a single issue correctly, doesn't have any false positives, and ignores all other dangerous situations. That model will have perfect precision, but it won't help much in reducing traffic accidents because it doesn't detect every problematic situation. Precision is not a good metric for this problem.

The training loss of the model won't help Summer either. We use the loss to determine whether the model learned, but it doesn't tell us anything about the capacity of the model to detect inattentive drivers. Like accuracy, we could have a low loss model that doesn't do well with problematic situations.

Summer should focus on the recall of her model while keeping an eye on its precision. A high recall will ensure that Summer's model detects as many problematic situations as possible.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.
* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.</p></details>

-----------------------

## Date - 2022-09-12


## Title - Broken bottles


### **Question** :

Parker works at a drink factory concerned with classifying defects as bottles come out of the line. She built a computer vision model to classify bottles into three classes: "ready," "almost ready," and "waste."

Most of the bottles that come out are "ready," and there are only a few samples that classify as "almost ready" or "waste." Parker is very aware of this imbalance.

The factory uses Parker's model to reduce the number of defective bottles that ship to customers (any bottle that's not "ready" is defective.) They consider each of the three classes equally important and wants to ensure the evaluation process reflects that.

**Which metric should Parker use to evaluate her model?**


### **Choices** :

- The accuracy of the model.
- The Micro-average F1-Score of the model.
- The Macro-average F1-Score of the model.
- The Weighted F1-Score of the model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Since Parker is working on a very imbalanced problem, she shouldn't use accuracy to evaluate her model's quality. If 99% of the bottles are "ready," Parker's model will have 99% accuracy, even if she ignores every sample belonging to the other two classes.

The F1-Score is helpful in these situations, but it depends on how we compute it in a multi-class classification problem. 

The Micro-average F1-Score calculates the proportion of correctly classified samples out of all instances, the same as the model's accuracy definition. Therefore, if we use the Micro-average F1-Score, we'll have the same issue we see when using accuracy.

The Weighted F1-Score scales the individual F1-Score of each class. This method favors the majority classes, which in Parker's case are the bottles classified as "ready." The factory considers every category equally important, and that's why the Weighted F1-Score is not the correct answer.

Finally, the Macro-average F1-Score penalizes the model equally for any class that doesn't perform well, regardless of its importance or how many support samples it has. This metric is the correct answer where every category is important, irrespective of how many instances we have in the dataset. This is the metric that Parker should use to evaluate her model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.
* ["Micro, Macro & Weighted Averages of F1 Score, Clearly Explained"](https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f) is a great article covering how to compute a global F1-Score metric in multi-class classification problem.
* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.</p></details>

-----------------------

## Date - 2022-09-13


## Title - Job descriptions


### **Question** :

Millie finished collecting a dataset with job descriptions and their corresponding salary ranges. 

Unfortunately, there's a lot of noise in the dataset, and Millie is not sure how to proceed.

A colleague recommended bagging, but Millie is unfamiliar with this technique. She looked into it and came up with a few questions.

**Select every statement below that's correct about bagging:**


### **Choices** :

- Bagging is an effective technique to reduce the variance of a model.
- Bagging is an effective technique to reduce the bias of a model.
- Bagging trains a group of models, each using a subset of data selected randomly without replacement from the original dataset.
- Bagging trains a group of models, each using a subset of data selected randomly with replacement from the original dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Bagging is a popular ensemble learning technique.

Bagging trains a group of models in parallel and independently from each other. Each model uses a subset of the data randomly selected with replacement from the original dataset. That's why bagging is also known as "bootstrap aggregating:" because it draws bootstrap samples from the training dataset. 

Bagging is an excellent approach to reducing the variance of a model, but it doesn't help reduce the bias. That's why we often use it with low-bias models, like unpruned decision trees. Here is a relevant quote from ["What is bagging?"](https://www.ibm.com/cloud/learn/bagging) about how it helps reduce the variance of a model:

> Bagging can reduce the variance within a learning algorithm. This is particularly helpful with high-dimensional data, where missing values can lead to higher variance, making it more prone to overfitting and preventing accurate generalization to new datasets.

In summary, the first and fourth choices answer this question correctly.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check IBM's ["What's Bagging?"](https://www.ibm.com/cloud/learn/bagging) summary for a detailed explanation of this technique.
* [What is the difference between Bagging and Boosting?](https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/) is a great summary of bagging and boosting and their advantages and disadvantages.</p></details>

-----------------------

## Date - 2022-09-14


## Title - The final exam


### **Question** :

Emersyn was running out of ideas to create new questions for the final exam.

In a last attempt to do something original, she copied an example code fragment from the Keras website and decided to ask a few questions about it:

```
model = keras.Sequential([
    keras.Input(shape=(28, 28, 1)),
    layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Flatten(),
    layers.Dropout(0.5),
    layers.Dense(10, activation="softmax"),
])
```

**Based on the code above, which of the questions Emersyn wrote are correct?**


### **Choices** :

- The kernel and pool size should have the same value. The model will likely fail to learn anything meaningful.
- The default activation function in `MaxPooling2D` layers is ReLU, which is why the code doesn't explicitly use it.
- The Dropout right before the output layer will cut down the number of learnable parameters from 1,600 to 800.
- The softmax activation function in the last layer hints that this is a multi-class classification model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The kernel size in convolutional layers and the pool size of the pooling layer don't need to have the same size. The code fragment is indeed part of a working example, and it's correctly configured.

[`MaxPooling2D`](https://keras.io/api/layers/pooling_layers/max_pooling2d/) layers don't use activation functions. That's the reason the code doesn't specify one. A max pooling operation calculates the maximum value in each patch of each feature map. Keras will throw an error if you try to set an activation function on the `MaxPooling2D` layer.

The Dropout right before the output layer doesn't reduce the number of learnable parameters in half. Instead, the Dropout will set the value of 50% of the neurons to 0.

Finally, the [softmax](https://en.wikipedia.org/wiki/Softmax_function) activation function and the size of the output layer point to a multi-class classification problem. Remember that softmax converts a vector of numbers into a vector of probabilities, which we need in multi-class classification tasks.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For more information about convolutional layers, check ["How Do Convolutional Layers Work in Deep Learning Neural Networks?"](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/).
* Check ["A Gentle Introduction to Pooling Layers for Convolutional Neural Networks"](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/) for more information about how max pooling works.
* ["A Gentle Introduction to Dropout for Regularizing Deep Neural Networks"](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) is an excellent introduction to Dropout.
* ["Simple MNIST convnet"](https://keras.io/examples/vision/mnist_convnet/) is a Keras example showing this particular code fragment.</p></details>

-----------------------

## Date - 2022-09-15


## Title - The gradient of a function


### **Question** :

Yeah, I'm not a huge fan of math, either.

But to make a career in machine learning, you need at least to know some of the underpinnings behind specific techniques.

Let's pick neural networks, for example. The whole idea of optimizing the network rests on understanding how to compute the gradient of a function.

**Which of the following is true about the gradient of a continuous and differentiable function?**


### **Choices** :

- The gradient of a continuous and differentiable function is zero at a minimum value.
- The gradient of a continuous and differentiable function is zero at a saddle point.
- The gradient of a continuous and differentiable function is non-zero at a maximum value.
- The gradient of a continuous and differentiable function decreases as it gets closer to a minimum value.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To keep it simple, the gradient of a function gives us the slope at any point on its curve. We can compute it by dividing the "rise" of the function by its "run" over a specific small interval.

Imagine the [Sine function](https://www.mathworks.com/help/examples/graphics/win64/Create2DLinePlotsExample_01.png). Pick a point on the left side, and think of the tangent to the curve as you move the point closer to the maximum value of the function. The gradient starts positive and decreases as we get closer to the maximum.

Repeat this exercise, starting from the maximum value and moving towards the minimum. The gradient is negative after the maximum point and increases until it reaches the minimum value. Visualizing this in your head is not easy, so [here is an excellent interactive chart](https://undergroundmathematics.org/calculus-of-powers/r7433/solution) where you can see how this works using a similar function.


What happens with the gradient when we get to a minimum, maximum, or saddle point? The gradient will equal zero if the curve has a horizontal slope at a particular point. That means that both the first and second choices correctly describe the properties of the gradient function. However, the third choice is invalid: we already know that the gradient function is zero when we hit a maximum point.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check ["Is the gradient function increasing or decreasing on this curve?"](https://undergroundmathematics.org/calculus-of-powers/r7433/solution) for a great visualization of the gradient of a function.
- The Wikipedia pages of ["Gradient"](https://en.wikipedia.org/wiki/Gradient) and ["Saddle point"](https://en.wikipedia.org/wiki/Saddle_point) contain all the information you need to answer this question.</p></details>

-----------------------

## Date - 2022-09-16


## Title - True Positives


### **Question** :

Here is the picture of a confusion matrix we generated after evaluating a model in a dataset with 100 samples:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186962046-7c076163-5b58-442f-9641-c09f5cf71003.jpg)

Assume that class B represents the outcomes of the model we are interested in finding.

**What's the total of True Positives on this evaluation round?**


### **Choices** :

- True Positives are class A samples the model predicted as class B, so the answer is 7.
- True Positives are class B samples the model predicted as class A, so the answer is 13.
- True Positives are class A samples the model predicted as class A, so the answer is 52.
- True Positives are class B samples the model predicted as class B, so the answer is 28.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We are interested in samples from class  B, which means we will treat class B as our "Positive" samples and class A as our "Negative" samples.

If we replace the classes in the confusion matrix with Positive and Negative instead of "B" and "A," it's much easier to reason about the model's number of True Positives:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/187207950-77e4eccd-0b63-43bc-8f07-be19f690d725.jpg)

True Positives are those samples that we expect to be Positive (class B), and the model predicted as Positive (class B.) Therefore, the correct answer to the question is 28. You can see every combination in the image above.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.
* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2022-09-17


## Title - Output activation


### **Question** :

If you want to learn something new, the best strategy is to practice it until things start making sense.

Alina wanted to do that. She wasn't confident in understanding classification problems and neural networks, so she tried a few ideas. She implemented a neural network from scratch, including different loss functions and the backpropagation process.

It was time to look into activation functions for the output layer of the network, and here is where Alina had the most questions.

**Which of the following activation functions could work in the output layer of a neural network that solves a classification problem?**


### **Choices** :

- Rectifier linear activation function (ReLU)
- Tanh
- f(x) = 1 if x > 0 else 0
- f(x) = 0 if x > 0 else 1


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Whenever we are working on a classification task, we need the network to output a finite range of values. Alina can use any of the choices on this question except [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)).

ReLU returns its same input if positive or zero otherwise. Assuming the input to an output layer using ReLU is a positive value, the result will be the same, which won't help Alina classify the sample.

The other three choices will help Alina classify images in one of two classes. [Tanh](https://www.sciencedirect.com/topics/mathematics/hyperbolic-tangent-function) will return a result between -1 and 1, so she can decide whether the image belongs to one class if the output is negative or the other if positive. The other two functions return a discrete value depending on their input.

Notice that, although not strictly required, we usually prefer activation functions to be differentiable. While tanh is differentiable, the other three options aren't. However, just like ReLU, this question's third and fourth functions are differentiable except at `x = 0`, which is enough for them to work without too much trouble.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["A Gentle Introduction to the Rectified Linear Unit (ReLU)"](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks) is a great introduction to ReLU.
* Check out ["Neural Network Binary Classification With Tanh Output Activation"](https://jamesmccaffrey.wordpress.com/2020/11/02/neural-network-binary-classification-with-tanh-output-activation/) for the source code of a binary classifier using a tanh activation function.</p></details>

-----------------------

## Date - 2022-09-18


## Title - Policing crime


### **Question** :

Brielle wants to build a machine learning model that will use traffic violations to predict how to distribute the city's police force.

She wants the model to predict the areas where new violations are likely to occur so the department can reinforce the security around those streets.

**Which of the following is a potential problem that Brielle should keep in mind?**


### **Choices** :

- There won't be any reliable way to evaluate this model.
- The model may suffer from survivorship bias.
- The model may suffer from decline bias.
- The model may create a positive feedback loop.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Evaluating this model doesn't need to be complicated. Assuming that Brielle uses a Supervised Learning approach, she will have several options to assess the quality of the model predictions. Therefore, the first choice is incorrect.

[Survivorship bias](https://en.wikipedia.org/wiki/Survivorship_bias) is when we concentrate on samples that made it past a selection process and ignore those that did not. Nothing in the problem statement indicates that Brielle's model will suffer from this problem.

[Decline bias](https://en.wikipedia.org/wiki/Declinism) refers to the tendency to compare the past to the present, leading to the assumption that things are worse or becoming worse simply because change is occurring. The third choice is not a correct answer either.

Finally, this model may create a [positive feedback loop](https://en.wikipedia.org/wiki/Positive_feedback). The more you patrol a neighborhood, the more traffic violations you'll find. Communities with no police force will never report any violations, while heavily patrolled communities will have the lion's share of transgressions.

The model will use that data and make the problem worse: it will predict that new violations will happen in already problematic areas, sending more police to those communities at the expense of areas with lower reports. A few rounds of this, and you'll have most reports from a few places while violations are rampant everywhere.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check the description of a ["Positive Feedback Loop"](https://en.wikipedia.org/wiki/Positive_feedback) in Wikipedia.
* ["How Positive Feedback Loops Are Hurting AI Applications"](https://levelup.gitconnected.com/how-positive-feedback-loops-are-hurting-ai-applications-6eae0304521c) is an excellent article explaining the dangers of positive feedback loops in machine learning.</p></details>

-----------------------

## Date - 2022-09-19


## Title - Learnable parameters


### **Question** :

Arianna is trying to learn how Convolutional Neural Networks work, so she decided to copy an online Keras example to start from somewhere.

Here is the core of the code she put together:

```
model = keras.Sequential([
    keras.Input(shape=(28, 28, 1)),
    layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Flatten(),
    layers.Dropout(0.5),
    layers.Dense(10, activation="softmax"),
])
```

**Based on the above code fragment, what are the correct statements regarding each layer's parameters (weights and biases)?**


### **Choices** :

- The first convolutional layer has a total of 21,632 parameters.
- The first max pooling layer has a total of 5,408 parameters.
- The second convolutional layer has a total of 18,496 parameters.
- The fully-connected layer has a total of 16,010 parameters.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We can compute the number of parameters of a convolutional layer using the following formula:

```
parameters = k * (f * h * w + 1)
```

Where `k` corresponds to the number of output filters from this layer, `f` corresponds to the number of filters coming from the previous layer, `h` corresponds to the kernel height and `w` to the kernel width. The value `1` corresponds to the bias parameter related to each filter. Here is the complete calculation for the first convolutional layer:

```
parameters = k * (f * h * w + 1)
parameters = 32 * (1 * 3 * 3 + 1) = 320
```

Max pooling layers don't have any parameters because they don't learn anything. The input to the first max pooling layer is `26x26x32`, but the layer doesn't have any weights or biases associated with it.

The second convolutional layer does have 18,496 parameters. Let's check:

```
parameters = k * (f * h * w + 1)
parameters = 64 * (32 * 3 * 3 + 1) = 18,496
```

Finally, the fully-connected layer has 16,010 parameters. To compute this, we need to calculate the size of each layer to understand the input to the fully-connected layer:
* Input: `28x28x1 = 784`
* Conv2D: `26x26x32 = 21,632`
* MaxPool2D: `13x13x32 = 5,408`
* Conv2D: `11x11x64 = 7,744`
* MaxPool2D: `5x5x64 = 1,600`

We can use the same formula to compute the number of parameters of the output layer:

```
parameters = 10 * (1600 + 1) = 16010
````</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Understanding and Calculating the number of Parameters in Convolution Neural Networks (CNNs)"](https://towardsdatascience.com/understanding-and-calculating-the-number-of-parameters-in-convolution-neural-networks-cnns-fc88790d530d) for instructions on how to compute the number of learnable parameters.
* ["Simple MNIST convnet"](https://keras.io/examples/vision/mnist_convnet/) is a Keras example showing this particular code fragment.</p></details>

-----------------------

## Date - 2022-09-20


## Title - Augmenting the dataset


### **Question** :

Esther had an excellent model already, but she had the budget to experiment a bit more and improve its results.

She was building a deep network to classify pictures. From the beginning, her Achilles' heel has been the size of her dataset. One of her teammates recommended she use a few data augmentation techniques.

Esther was all-in. Although she wasn't sure about the advantages of data augmentation, she was willing to do some research and start using it.

**Which of the following statements about data augmentation are true?**


### **Choices** :

- Esther can use data augmentation to expand her training dataset and assist her model in extracting and learning features regardless of their position, size, rotation, etc.
- Esther can use data augmentation to expand the test dataset, have the model predict the original image plus each copy, and return an ensemble of those predictions.
- Esther will benefit from the ability of data augmentation to act as a regularizer and help reduce overfitting.
- Esther has to be careful because data augmentation will reduce the ability of her model to generalize to unseen images.


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>One significant advantage of data augmentation is its ability to make a model resilient to variations in the data. For example, assuming we are working with images, we can use data augmentation to generate synthetic copies of each picture and help the model learn features regardless of where and how they appear. 

A few popular augmentation techniques when working with images are small rotations, horizontal and vertical flipping, turning the picture to grayscale, or cropping the image at different scales. The [following example](https://www.v7labs.com/blog/data-augmentation-guide) shows four versions of an image generated by changing the original picture's brightness, contrast, saturation, and hue:

![Data augmentation](https://user-images.githubusercontent.com/1126730/179006718-fa88e435-0347-4741-a6e7-6b82266316b3.png)

Data augmentation is also helpful during testing time: Test-Time Augmentation is a technique where we augment samples before running them through a model, then average the prediction results. Test-Time Augmentation often results in better predictive performance.

Instead of predicting an individual sample from the test set, we can augment it and run each copy through the model. Esther is working on a classification problem, so her model will output a softmax vector for each sample. She can then average all these vectors and use the result to choose the correct class representing the original sample.

Using data augmentation, Esther can reduce overfitting and help her model perform better on unseen data. Data augmentation has a regularization effect. Increasing the training data through data augmentation decreases the model's variance and, in turn, increases the model's generalization ability. Therefore, the third choice is correct, but the fourth one is not.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["The Essential Guide to Data Augmentation in Deep Learning"](https://www.v7labs.com/blog/data-augmentation-guide) is an excellent article discussing data augmentation in detail.
* Check ["Test-Time augmentation"](https://articles.bnomial.com/test-time-augmentation) for an introduction that will help you make better predictions with your machine learning model.</p></details>

-----------------------

## Date - 2022-09-21


## Title - Vanishing gradients


### **Question** :

Kaylee's model is not doing great.

She is building a deep neural network but can't even get it to train properly.

Kaylee mainly uses sigmoid as the activation function on the network's layers, so a colleague mentioned the vanishing gradient problem as a potential cause.

**Assuming Kaylee's colleague is correct, which of the following statements should help the network learn?**


### **Choices** :

- Kaylee should use tanh instead of sigmoid because it doesn't suffer from vanishing gradients.
- Kaylee should use ReLU instead of Sigmoid because it doesn't suffer from vanishing gradients.
- Kaylee should modify her model and introduce Batch Normalization to mitigate the problem.
- Kaylee should initialize the network weights with large values to avoid vanishing gradients.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>If the gradients of the loss function approach zero, the model will stop learning because the network will stop updating the weights. This phenomenon is known as the [Vanishing Gradient Problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and it's common when using the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) and [tanh](https://www.sciencedirect.com/topics/mathematics/hyperbolic-tangent-function) activation functions in deep neural networks. Kaylee won't fix the problem by replacing sigmoid with tanh.

In contrast, [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) is a way to solve the vanishing gradient problem. ReLU is much less likely to saturate, and its derivative is 1 for values larger than zero, so Kayle should look at it to replace sigmoid.

[Batch normalization](https://en.wikipedia.org/wiki/Batch_normalization) is another way to mitigate the vanishing gradient problem. Suppose we have a layer that uses a sigmoid activation function. We can normalize the input to that layer to ensure that the values don't reach the edges and stay around the area where derivatives aren't too small. By modifying the input to this layer with batch normalization, Kaylee will mitigate the vanishing gradient problem.

Finally, while Kaylee may suffer from the vanishing gradient problem if she uses small values to initialize the network's weights, she will have the opposite problem (exploding gradients) if she initializes the network with large values. ["Initializing neural networks"](https://www.deeplearning.ai/ai-notes/initialization/) is an excellent introduction to the importance of correct weight initialization.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["How to Fix the Vanishing Gradients Problem Using the ReLU"](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/) for recommendations on how to fix the vanishing gradient problem.
* ["Initializing neural networks"](https://www.deeplearning.ai/ai-notes/initialization/) is an excellent summary of the importance of weight initialization.</p></details>

-----------------------

## Date - 2022-09-22


## Title - Bagging or Boosting?


### **Question** :

Katherine wants to use an ensemble model to process her dataset. 

There's only one question for her to answer: Should she use bagging or boosting?

Both techniques have different advantages and disadvantages, and Katherina wants to ensure she evaluates them correctly before committing to one solution.

**Which of the following statements are true about bagging and boosting?**


### **Choices** :

- Bagging trains individual models sequentially, using the results from the previous model to inform the selection of training samples.
- Boosting trains individual models sequentially, using the results from the previous model to inform the selection of training samples.
- Bagging trains a group of models, each using a subset of data selected randomly with replacement from the original dataset.
- Each model receives equal weight in bagging to compute the final prediction while boosting uses some way of weighing each model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Ensembling is where we combine a group of models to produce a new model that yields better results than any initial individual models. Bagging and boosting are two popular ensemble techniques.

Bagging trains a group of models in parallel and independently from each other. Each model uses a subset of the data randomly selected with replacement from the original dataset. In contrast, Boosting trains a group of learners sequentially, using the results from each model to inform which samples to use to train the next model.

This summary helps us conclude that the first choice is incorrect, but the second and third choices are correct.

Finally, when computing the final prediction, bagging averages out the results of each model. Boosting, however, weights each model depending on its performance. Therefore, the fourth choice is also correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check [Bagging vs. Boosting in Machine Learning: Difference Between Bagging and Boosting](https://www.upgrad.com/blog/bagging-vs-boosting/) for a detailed comparison between both techniques.
* [What is the difference between Bagging and Boosting?](https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/) is another great summary of both techniques and their advantages and disadvantages.</p></details>

-----------------------

## Date - 2022-09-23


## Title - Pooling layers


### **Question** :

Sienna realized she needed more than convolutional layers to process her image dataset.

After stacking a few convolutional layers, her model started to make progress. Unfortunately, only very similar images returned positive results. Sienna discovered that her model lacked translation invariance: it was paying too much attention to the precise location of every feature.

Fortunately, Sienna found out that she could use pooling layers.

**Which of the following statements about pooling layers are correct?**


### **Choices** :

- During the training process, the network will learn the best configuration for the pooling layer.
- A pooling layer with a stride of 2 will cut the number of feature maps from the previous convolutional layer in half.
- Pooling layers create the same number of pooled feature maps.
- Average pooling and max pooling are two of the most common pooling operations.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Pooling layers don't have any learnable parameters. When designing the model, Sienna must specify the pooling operation and configuration she wants to use. 

Pooling layers work on each feature map independently and, depending on the pool size and stride, downsample these feature maps. The result is always a new set of pooled feature maps. Therefore, the second choice is incorrect, but the third is correct.

Finally, Max Pooling and Average Pooling are the two most common pooling operations. Average Pooling computes the average value of each patch, while Max Pooling calculates the maximum value.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Max Pooling in Convolutional Neural Network and Its Features"](https://analyticsindiamag.com/max-pooling-in-convolutional-neural-network-and-its-features/) is a great introduction to Max Pooling.
* Check ["A Gentle Introduction to Pooling Layers for Convolutional Neural Networks"](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/) for more information about how pooling layers work.</p></details>

-----------------------

## Date - 2022-09-24


## Title - The output of the network


### **Question** :

Juniper learned that designing a neural network architecture for a supervised classification problem wasn't hard.

Although most of it needed experimentation, one thing Juniper could count on was the design of the output layer of the network.

**Which of the following is a correct statement about the neurons in the output layer of a classification network?**


### **Choices** :

- The number of neurons in the output layer should always match the number of classes.
- The number of neurons in the output layer doesn't necessarily need to match the number of classes.
- The number of neurons in the output layer should always be greater than one.
- The number of neurons in the output layer should always be a multiple of 2.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When working on a multi-class classification problem, setting the output layer to the same number of classes we are interested in predicting is common. But what happens when we are working on a binary classification problem?

A network to solve a binary classification problem doesn't need an output with two neurons. Instead, we can use a single neuron to determine the class by deciding on a cutoff threshold. For example, the result could be positive if the output exceeds 0.5 and negative otherwise.

That means we can solve a problem requiring two classes with a single neuron.

We can stretch the same idea to multi-class classification problems: We could interpret the output layer as a binary result, allowing us to represent multiple classes with fewer neurons. For example, we would need only two neurons to classify instances into four different categories (`00`, `01`, `10`, `11`.) This approach, although not popular, it's possible.

Therefore, the second choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["But what is a neural network?"](https://www.youtube.com/watch?v=aircAruvnKk) is Grant Sanderson's introduction to neural networks on YouTube. Highly recommended!
* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html) is a free online book written by [Michael Nielsen](https://twitter.com/michael_nielsen).</p></details>

-----------------------

## Date - 2022-09-25


## Title - Studying decision trees


### **Question** :

Kehlani is one of many software developers that decided to start learning machine learning.

Following the recommendation of her peers, the first algorithm she looked into was decision trees. She found a lot of resemblance with some of the techniques she already knew.

After a few weeks, Kehlani wants to summarize what she learned in an email to her team, but first, she wants you to review it.

**These are the advantages that Kehlani listed. Which of them would you say are actual advantages of decision trees?**


### **Choices** :

- Decision trees are simple to understand and interpret, and we can visualize them.
- Unlike other algorithms, many implementations of decision trees work with missing values and categorical data.
- Decision trees always generalize well and are resistant to overfitting.
- Decision trees require little data preparation compared to other algorithms. For example, they don't need the scaling of data.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Decision Trees](https://en.wikipedia.org/wiki/Decision_tree_learning) are an excellent starting point for developers that want to learn machine learning.

One of the reasons decision trees are popular is because they are easier to understand and interpret than other algorithms. You can explain the output of a tree by boolean logic, which makes them transparent compared to algorithms we can't explain. Kehlani did an excellent job by including this advantage in her email.

Many different implementations of decision trees are very flexible with the data they can process. For example, some implementations can handle missing values and work directly with categorical data. Most machine learning algorithms don't have this luxury and require a much more extensive pre-processing step before training a model. 

Something similar happens with having to scale the data. Decision trees can handle features with different scales. For example, it can take a column representing a person's age with values between 0 and 100 and another with a salary ranging from 20,000 to 500,000. Not every algorithm has this flexibility. Neural networks, for example, struggle when features don't have the same approximate scale.

Decision trees, unfortunately, are prone to overfitting if we don't take careful care of their depth. In other words, unless we ensure our tree doesn't go too deep, it will tend to fit noisy samples and output the wrong prediction. The only example where Kehlani made a mistake is the third option. 

Remember that while a single decision tree is prone to overfitting, using an ensemble of trees is more resistant. Here is a quote from "[To Boost or not to Boost: On the Limits of Boosted Neural Networks](https://arxiv.org/pdf/2107.13600.pdf)": "[these experiments] confirm that training single large decision trees is prone to overfitting while boosted ensembles of decision trees are resistant to overfitting."</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The Scikit-Learn's ["Decision Trees"](https://scikit-learn.org/stable/modules/tree.html) page contains an extensive list of advantages and disadvantages of decision trees.
* Check ["To Boost or not to Boost: On the Limits of Boosted Neural Networks"](https://arxiv.org/pdf/2107.13600.pdf), the paper cited above comparing the overfitting tendency of a single Decision Tree versus an ensemble of trees.
* ["Decision tree pruning"](https://en.wikipedia.org/wiki/Decision_tree_pruning) covers the process of pruning a tree to reduce overfitting.</p></details>

-----------------------

## Date - 2022-09-26


## Title - Defective chips


### **Question** :

A company is using autoencoders to detect anomalies with the chips they produce.

They trained their autoencoder on millions of pictures of working chips to generate a compressed representation of the common characteristics of a chip in good condition.

They run every new picture in production through the autoencoder to determine whether there's a problem with that particular chip.

**Which of the following is the company's process to determine whether the chip is in good working condition?**


### **Choices** :

- The autoencoder's output is a softmax vector that classifies the picture into two classes. If the largest value in the vector corresponds to the anomaly class, the company knows this is a defective chip.
- Autoencoders work like a binary classification model, where any output value greater than a predefined threshold indicates the input is an anomaly.
- The company computes the error between the model's original input and output. The picture represents a defective chip if the error exceeds a predefined threshold.
- The autoencoder's output is a value indicating how different the input image is from a working chip, so the largest this value is, the more likely it's for the input to be a defective chip.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Autoencoders](https://essays.bnomial.com/autoencoders) are learning algorithms that can help with anomaly detection problems. 

An autoencoder is a neural network that we can split into three sections: an encoder, a bottleneck, and a decoder. The encoder compresses the original input into an intermediate representation, and the decoder reverses the process to reconstruct the original data. The bottleneck sits between the encoder and decoder and is the section that stores the compressed representation of the data. 

In this particular problem, we can train an autoencoder by showing it images of working chips and expecting the model to reproduce the same input image. In other words, the autoencoder's input and expected output are the same. If we compute the similarity between the input and output images, we can determine whether the original picture belongs to a working or defective chip.

A defective chip will have specific visual characteristics that will be impossible for the autoencoder to reproduce. Remember that the autoencoder learned on a dataset of working samples, so it won't have any notion of features that aren't common among chips in good status. 

We should expect a reproduction error larger than normal whenever we input a defective chip into the autoencoder. Therefore, the third option is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Autoencoders"](https://articles.bnomial.com/autoencoders) for an introduction to a learning technique to represent data efficiently using neural networks.</p></details>

-----------------------

## Date - 2022-09-27


## Title - Data augmentation on YouTube


### **Question** :

Amara is writing a script for a YouTube video about data augmentation.

She wants to cover some of the most critical aspects of using the technique on a dataset of pictures.

Here is Amara's list with the key takeaways she wants to leave for her audience.

**Which of the following statements would you let Amara share with her audience?**


### **Choices** :

- Using data augmentation, we can artificially increase the amount of data by generating new samples from existing data.
- Generative Adversarial Networks (GANs) and Style Transfer are advanced techniques that can help with data augmentation.
- Data augmentation has a regularization effect when used to increase the amount of data before training a model.
- Data augmentation helps remove the inherent bias present in the original data.


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The first choice is the definition of data augmentation: we can augment the size of the dataset by generating new samples from our existing data.

Generative Adversarial Networks (GANs) are a popular way to generate synthetic images. [We can also use Style Transfer](https://arxiv.org/abs/1909.01056) to create new data based on existing samples.

The third choice is also correct: Data augmentation has a regularization effect. Increasing the training data through data augmentation decreases the model's variance and, in turn, increases the model's generalization ability.

Finally, when we use data augmentation, we will propagate any of the biases already present in the original data. Remember that data augmentation uses existing data as the foundation for any new data, so any problems with the original dataset will persist on the augmented one. Therefore, the fourth choice is incorrect.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["STaDA: Style Transfer as Data Augmentation"](https://arxiv.org/abs/1909.01056) is a paper illustrating how to use Style Transfer for data augmentation.
* ["The Essential Guide to Data Augmentation in Deep Learning"](https://www.v7labs.com/blog/data-augmentation-guide) is an excellent article discussing data augmentation in detail.
* Check ["Test-Time augmentation"](https://articles.bnomial.com/test-time-augmentation) for an introduction that will help you make better predictions with your machine learning model.</p></details>

-----------------------

## Date - 2022-09-28


## Title - Six months of data


### **Question** :

Valerie's company gave her access to their entire dataset from day one.

She used six consecutive months of data: the first five months to train the model and the sixth to test it.

After trying multiple times, Valerie's model performance on the training data had a large gap with the performance on the test data. The model works well with the training dataset but struggles to keep up with the test data.

**Which of the following could be valid reasons for this problem?**


### **Choices** :

- Valerie's optimizer is incorrect for this particular problem.
- Valerie is not using the correct loss function for this particular problem.
- Valerie's model doesn't have enough complexity to capture the relevant signal from the data.
- The training data does not come from the same distribution as the test data.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Valerie is training her model on five consecutive months' worth of data and testing it with another month worth of data. If something changed during that time, Valerie might have a test set that fundamentally differs from her training data.

This problem is common, and a way to approach it is to ensure the training and test data come from the same distribution. ["Adversarial validation"](https://articles.bnomial.com/adversarial-validation) is a clever technique you can use to accomplish this.

None of the other three options explain a model that works with the training data but struggles with the test dataset.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Adversarial validation"](https://articles.bnomial.com/adversarial-validation) for a quick introduction to this technique.</p></details>

-----------------------

## Date - 2022-09-29


## Title - Nodes and connections


### **Question** :

June has just started learning about multilayer perceptrons: inputs, outputs, layers, and nodes.

It's a lot for a single day of study, but June doesn't want to go back home without understanding some core ideas.

She devised a simple rule: assuming she had two networks, one with more nodes than the other, this network must also have more connections. 

**What do you think about June's rule?**


### **Choices** :

- June is correct: the network with more nodes will always have more connections than the one with fewer nodes.
- June is incorrect: the network with fewer nodes will always have more connections than the one with more nodes.
- June is incorrect: every multilayer perceptron has the same number of connections.
- June is incorrect: either network may have more connections than the other, regardless of the number of nodes.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>June's rule is incorrect.

Imagine a network with three layers:
* The first layer with 64 nodes
* The second layer with 8
* The third layer with 64 nodes

This network has a total of 136 nodes and 1,024 connections.

The second network will have only two layers:
* The first layer with 64 nodes
* The second layer with 64 nodes also

This network has a total of 128 nodes and 4,096 connections.

Despite the first network having more nodes than the second one (136 versus 128,) the second network has more connections (4,096 versus 1,024.)

Therefore, the correct answer to this question is the fourth choice.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["But what is a neural network?"](https://www.youtube.com/watch?v=aircAruvnKk) is Grant Sanderson's introduction to neural networks on YouTube. Highly recommended!
* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html) is a free online book written by [Michael Nielsen](https://twitter.com/michael_nielsen).</p></details>

-----------------------

## Date - 2022-09-30


## Title - Perfect balance


### **Question** :

Here is the Fβ score formula:

![FBeta-Score](https://user-images.githubusercontent.com/1126730/193036770-0a37f6f7-9a5d-4be3-9b89-7018931140eb.jpg)

The Fβ score lets us combine precision and recall into a single metric.

We want to use this formula to balance precision and recall perfectly. 

**What's the correct value for the β parameter to achieve this?**


### **Choices** :

- β = 0
- β = 0.5
- β = 1
- β = 2


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The Fβ score lets us combine precision and recall into a single metric. When using β = 1, we place equal weight on precision and recall. For values of β > 1, recall is weighted higher than precision; for values of β < 1, precision is weighted higher than recall.

Therefore, the correct answer to this question is β = 1.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What is the F-Score?"](https://deepai.org/machine-learning-glossary-and-terms/f-score) is a short introduction to this metric.
* ["Micro, Macro & Weighted Averages of F1 Score, Clearly Explained"](https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f) is a great article covering how to compute a global F1-Score metric in multi-class classification problem.
* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2022-10-01


## Title - High training


### **Question** :

There's not a lot of context for you other than the following chart showing the training loss of a machine learning model:

![Training Loss Chart](https://user-images.githubusercontent.com/1126730/188470675-c76e29e3-e68c-4eaa-8b05-879bed678b2f.jpg)

As you can see, after finishing training, the loss stays too high.

**What's a reasonable conclusion about this machine learning model?**


### **Choices** :

- The model is overfitting.
- The model is underfitting.
- The model is neither overfitting nor underfitting.
- The model is either overfitting or underfitting, but we can't say for sure.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A good model should capture valuable patterns in the data and discard any noise that doesn't help with predictions. An overfitting model will fit that noise. An underfitting model will not capture the relevant patterns in the dataset. 

An overfitting model should not have any problems with the training data, so we should expect a low training loss. An underfitting model should struggle with the training data, so its training loss will be high.

This model shows a high training loss, which we expect for an underfitting model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.</p></details>

-----------------------

## Date - 2022-10-02


## Title - Useless explanation


### **Question** :

"Remove every ordinal feature. Keep the nominal ones."

Those were the instructions that Brad sent Angelina. "I hate him so much," she couldn't stop thinking.

Angelina's dataset had a lot of columns, and she couldn't fit it all into memory. Removing some of the columns was an excellent place to start, and since Brad had solved the problem already, she asked him.

"But of course" —she kept thinking— "he couldn't have been more useless."

Angelina doesn't know the difference between ordinal and nominal columns. 

**Which of the following is the way forward for Angelina?**


### **Choices** :

- Angelina should remove every categorical feature whose values have a meaningful order.
- Angelina should remove every categorical feature whose values don't have a meaningful order.
- Angelina should remove every numerical feature but leave categorical features.
- Angelina should remove every categorical feature but leave numerical features.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Both ordinal and nominal features refer to categorical columns. 

Ordinal columns have a meaningful order. For example, you could have a feature representing a person's economic status with three possible values: "low," "medium," and "high." Notice how there's a clear order among these values: "low" comes first, then "medium," and finally "high." 

Compare this with a nominal variable that doesn't have a meaningful order between values. For example, a feature representing a "color" will not have any discernible order.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Nominal, ordinal, or numerical variables?"](https://s4be.cochrane.org/blog/2015/07/24/nominal-ordinal-numerical-variables/) to understand the difference between different types of features.</p></details>

-----------------------

## Date - 2022-10-03


## Title - Date of sale


### **Question** :

When Kylie started working for Nike, she didn't believe her first project was at the core of their sales process.

She found a team working on a machine learning model for about a year with mediocre results. After a couple of weeks, Kylie proposed to do some feature engineering around a feature representing the sale date to help the model improve its predictions.

**Which of the following are examples of feature engineering techniques that Kylie could do to improve the model?**


### **Choices** :

- Replacing the feature representing the date of the sale with three separate columns for the year, month, and day.
- Replacing the feature representing the date of the sale with a single value containing the number of seconds since the year started.
- Replacing the feature representing the date of the sale with a single value that contains the number of the month.
- Keeping the date feature untouched and adding a new column representing the number of samples sold during the same month.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We don't know which of these options will be the most useful, but every one of them is an example of feature engineering that could help the model make better predictions.

The first three options are examples of how we can derive new features from a column in our dataset. In this case, Kylie can turn a date field into three components or simplify it by replacing it with a single value that keeps the necessary information for the model. Notice that, in these three cases, Kylie is not introducing anything new. Instead, she is transforming the data so the model can use it.

The fourth option is an example of frequency encoding, where Kylie counts how many products have been sold every month and creates a meta-feature with this information.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.
* [_Feature Engineering for Machine Learning_](https://amzn.to/3SsnLAc) is an excellent book covering feature engineering.</p></details>

-----------------------

## Date - 2022-10-04


## Title - Two experiments. Round one.


### **Question** :

Lena ran two experiments training a neural network on a sample dataset. Her goal is to understand how different batch sizes affect the training process.

One of the experiments used a very small batch size, and the other used a batch size equal to all existing training data.

After training and evaluating her models, Lena plotted the training and testing losses over 100 epochs of one of the experiments, and this is what she got:

![Learning Curves](https://user-images.githubusercontent.com/1126730/191095007-c31eddb5-e4b6-481c-b529-9064cf7dda20.jpg)

**Which of the following options is the most likely to be true?**


### **Choices** :

- The model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took less time to train.
- The model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took longer to train.
- The model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took less time to train.
- The model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took longer to train.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The amount of noise in the plot is crucial to answering this question.

The lower the batch size, the more noise we will get in the model's loss. When we only use a few samples, any instances that vary dramatically will cause the loss to swing wildly. When we use a larger batch, no individual sample would have the power to sway the loss too much, so we should expect less noise.

This plot shows a very smooth loss with almost no noise, which is likely the experiment that uses the entire training set during every epoch.

The smaller the batch size, the more times we will need to compute the loss and update the model's weights during backpropagation. For example, if Lena's training set has 800 samples and uses one as her batch size, the algorithm will update the model's weights 800 times. However, if she uses a batch size equal to the size of her training set, the algorithm will update the model's weights 1 time.

Updating the model during every iteration is computationally expensive, so using a larger batch size will usually take less time. Therefore, the third option is probably the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["The wrong batch size is all it takes" ](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) explains how different batch sizes influence the training process of neural networks using gradient descent.
* Check ["An overview of gradient descent optimization algorithms"](https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants) for a deep dive into gradient descent and every one of its variants.</p></details>

-----------------------

## Date - 2022-10-05


## Title - Tossing a fair coin


### **Question** :

I have a fair coin and want to play a quick game with you.

I'll give the coin to you, and I want you to flip it five times.

Let's image two different scenarios: one where you get the coin to land five consecutive times on heads, and the second where the coin lands four straight times on heads, then on tails.

**Which of those two scenarios is most likely to happen?**


### **Choices** :

- Getting five straight heads is more likely than getting four heads followed by tails.
- Getting four heads followed by tails is more likely than getting five consecutive heads.
- Getting four heads followed by tails is equally likely than getting five successive heads.
- If the coin is fair, this is a random event, and we can't say which one is more likely than the other.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Both scenarios are equally likely to happen with a probability of `1/32`.

This scenario is an example of the Gambler's Fallacy, the irrational belief that prior outcomes in a series of events affect the probability of a future outcome, even though the events are independent and identically distributed. 

Here is a quote from the [Gambler's Fallacy](https://en.wikipedia.org/wiki/Gambler%27s_fallacy) Wikipedia page: 

> If after tossing four heads in a row, the next coin toss also came up heads, it would complete a run of five successive heads. Since the probability of a run of five successive heads is `1/32`, a person might believe that the next flip would be more likely to come up tails rather than heads again. This is incorrect and is an example of the gambler's fallacy. The event "5 heads in a row" and the event "first 4 heads, then a tails" are equally likely, each having probability `1/32`.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Gambler's Fallacy"](https://en.wikipedia.org/wiki/Gambler%27s_fallacy) on Wikipedia for an introduction to the situation described in this question.</p></details>

-----------------------

## Date - 2022-10-06


## Title - A clever loss function


### **Question** :

Adalynn is using a clever approach with her deep learning model.

During training, Adalynn runs three samples simultaneously through her model:
* An anchor image
* A matching picture representing the same entity as the anchor
* A non-matching picture representing a different entity as the anchor

The loss function she uses takes these three inputs and minimizes the distance between the anchor and the matching sample while maximizing the distance with the non-matching picture.

But Adalynn didn't invent this function.

**Which of the following is the name of the loss function that Adalynn is using?**


### **Choices** :

- Adalynn is using a Contrastive loss function
- Adalynn is using a Binary Cross-entropy loss function
- Adalynn is using a Categorical Cross-entropy loss function
- Adalynn is using a Triplet loss function


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Adalynn is using [Triplet loss](https://en.wikipedia.org/wiki/Triplet_loss), helpful in [similarity learning problems](https://en.wikipedia.org/wiki/Similarity_learning) where the goal is to learn a function that measures how similar two objects are.

The Triplet loss takes the three inputs: the anchor, a positive sample, and a negative sample, and minimizes the anchor—positive distance while maximizing the anchor—negative distance.

The Triplet loss function compares a baseline (the anchor) to positive and negative inputs. It then reduces the distance between the baseline and the positive sample while increasing the distance between the anchor and the negative input.

A popular application of Triplet loss is to teach a model to detect faces. Instead of framing this use case as a classification problem, we can look at it as a similarity learning problem. The goal would be for a network to compare two pictures and output whether they are similar enough. 

Notice that we also use [Contrastive loss](https://medium.com/@maksym.bekuzarov/losses-explained-contrastive-loss-f8f57fe32246) in similarity learning problems, but this function receives two inputs and whether they represent the same concept or not.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What is Triplet loss"](https://deepchecks.com/glossary/triplet-loss/) is a short introduction to this loss function. 
* The [Wikipedia page of the Triplet loss](https://en.wikipedia.org/wiki/Triplet_loss) function is also a great reference.
* Check ["Losses explained: Contrastive Loss"](https://medium.com/@maksym.bekuzarov/losses-explained-contrastive-loss-f8f57fe32246) for an explanation of the Contrastive loss.</p></details>

-----------------------

## Date - 2022-10-07


## Title - Features and data


### **Question** :

Balancing the size of a dataset and the number of features to train a model is always a problem you need to consider.

**Which of the following statements correctly summarizes your thoughts about the relationship between features and dataset size?**


### **Choices** :

- When training a model, as you add more features to the dataset, you often need to increase the dataset's size to ensure the model learns reliably.
- When training a model, adding more features to the dataset increases the amount of information you can extract, allowing you to use smaller datasets and still extract good performance from the data.
- When training a learning algorithm, as you decrease the number of features in your dataset, you need to increase the number of training samples to make up the difference.
- When training a learning algorithm, the features in your dataset are entirely independent of the number of training samples.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Removing features reduces the number of dimensions in our data. It concentrates the samples we have in a lower-dimensional space. We can't replace the information provided by a feature with more data.

Imagine plotting a set of numbers. Since you have only one dimension, they will all lie somewhere in a line. Don't add new values; increase the features by adding a second dimension. Your values now become a set of 2D coordinates `(x, y)`; if you graph them, they will all be somewhere in a plane.

If you compare the 1D line with the 2D plane (or even a 3D space, assuming you add a third dimension,) something will become apparent quick: As we increase the dimensionality of the data, it will be harder and harder to fill up the space with the same points.

This increase in sparsity will make it much harder for the learning algorithm to find interesting patterns. 

Based on this, we can conclude that there's a relationship between features and samples, and the more features we add, the more data points we need.

The first choice is the only correct solution to this question. For a more formal definition, look at the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality): The amount of data needed to extract relevant information increases exponentially with the number of features in your dataset.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["What is the Curse of Dimensionality?"](https://deepai.org/machine-learning-glossary-and-terms/curse-of-dimensionality) for an introduction to the Curse of Dimensionality.</p></details>

-----------------------

## Date - 2022-10-08


## Title - False Negatives


### **Question** :

Here is the picture of a confusion matrix we generated after evaluating a model in a dataset with 100 samples:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186962046-7c076163-5b58-442f-9641-c09f5cf71003.jpg)

Assume that class A represents the outcomes of the model we are interested in finding.

**What's the total of False Negatives on this evaluation round?**


### **Choices** :

- False Negatives are class A samples the model predicted as class B, so the answer is 7.
- False Negatives are class B samples the model predicted as class A, so the answer is 13.
- False Negatives are class A samples the model predicted as class A, so the answer is 52.
- False Negatives are class B samples the model predicted as class B, so the answer is 28.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We are interested in samples from class A which means we will treat class A as our "Positive" samples and class B as our "Negative" samples.

If we replace the classes in the confusion matrix with Positive and Negative instead of "A" and "B," it's much easier to reason about the model's number of False Negatives:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186963826-ec29e2b5-c065-4569-9542-712acab129da.jpg)

False Negatives are those samples that we expect to be Positive (class A), but the model predicted as Negative (class B.) Therefore, the correct answer to the question is 7. You can see every combination in the image above.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.
* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2022-10-09


## Title - Scikit-Learn's fan


### **Question** :

So far, Melody is a big fan of Scikit-Learn.

She hasn't used it for too long, but every time she learns something new, there's always an elegant way to do it using Scikit-Learn.

But today is perhaps the exception: Melody finished training a multi-class classification model, and after displaying Scikit-Learn's classification report, she noticed something wasn't right.

She has to analyze the Macro-average, Micro-average, and Weighted F1-Score values for her model, but the classification report doesn't display the Micro-average F1-Score.

**How can Melody move forward?**


### **Choices** :

- The Micro-average F1-Score is the same as the model's accuracy. Melody can run her analysis using the accuracy of her model.
- The Micro-average F1-Score is the same as the model's precision. Melody can run her analysis using the precision of her model.
- The Micro-average F1-Score is the same as the model's recall. Melody can run her analysis using the recall of her model.
- Melody will need to compute the Micro-average F1-Score of her model manually.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When we work on multi-class classification problems, it's simple to compute the F1-Score for each class. Several strategies exist to calculate a global F1-Score representing the entire model's performance: Macro-average F1-Score, Micro-average F1-Score, and Weighted F1-Score.

Scikit-Learn's classification report shows all three of these metrics but uses a different name for the Micro-average F1-Score. That's why Melody is confused.

The Micro-average F1-Score is a metric where we sum all of the contributions from each category to compute an aggregated F1-Score. Micro-average F1-Score calculates the proportion of correctly classified samples out of all samples, which is the model's accuracy definition.

In any multi-class classification problem, the Micro-average F1-Score is the same as the accuracy. That's why Scikit-Learn displays it under the "accuracy" label.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Micro, Macro & Weighted Averages of F1 Score, Clearly Explained"](https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f) is a great article covering how to compute a global F1-Score metric in multi-class classification problem.
* Check ["What is the F-Score?](https://deepai.org/machine-learning-glossary-and-terms/f-score) for a short introduction to the Fβ-Score.</p></details>

-----------------------

## Date - 2022-10-10


## Title - Learning gap


### **Question** :

There's not a lot of context for you other than the following chart showing the training and testing losses of a machine learning model:

![Training and Testing Loss Chart](https://user-images.githubusercontent.com/1126730/188513106-72f3246f-7fbc-429b-a4b5-bb05ca0da706.jpg)

As you can see, after finishing training, both losses decreased nicely, but there's a large gap between the training and the testing curves.

**What's a reasonable conclusion about this machine learning model?**


### **Choices** :

- This model is underfitting.
- This model is well-fit.
- The training dataset is not sufficiently large.
- The training and testing datasets are different enough for the model to struggle to solve testing samples.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A good model should capture valuable patterns in the data and discard any noise that doesn't help with predictions. An overfitting model will fit that noise. An underfitting model will not capture the relevant patterns in the dataset. 

An overfitting model should not have any problems with the training data but stumble with the testing data. Therefore, we should expect a low training loss and a high testing loss. An underfitting model should struggle with the training and testing datasets, so both of its losses should be high. A well-fit model, however, should have low training and testing losses.

The chart shows a model that, while making progress, can't entirely solve the testing dataset. This happens whenever the training data is insufficient to train a model that performs well on unseen data.

For example, the training and the testing data might differ enough that the model can't get many of the testing samples correct, or the training dataset might not be large enough.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.</p></details>

-----------------------

## Date - 2022-10-11


## Title - Extra regularization


### **Question** :

Eloise found that adding Batch Normalization was the best approach to improve the training speed of her deep neural network.

But everything comes at a price. Batch Normalization introduces some regularization to the network, and Eloise knows this will affect the Dropout she is currently using.

**How should Eloise adjust the Dropout her network is using?**


### **Choices** :

- After adding Batch Normalization, Eloise should remove the Dropout she is currently using.
- After adding Batch Normalization, Eloise should slightly decrease the amount of Dropout she is currently using.
- After adding Batch Normalization, Eloise should slightly increase the amount of Dropout she is currently using.
- After adding Batch Normalization, Eloise should significantly increase the amount of Dropout she is using.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Batch Normalization is a popular technique used to train deep neural networks. It normalizes the input to a layer during every training iteration using a mini-batch of data. It smooths and simplifies the optimization function leading to a more stable and faster training.

Batch Normalization acts as a regularizer. The mean of a mini-batch of data is a noisier version of the true mean of the population, and when used to normalize the data, it adds randomness to the optimization process. Eloise will need to adjust the Dropout she is using to accommodate this extra regularization.

Keep in mind that Batch Normalization doesn't introduce a lot of regularization, so it's unlikely that Eloise can remove all of the Dropout she is using. She should, however, slightly decrease the Dropout she is using to account for the extra regularization.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"](https://arxiv.org/pdf/1502.03167.pdf) is the original paper introducing Batch Normalization.
* ["Intro to Optimization in Deep Learning: Busting the Myth About Batch Normalization"](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/) is an excellent blog post challenging some of the ideas from the original paper.
* ["A Gentle Introduction to Batch Normalization for Deep Neural Networks"](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/) is a good introduction to Batch Normalization.</p></details>

-----------------------

## Date - 2022-10-12


## Title - Recurring daughter


### **Question** :

I'm trying to convince my daughter that she will have difficulty using a traditional feedforward network to process comments from her followers on YouTube.

Instead, I recommended recurrent neural networks as a good starting point. Going to transformers is overkill, and I'd rather have her look into some foundational ideas first.

**Which of the following do you think I should include in the list of differences between recurrent and traditional networks?**


### **Choices** :

- Traditional neural networks can process inputs of any length. Recurrent neural networks require a fixed-size sequence as their input.
- Recurrent neural networks capture the sequential information present in the input data. Traditional neural networks don't have this ability.
- Recurrent neural networks share weights across time. Traditional networks use different weights on each input node.
- Recurrent neural networks can consider future inputs to compute their current state. Traditional neural networks can't access future inputs.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNN) are a type of artificial neural network that can process sequential or time series data. Their main difference from traditional networks is their ability to take information from prior inputs to influence the current input and output. This ability allows them to capture any sequential information present in the data. For example, an RNN is ideal for capturing the dependency between words of a sentence.

RNN processes the data sequentially so a model can process sequences of varying sizes. For example, an RNN can process a 5-word and 10-word sentence using the same input structure, unlike a traditional neural network that will need a different input size for each case. 

RNNs share the same weight parameters for every input sample, unlike traditional networks with different weights across each input node. Sharing parameters helps an RNN generalize to sequences of varying lengths and operate similarly on sequences with the same meaning but organized differently. The [accepted answer](https://stats.stackexchange.com/a/318428) to [this question](https://stats.stackexchange.com/questions/221513/why-are-the-weights-of-rnn-lstm-networks-shared-across-time) expands on this topic with an example.

Finally, neither recurrent nor traditional networks can access future inputs to compute their current state.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Recurrent Neural Networks"](https://www.ibm.com/cloud/learn/recurrent-neural-networks) for a description of what they are and how they work.
* ["An Introduction To Recurrent Neural Networks And The Math That Powers Them"](https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/) is a deeper dive into RNNs.</p></details>

-----------------------

## Date - 2022-10-13


## Title - Coffee beans


### **Question** :

Hadley is building a multi-class classification model to separate coffee beans into five categories.

To measure the model's efficacy, Hadley decided to use the F1-Score.

But there's one problem: Hadley knows how to compute the F1-Score for every class separately, but she needs a single F1-Score value to compare different model versions and choose the best one.

**Select from the following list every metric that Hadley can use to summarize the F1-Score for her model?**


### **Choices** :

- Hadley can compute the Macro-average F1-Score.
- Hadley can compute the Micro-average F1-Score.
- Hadley can compute the ROC F1-Score.
- Hadley can compute the Weighted F1-Score.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When building a classification model, the precision and recall scores are two metrics that indicate how effective the model is. There's a trade-off between these metrics, so higher precision models sacrifice recall and vice versa.

It's hard to compare classifiers unless we have a single metric that summarizes the balance between precision and recall. The Fβ score lets us do that. 

![FBeta-Score](https://user-images.githubusercontent.com/1126730/193036770-0a37f6f7-9a5d-4be3-9b89-7018931140eb.jpg)

When using β = 1, we place equal weight on precision and recall. For values of β > 1, recall is weighted higher than precision; for values of β < 1, precision is weighted higher than recall. F1-Score is the most commonly used, which is the Fβ score with β = 1.

Computing the F1-Score for each class in a multi-class classification problem is simple: F1-Score = 2 × (precision × recall) / (precision + recall), but this leads to the problem Hadley is facing: How can she combine the individual F1-score values into a single F1-Score value?

The Macro-average F1-Score is one approach where we calculate the F1-Score for each category and then average all the results. This method penalizes the model equally for any class that doesn't perform well, regardless of its importance or how many support samples it has.

The Micro-average F1-Score is another approach where we sum all of the contributions from each category to compute an aggregated F1-Score. In this case, we don't use the individual F1-Scores but the overall precision and recall across all samples. This method doesn't favor or penalize any class in particular.

Finally, the Weighted F1-Score is another way of computing a global F1-Score. In this approach, we weigh every individual F1-Score using the number of true labels of each class and then sum them to produce the global F1-Score. This method favors the majority classes because they will be weighted more in the computation.

The ROC F1-Score is a made-up term and therefore is not correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Micro, Macro & Weighted Averages of F1 Score, Clearly Explained"](https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f) is a great article covering how to compute a global F1-Score metric in multi-class classification problem.
* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.
* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.</p></details>

-----------------------

## Date - 2022-10-14


## Title - High-speed train distance


### **Question** :

Ladybug and Lemon are working on a machine learning system to control high-speed trains.

They decided to use k-Nearest Neighbors in one of the system modules but had to stop when they realized they had to work with categorical data.

After thinking about it, they used one-hot encoding to convert the data into a vector of zeros and ones. All left was to pick the appropriate distance measure to compute the similarity between two one-hot-encoded columns. 

**Which of the following is the appropriate measure to compute the distance between two one-hot encoded vectors?**


### **Choices** :

- Minkowski distance
- Euclidean distance
- Manhattan distance
- Hamming distance


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The Minkowski distance is a generalization of the Euclidean and Manhattan distances. Both of these work with real-value vectors, but the Euclidean distance is the shortest path between objects, while the Manhattan distance is the rectilinear distance between them. Using the Minkowski distance, we can control which approach to use depending on the data. 

Ladybug and Lemon, however, need the distance between categorical features encoded using one-hot encoding.

The Hamming distance computes the distance between two binary vectors. It's the ideal function for one-hot encoded vectors.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["4 Distance Measures for Machine Learning"](https://machinelearningmastery.com/distance-measures-for-machine-learning/) for a complete explanation of these four distance measures.
* ["Five Common Distance Measures in Data Science With Formulas and Examples"](https://regenerativetoday.com/five-common-distance-measures-in-data-science-with-formulas-and-examples/) is a deeper dive into these distance measures.</p></details>

-----------------------

## Date - 2022-10-15


## Title - Maximum performance


### **Question** :

Claire and Phil aren't on the same page with their plan.

They want to train a machine learning model but want to minimize the number of samples they need to label. Labeling takes too long, and they want to avoid it as much as possible.

Claire argues that they don't need to train with the entire dataset. Instead, she believes they can maximize the model's performance without using all the data. 

Phil disagrees. He argues that the only way to achieve the maximum possible performance is to train with the entire dataset. Since they aren't willing to label all the data, they will need to settle for a mediocre model.

**What's your opinion about this situation?**


### **Choices** :

- Achieving the maximum possible performance without using the entire dataset is theoretically possible but very unlikely.
- They can achieve the maximum possible performance without using the entire dataset by randomly sampling a portion of the data, labeling it, and training the model.
- They can achieve the maximum possible performance without using the entire dataset, but they need a good strategy to sample the data they will label to train the model.
- They will never achieve the maximum possible performance without using the entire dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Claire and Phil do not need to use the entire dataset to build a model that reaches its maximum possible performance. However, they will need a smart strategy to select the data they need to label.

Let's imagine a dataset with two classes that we can represent in two dimensions and a linear model that splits the data into two groups. Any samples around the lines' boundaries that separate both classes are critical in our dataset. Those samples help the model decide how to split the data!

But what about samples far away from the split? They contribute much less to the model, and we don't need them to find the separation between classes. The same happens with duplicate samples or samples that are too similar to existing ones.

Claire and Phil, however, can't depend on randomly sampling the dataset to decide which instances to label. They need a better strategy to determine which samples to pick.

This scenario is an example of [Active learning](https://articles.bnomial.com/active-learning). This learning technique allows us to build a better-performing machine learning model using fewer training labels by strategically choosing the samples to train the model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Active Learning"](https://articles.bnomial.com/active-learning) is a short introduction to active learning and how the process works.</p></details>

-----------------------

## Date - 2022-10-16


## Title - The model's recall


### **Question** :

A team built a binary classification model. They named the classes `A` and `B`.

After finishing training, they evaluated the model on a validation set, and here is the confusion matrix with the results:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186248358-1b39a042-5725-408b-84aa-8dd915a6d99d.jpg)

**Given the above confusion matrix, what of the following correctly represents the model's recall predicting classes `A` and `B`, respectively?**


### **Choices** :

- The model's recall predicting class `A` is 80%, and class `B` is 80%.
- The model's recall predicting class `A` is 88%, and class `B` is 68%.
- The model's recall predicting class `A` is 88%, and class `B` is 88%.
- The model's recall predicting class `A` is 68%, and class `B` is 88%.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To compute the model's recall, we can use the following formula:

```
recall = TP / (TP + FN)
```

Let's start with class `A`. We have 52 true positive samples and 7 false negatives in this example. Substituting these values in our formula:

```
recall = TP / (TP + FN)
recall = 52 / (52 + 7)
recall = 52 / 59
recall = 0.88
```

Therefore, the model's recall at predicting class `A` is 88%.

We can now look at class `B`. We have 28 true positive samples and 13 false negatives in this example. Substituting these values in our formula:

```
recall = TP / (TP + FN)
recall = 28 / (28 + 13)
recall = 28 / 41
recall = 0.68
```
Therefore, the model's recall at predicting class `B` is 68%.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.
* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.</p></details>

-----------------------

## Date - 2022-10-17


## Title - Logistic regression exercices


### **Question** :

There were four different exercises, but Jade only knew how to use logistic regression.

Luckily, Jade only needed to choose one exercise and solve it. But she needs to ensure she can find a solution using logistic regression; she doesn't have time to learn anything else.

**Select every exercise that Jade could solve using logistic regression?**


### **Choices** :

- The first exercise is a regression problem.
- The second exercise is a binary classification problem.
- The third exercise is a 2-class classification problem.
- The fourth exercise is a 3-class classification problem.


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Fortunately for Jade, she can tackle the second, third, and fourth exercises.

[Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) estimates the probability of an event occurring by outputting a single value bounded between 0 and 1. The closer the result is to zero, the less likely the event will occur, while the closer is to 1, the more likely it is. This structure makes logistic regression ideal for tackling binary or 2-class classification problems. Notice that we can formulate a binary classification task as a 2-class classification task and vice versa.

Jade can also work on the 3-class classification exercise using the One-vs-All method (also called "One-vs-Rest.") She can train three different logistic regression models:

* Model 1: Identifying Class 1 vs. [Class 2, Class 3]
* Model 2: Identifying Class 2 vs. [Class 1, Class 3]
* Model 3: Identifying Class 3 vs. [Class 1, Class 2]

The final result would be the prediction from the model with the highest confidence.

Jade should stay away from the first exercise. Logistic regression will not help her solve a regression problem.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check out ["Logistic Regression for Machine Learning"](https://machinelearningmastery.com/logistic-regression-for-machine-learning/) for an introduction to Logistic regression.
- ["Essential Data Science Tips: How to Use One-Vs-Rest and One-Vs-One for Multi-Class Classification"](https://www.kdnuggets.com/2020/08/one-vs-rest-one-multi-class-classification.html) is a great reference to understand how to use the One-vs-All method for multi-class classification.</p></details>

-----------------------

## Date - 2022-10-18


## Title - Scrubbing movies


### **Question** :

Over the years, Brianna's agency pivoted to work for big Hollywood studios. They scrubbed early releases looking for mistakes and gathering information to determine the potential ratings that a movie would get.

But watching every hour of every film didn't scale, so they used an active learning approach to only select critical scenes for their team to review. 

They created a machine learning model to predict the ratings. They trained this model on a few randomly selected frames from each movie, processed the entire video, and used the output predictions to decide which scenes to review next. They retrained the model with the new labels and repeated several more iterations.

Thanks to this process, the team reduced the review time by more than 60 percent.

**Which of the following is the team's criterion to select which scenes they will manually review after each iteration?**


### **Choices** :

- The team selects any scene their model predicts with high confidence.
- The team selects any scene their model predicts with low confidence.
- The team selects any scenes that the model predicts correctly.
- The team selects any scenes where the model makes a mistake.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Active learning](https://articles.bnomial.com/active-learning) is a learning technique that allows us to build a better-performing machine learning model using fewer training labels by strategically choosing the samples to train the model.

There are many ways to select which scenes will be better for the model, but in summary, we want to strategically choose those that will provide the most value to the model.

The model returns the predicted ratings and the confidence in those predictions. High-confidence samples aren't a challenge for the model, and the amount of information the team will get from labeling them is low. Conversely, low-confidence samples indicate that the model needs help, so the team should focus their time exclusively on them.

Notice that the team doesn't know which mistakes the model makes unless they manually review each scene. Doing this will defeat the purpose of reducing the time they take reviewing footage. They should only rely on the prediction confidence to decide what portions of the video they should consider.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Active Learning"](https://articles.bnomial.com/active-learning) is a short introduction to active learning and how the process works.</p></details>

-----------------------

## Date - 2022-10-19


## Title - Low training and testing


### **Question** :

There's not a lot of context for you other than the following chart showing the training and testing losses of a machine learning model:

![Training and Testing Loss Chart](https://user-images.githubusercontent.com/1126730/188513133-fefa6ed8-9541-4bbe-b1f2-cae6e19c09f7.jpg)

As you can see, after finishing training, both losses are low.

**What's a reasonable conclusion about this machine learning model?**


### **Choices** :

- Your model is overfitting.
- Your model is underfitting.
- Your model is either overfitting or underfitting, but we can't tell.
- Your model is well-fit.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A good model should capture valuable patterns in the data and discard any noise that doesn't help with predictions. An overfitting model will fit that noise. An underfitting model will not capture the relevant patterns in the dataset. 

An overfitting model should not have any problems with the training data but stumble with the testing data. Therefore, we should expect a low training loss and a high testing loss. An underfitting model should struggle with the training and testing datasets, so both of its losses should be high.

A well-fit model, however, should have low training and testing losses, which is what we see in the chart.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.</p></details>

-----------------------

## Date - 2022-10-20


## Title - Junior suggestions


### **Question** :

Amaya leads the team responsible for maintaining a machine learning model that helps run their company's marketing budget. The model has been running for a long time, and Amaya's team makes periodic updates and improvements.

But they want more.

The team aims to find some breakthroughs to improve the model considerably. Surprisingly, the most junior person on the team was the one coming up with two different ideas: 

1. Take each row separately, and count the missing values across all columns. Then add a new column to the dataset with that number.
2. Replace a categorical feature on their dataset with the number of times each value appears across all samples.

**Which of the following would be your recommendation for Amaya regarding these two ideas?**


### **Choices** :

- Amaya shouldn't consider any of these techniques because they aren't valid forms of feature engineering.
- Amaya should only consider the first technique. The second one is not a valid form of feature engineering.
- Amaya should only consider the second technique. The first one is not a valid form of feature engineering
- Amaya should consider both methods.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Amaya should experiment with both techniques to see whether they improve their model. The two suggestions are examples of meta-features based on the rows and columns of the team's dataset.

The first suggestion, for example, will help the model understand which rows have an excess of missing values and which are complete. Of course, there's no guarantee this information is helpful, but that's something Amaya will have to decide based on her experiments.

The second suggestion is an example of frequency encoding. It will make evident to the model the importance of an individual row based on how prominently it appears on the data. For instance, we could replace a job position title with the number of employees with that title.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.
* [_Feature Engineering for Machine Learning_](https://amzn.to/3SsnLAc) is an excellent book covering feature engineering.</p></details>

-----------------------

## Date - 2022-10-21


## Title - Classifying bottles


### **Question** :

Esther decided to do something with her machine learning model to get better results from her model.

She was responsible for a deep learning model to classify bottles in a factory. Instead of running every picture through her model, she created two more copies of the image: the first by flipping the original image vertically and the second by zooming in around 10%.

Esther ran all three images through the model. She figured that having multiple images to construct the final answer would give her better results. 

**What's the best way for Esther to use the three softmax vectors that her model will output to make a final decision? Select only one option.**


### **Choices** :

- Esther should select the best one of the three vectors that come out of her model.
- Esther should use the softmax vector that contains the lowest value to compute the final result.
- Esther should use the softmax vector that includes the highest value to calculate the final result.
- Esther should average out every softmax vector and use that new vector to compute the final result.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Esther is using [Test-time augmentation](https://articles.bnomial.com/test-time-augmentation), a technique that relies on augmenting the original image before running it through a model. By doing this, Esther gives the model more opportunities to compute the correct prediction.

When Esther runs three pictures through the model, she will get back three different vectors. How can she use these vectors to determine the final result?

The best approach from the ones provided in this question is to average all three vectors and use the result to compute the correct class. By doing this, Esther will take advantage of each picture to make the final decision.

There's no way to know which vector is "the best one," so the first option is incorrect. Choosing the vector will the lowest value doesn't seem like a great idea to make the best prediction, and although Esther could use the vector with the highest value, averaging all vectors will be a better approach.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check [Test-time augmentation](https://articles.bnomial.com/test-time-augmentation) for an introduction to a technique that will help you make better predictions with your machine learning model.</p></details>

-----------------------

## Date - 2022-10-22


## Title - Know-it-all students


### **Question** :

Athena is preparing for next week's lecture. She teaches computer vision at a prestigious college and has to deal with a couple of know-it-all students.

Athena needs to ensure her material is sharp.

She will cover Max Pooling, when to use it, and its advantages. To finish her lecture, she prepared a quiz just in case the students gave her a hard time.

**Here's Athena's quiz. Which of the following is true about Max Pooling?**


### **Choices** :

- Max pooling will always decrease the number of parameters in the network compared to another network that doesn't use Max pooling.
- Max pooling will always increase the number of parameters in the network compared to another network that doesn't use Max pooling.
- Max pooling will improve the network's ability to detect the same features in different locations within the image.
- Max pooling extracts smooth features by calculating the average value for every patch on the feature map.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>There's a "gotcha" in Athena's quiz. We commonly use Max pooling to downsample feature maps, thus reducing the number of parameters in the network, but it depends on how large we configure the pool size. The first choice of Athena's quiz states that Max pooling will always decrease the number of parameters, but if we use a pool size of 1, there will be no downsampling. Thus, neither the quiz's first nor the second choices are correct.

Max pooling does add translation invariance properties to the network, which is important because we usually care about whether a feature is present in the image rather than where it is. 

Finally, the fourth choice describes how Average pooling works. On the other hand, Max pooling uses the maximum value in a patch rather than the average value.

Athena's students will undoubtedly have a lot of fun with the quiz.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Max Pooling in Convolutional Neural Network and Its Features"](https://analyticsindiamag.com/max-pooling-in-convolutional-neural-network-and-its-features/) is a great introduction to Max Pooling.
* Check ["A Gentle Introduction to Pooling Layers for Convolutional Neural Networks"](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/) for more information about how pooling layers work.</p></details>

-----------------------

## Date - 2022-10-23


## Title - The beer factory


### **Question** :

Logan works in the quality department of a beer factory.

She is responsible for maintaining several machine learning models that the factory uses to research and develop new drinks and containers.

While evaluating a new process, Logan put together the following table to show her manager the results. The red chips correspond to defective containers, and the model's results are those within the black lines at the center:

![Model Results](https://articles.bnomial.com/images/article-when-accuracy-doesnt-help-4.jpg)

**Which of the following is the correct f1-score of this model?**


### **Choices** :

- The f1-score of Logan's model is 43%.
- The f1-score of Logan's model is 50%.
- The f1-score of Logan's model is 86%.
- The f1-score of Logan's model is 100%.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The model's f1-score is the harmonic mean between precision and recall. We can use the following formula to compute it:

![F1-Score](https://articles.bnomial.com/images/article-when-accuracy-doesnt-help-3.jpg)

Both the precision and recall of this model are 43%. Substituting this value in the previous formula, we get that the f1-score of Logan's model is also 43%.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.
* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.</p></details>

-----------------------

## Date - 2022-10-24


## Title - Computationally expensive


### **Question** :

Elliana's mandate to her team was to build a model as fast as possible and put it out there.

She leads a startup focused on keeping costs down and providing value as quickly as possible. Elliana understands that investing resources into a better model has diminishing returns. 

Her team is trying to decide what process to follow to build their model. They want to prioritize an approach that's as computationally cheap as possible. They have a list of potential choices.

**Looking at the list, which will be the first option the team should discard because of how computationally expensive it is?**


### **Choices** :

- Splitting the dataset and training a model with 80% of the data while testing it with the remaining 20%.
- Using a leave-one-out cross-validation approach to build the model.
- Using a 5-Fold cross-validation approach to build the model.
- Using a 10-Fold cross-validation approach to build the model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The team should scratch off the [leave-one-out cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation) approach right away.

Leave-one-out cross-validation is a variant of cross-validation where the number of folds equals the number of samples in the dataset. 

To use leave-one-out cross-validation, we build one model for each sample in the dataset. We train each model using all data except one instance we later use to evaluate its performance. Finally, we compute the overall performance by averaging the result of each model.

Assuming the team will use leave-one-out cross-validation on a dataset with 10,000 samples, they will need to train 10,000 models. Compare this with 10-Fold cross-validation, where they will only need to build ten models. 

On the other hand, leave-one-out cross-validation will give a more robust estimate of model performance. Each sample has an opportunity to represent the entire dataset and contribute to the final evaluation, and this will result in a reliable and unbiased estimate of model performance.

The team is not interested in better performance and wants a cheaper model to train. Leave-one-out cross-validation is significantly more expensive than every other choice on this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["LOOCV for Evaluating Machine Learning Algorithms"](https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/) is an excellent introduction to leave-one-out cross-validation.
* Check ["A Quick Intro to Leave-One-Out Cross-Validation (LOOCV)"](https://www.statology.org/leave-one-out-cross-validation/) for a succinct introduction to leave-one-out cross-validation.</p></details>

-----------------------

## Date - 2022-10-25


## Title - Paul, the octopus


### **Question** :

During the 2010 world cup, an octopus named Paul became famous for correctly predicting the result of 8 soccer matches with no misses. 

Paul's keepers would show him two boxes of food, each decorated with a team's flag. Whichever box Paul ate from first would be his prediction.

The probability of doing what Paul did was a mere 0.39%, hence his rise to fame.

**Which of the following is a correct statement about this story?**


### **Choices** :

- This story is an example of survivorship bias.
- This story is an example of confirmation bias.
- This story is an example of group attribution bias.
- This story doesn't show any biases.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Paul was undoubtedly an impressive octopus, and without taking anything from him rather than magic, this situation shows the effects of [survivorship bias](https://en.wikipedia.org/wiki/Survivorship_bias).

While the cameras focused on Paul because of his success, we don't consider all the other animals that tried but failed. If we assume that 1,000 animals tried to predict all eight games, probabilistically 3.9 of them would have gotten the result correctly.

Of course, nobody cared about animals that failed their predictions but immediately gravitated toward the seemingly supernatural Paul. 

Survivorship bias seriously compromises our ability to determine the odds of something happening. We focus on what we can see (the winners) and ignore what we can't see (the losers). Always seek out stories of failure and learn from them.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The ["Survivorship bias"](https://en.wikipedia.org/wiki/Survivorship_bias) page in Wikipedia is an excellent resource.
* Paul has a [Wikipedia page](https://en.wikipedia.org/wiki/Paul_the_Octopus) as well.</p></details>

-----------------------

## Date - 2022-10-26


## Title - Model structures


### **Question** :

Nothing bothered Josie more than trying to learn something new without first looking at practical examples. 

The professor knew Josie well, so he wasn't surprised when she stood up with four different scenarios during his presentation about Recurrent Neural Networks.

Her question was straightforward but made everyone think:

**Which of the following scenarios can you model using a Recurrent Neural Network?**


### **Choices** :

- A model that takes a fixed-size vector as the input and outputs a sequence of vectors.
- A model that takes a sequence of vectors as the input and outputs a fixed-size vector.
- A model that takes a sequence of vectors as the input and outputs a sequence of vectors.
- A model that takes a fixed-size vector as the input and outputs a fixed-sized vector.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Recurrent Neural Networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNNs) have several advantages over vanilla neural networks. One that helps answer this question is their ability to use sequences of vectors, both as their input and output.

A feed-forward network, for example, is limited to taking a fixed-size input vector and outputting another fixed-size vector. Unfortunately, this characteristic limits their usefulness.

Generating an image caption is an example of a model that requires a fixed-size vector as its input (the image) and a sequence of vectors as the output (the generated caption.) 

Sentiment analysis is an example of a model that takes a sequence of vectors as its input (the text) and outputs a fixed-size vector (a numerical value indicating the sentiment.)

To build a model capable of translating text from one language to another, we could use a sequence of vectors as the input (the original text) and output another sequence of vectors (the translation.)

Finally, we don't need an RNN to build a model that takes a fixed-size vector as its input and output, but that doesn't mean that RNNs don't support this structure. [Here is an example](https://arxiv.org/abs/1412.7755) using an RNN to process a picture (fixed-size vector) and outputs the number on the image (fixed-size vector.)

In summary, we could build an RNN model with any structures listed in this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["The Unreasonable Effectiveness of Recurrent Neural Networks"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy for the motivation behind Recurrent Neural Networks.
* ["An Introduction To Recurrent Neural Networks And The Math That Powers Them"](https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/) is an excellent introduction to Recurrent Neural Networks.</p></details>

-----------------------

## Date - 2022-10-27


## Title - Evaluating logistic regression


### **Question** :

Eva is working on a logistic regression model.

She works for an auto manufacturer that wants to predict whether previous customers are potential future buyers.

Eva has a well-balanced dataset and is ready to evaluate her model after a few iterations.

**Which of the following are viable ways for Eva to evaluate her model?**


### **Choices** :

- Eva could evaluate the model by computing its accuracy.
- Eva could evaluate the model by computing its log loss.
- Eva could evaluate the model using the Mean Squared Error.
- Eva could evaluate the model using the Mean Absolute Error.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Eva is building a [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) model to solve a binary classification problem. She is only concerned about two classes: potential buyers and non-potential buyers.

Eva could compute the accuracy of her model and use it to determine how good it is. Accuracy is misleading for imbalanced datasets, but this is not the case.

Eva could also compute the log loss of her model to evaluate it. You can use the log loss to determine how close are the prediction probabilities returned by the model to the actual values. The closer these are, the lower the log loss will be.

Finally, Eva cannot use the [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) (MSE) or the [Mean Absolute Error](https://en.wikipedia.org/wiki/Mean_absolute_error) (MAE) to evaluate her model. These metrics work for regression problems, where the model's output is a continuous value, but Eva is working on a classification problem with a model's output bounded between 0 and 1.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check out ["Logistic Regression for Machine Learning"](https://machinelearningmastery.com/logistic-regression-for-machine-learning/) for an introduction to Logistic regression.
- ["Intuition behind Log-loss score"](https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a) is a great explanation of log loss and how it works.</p></details>

-----------------------

## Date - 2022-10-28


## Title - Representing music


### **Question** :

Ada needs to process a dataset of songs from her favorite musician and create a compact representation of what makes her music unique and distinctive.

She read about autoencoders and decided to design a network to solve her problem. But creating the bottleneck to force the model to represent the music wasn't as simple as she thought.

**Which of the following are valid approaches that Ada could follow to design the bottleneck of her autoencoder?**


### **Choices** :

- Ada can introduce an information bottleneck by constraining the number of nodes in the middle part of the network.
- Ada can introduce an information bottleneck by ensuring the middle part of the network has sufficient nodes to represent the input data in its original form.
- Ada can introduce an information bottleneck by using the proper optimization algorithm.
- Ada can introduce an information bottleneck by using a loss function that relies on activating a small number of nodes.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Autoencoders](https://essays.bnomial.com/autoencoders) are learning algorithms that can help with anomaly detection problems. 

An autoencoder is a neural network that we can split into three sections: an encoder, a bottleneck, and a decoder. The encoder compresses the original input into an intermediate representation, and the decoder reverses the process to reconstruct the original data. The bottleneck sits between the encoder and decoder and is the section that stores the compressed representation of the data. 

Regular autoencoders constrain the number of nodes in the bottleneck layers of the network. This characteristic forces the model to learn a compact representation of the data instead of memorizing the input. Therefore, the first choice is correct.

Sparse autoencoders employ a different technique to introduce an information bottleneck. They use a loss function that relies on a small number of nodes. These autoencoders don't need bottleneck layers since penalizing activations is enough to accomplish the same purpose. Therefore, the fourth option is also correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Autoencoders"](https://articles.bnomial.com/autoencoders) for an introduction to a learning technique to represent data efficiently using neural networks.
* For more information about sparse autoencoders, check the ["Introduction to autoencoders"](https://www.jeremyjordan.me/autoencoders/) article by Jeremy Jordan.</p></details>

-----------------------

## Date - 2022-10-29


## Title - True Negatives


### **Question** :

Here is the picture of a confusion matrix we generated after evaluating a model in a dataset with 100 samples:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186962046-7c076163-5b58-442f-9641-c09f5cf71003.jpg)

Assume that class B represents the outcomes of the model we are interested in finding.

**What's the total of True Negatives on this evaluation round?**


### **Choices** :

- True Negatives are class A samples the model predicted as class B, so the answer is 7.
- True Negatives are class B samples the model predicted as class A, so the answer is 13.
- True Negatives are class A samples the model predicted as class A, so the answer is 52.
- True Negatives are class B samples the model predicted as class B, so the answer is 28.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We are interested in samples from class B, which means we will treat class B as our "Positive" samples and class A as our "Negative" samples.

If we replace the classes in the confusion matrix with Positive and Negative instead of "B" and "A," it's much easier to reason about the model's number of True Positives:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/187207950-77e4eccd-0b63-43bc-8f07-be19f690d725.jpg)

True Negatives are those samples that we expect to be Negative (class A), and the model predicted as Negative (class A.) Therefore, the correct answer to the question is 52. You can see every combination in the image above.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.
* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2022-10-30


## Title - A morning walk


### **Question** :

Maria was taking her morning walk when she decided to modify her deep neural network architecture.

She's been researching different ways to speed up her training process and decided to experiment with Batch Normalization right before one of the hidden layers of her network.

There was only one question left to answer:

**How should Maria use Batch Normalization as part of her model?**


### **Choices** :

- Maria should use Batch Normalization before the activation function of the layer that comes before the layer she wants to affect.
- Maria should use Batch Normalization after the activation function of the layer that comes before the layer she wants to affect.
- Maria should use Batch Normalization right after the layer she wants to affect but before that layer's activation function.
- Maria should use Batch Normalization right after the activation function of the layer she wants to affect.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Batch Normalization is a popular technique used to train deep neural networks. It normalizes the input to a layer during every training iteration using a mini-batch of data. It smooths and simplifies the optimization function leading to more stable and faster training.

Maria has two options to batch-normalize the input of the hidden layer she is interested in: she can use Batch Normalization before the layer she wants to affect, either before the previous activation function or after. Remember that Batch Normalization will normalize the data that goes into the layer, so Maria needs to ensure to apply it before that layer.

But where is it better to use Batch Normalization? Before or after the previous layer's activation function?

The authors of ["Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"](https://arxiv.org/pdf/1502.03167.pdf) recommend using it right before the activation function:

> The goal of Batch Normalization is to achieve a stable distribution of activation values throughout training, and in our experiments we apply it before the nonlinearity since that is where matching the first and second moments is more likely to result in a stable distribution.

That has been the way many teams have used Batch Normalization, but later, there are experiments showing that Batch Normalization works best when used after the previous layer's activation function. Here is an excerpt from ["Busting the Myth About Batch Normalization"](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/) from the [Paperspace blog](https://blog.paperspace.com/):

> While the original paper talks about applying batch norm just before the activation function, it has been found in practice that applying batch norm after the activation yields better results. This seems to make sense, as if we were to put an activation after batch norm, then the batch norm layer cannot fully control the statistics of the input going into the next layer since the output of the batch norm layer has to go through an activation. 

Following the last information we have, for her use case, Maria should use Batch Normalization after the previous's layer activation function.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"](https://arxiv.org/pdf/1502.03167.pdf) is the original paper introducing Batch Normalization.
* ["Intro to Optimization in Deep Learning: Busting the Myth About Batch Normalization"](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/) is the blog post challenging the original paper's position regarding where to use Batch Normalization.
* ["A Gentle Introduction to Batch Normalization for Deep Neural Networks"](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/) is a good introduction to Batch Normalization.</p></details>

-----------------------

## Date - 2022-10-31


## Title - Encoding features


### **Question** :

After looking at the data she got from her client, Bailey knew she had a lot of work in front of her.

Many features were categorical, and some had a very high cardinality. Baily planned to use a neural network to process everything, so she had to encode every column.

**Which of the following encoding techniques could Bailey use to replace the categorical features of her dataset?**


### **Choices** :

- Label encoding
- Ordinal encoding
- One-hot encoding
- Target encoding


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Every option is a valid encoding technique.

Label encoding replaces each category with a consecutive number starting from 0. We can use Label encoding for nominal variables, where order doesn't matter. For example, "cloudy," "rainy," and "sunny."

On the other hand, Ordinal encoding works similarly to Label encoding, but we use it when the order matters. For example, "first," "second," and "third."

One-Hot encoding creates a new feature for each unique value of the original categorical variable. For example, a "weather" feature with three values will get us three new features, one for each value of the original "weather" column. 

Finally, Target encoding helps process categorical features with high cardinality. If we use one-hot encoding on a column with too many different values, we will end up with a sparse representation that will be cumbersome. Instead, target encoding replaces the categories of a column with the average target value of all data points belonging to that category.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["6 Ways to Encode Features for Machine Learning Algorithms"](https://towardsdatascience.com/6-ways-to-encode-features-for-machine-learning-algorithms-21593f6238b0) to see some of these encoding techniques in action.
* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.
* [_Feature Engineering for Machine Learning_](https://amzn.to/3SsnLAc) is an excellent book covering feature engineering.</p></details>

-----------------------

## Date - 2022-11-01


## Title - Regularizing a model


### **Question** :

Charlotte's machine learning model is overfitting.

She needs to find a way to handle it, but before trying anything, she wants to understand her options.

**Which of the following are regularization techniques that Charlotte could consider?**


### **Choices** :

- Validation-based early stopping
- Dropout
- Data augmentation
- Cross-validation


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Validation-based early stopping is a regularization technique that stops the training process as soon as the generalization error of the model increases. In other words, if the model's performance on the validation set starts degrading, the training process stops. 

Dropout is a regularization method that works well and is vital for reducing overfitting. Dropout randomly removes a percentage of the nodes, forcing the network to learn in a balanced way, and tackling a phenomenon that we call "[co-adaptation](https://machinelearning.wtf/terms/co-adaptation/)."

Data augmentation has a regularization effect. Increasing the training data through data augmentation decreases the model's variance and, in turn, increases the model's generalization ability.

Finally, cross-validation is a validation scheme and not a regularization method.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Early Stopping"](https://articles.bnomial.com/early-stopping) for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models.
* ["A Gentle Introduction to Dropout for Regularizing Deep Neural Networks"](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) is an excellent introduction to Dropout.
* ["The Essential Guide to Data Augmentation in Deep Learning"](https://www.v7labs.com/blog/data-augmentation-guide) is an excellent article discussing data augmentation in detail.</p></details>

-----------------------

## Date - 2022-11-02


## Title - Dropout and loss


### **Question** :

It's the final round of interviews, and Savannah has done wonderfully well.

The final round is more technical. Savannah needs to answer a series of questions to demonstrate her understanding of different fundamental concepts.

While talking about deep learning, the interviewer focuses on the use of Dropout and asks Savannah a question about something she hasn't thought about before:

**How would you expect the training loss to behave if you train your model several times, each using an increasing Dropout rate?**


### **Choices** :

- The training loss will be higher as we increase the Dropout rate.
- The training loss will be lower as we increase the Dropout rate.
- The training loss will start oscillating as we increase the Dropout rate.
- The training loss will stay the same independently of the Dropout rate.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Dropout is a regularization method that works well and is vital for reducing overfitting.

Sometimes, the nodes in a neural network create strong dependencies on other nodes, which may lead to overfitting. An example is when a few nodes on a layer do most of the work, and the network ignores all the other nodes. Despite having many nodes on the layer, you only have a small percentage of those nodes contributing to predictions. We call this phenomenon "[co-adaptation](https://machinelearning.wtf/terms/co-adaptation/)," and we can tackle it using Dropout.

During training, Dropout randomly removes a percentage of the nodes, forcing the network to learn in a balanced way. Now every node is on its own and can't rely on other nodes to do their work. They have to work harder by themselves. 

One crucial characteristic of Dropout will help Savanah answer the question correctly: Like most regularization methods, Dropout sacrifices training accuracy to improve generalization. 

If we run a few training sessions, each using an increasing amount of Dropout, we should see the training loss trend higher. In other words, the more we regularize our model, the harder it will be to learn the training data.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For more information about co-adaptation and how to use Dropout, check ["Improving neural networks by preventing
co-adaptation of feature detectors"](https://arxiv.org/pdf/1207.0580.pdf).
* ["A Gentle Introduction to Dropout for Regularizing Deep Neural Networks"](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) is an excellent introduction to Dropout.</p></details>

-----------------------

## Date - 2022-11-03


## Title - A decision tree for that?


### **Question** :

Gianna had the opportunity to meet some of her technical heroes at a conference, and of course, she peppered them with questions!

At some point, the conversation centered around a machine learning solution they implemented. Gianna was delighted to hear that they used a decision tree for their solution.

The next day, she was still thinking about the conversation. She didn't have much machine learning experience but knew enough about decision trees to feel validated. It was fantastic to find out that decision trees are useful!

**Which of the following machine learning problems can you solve with a decision tree?**


### **Choices** :

- Binary classification problems where you need to decide the correct category for a sample among two possible choices.
- Multi-class classification problems where you need to decide the correct category for a sample among multiple choices.
- Multi-label classification problems where you need to decide the correct categories for a sample among multiple choices.
- Regression problems where you need to predict a continuous output.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To get straight to the answer, every one of the choices is a correct answer: decision trees are really powerful!

We usually discuss two types of decision trees in machine learning: classification and regression trees. The former covers the first three choices, while the latter covers the fourth choice.

Binary classification problems aim to classify one sample into two different categories. Multi-class classification problems are similar, but they classify samples into more than two categories. Decision trees are a perfect fit for these problems.

[Multi-label classification](https://en.wikipedia.org/wiki/Multi-label_classification) is somewhat different. Here we want to classify a sample into one or more categories. Decision trees can also solve these problems. Check out [Scikit-Learn's implementation](https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification) to see how they tackle multi-label classification.

Finally, decision trees can also solve regression problems where we want to predict a continuous target variable. ["How can Regression Trees be used for Solving Regression Problems?"](https://medium.com/analytics-vidhya/regression-trees-decision-tree-for-regression-machine-learning-e4d7525d8047) is an excellent introduction.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Scikit-Learn's ["Multiclass and multioutput algorithms"](https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification) will introduce you to solve these problems using Scikit-Learn's decision tree implementation.
* ["How can Regression Trees be used for Solving Regression Problems?"](https://medium.com/analytics-vidhya/regression-trees-decision-tree-for-regression-machine-learning-e4d7525d8047) is an excellent article about using decision trees for regression tasks.</p></details>

-----------------------

## Date - 2022-11-04


## Title - Weather predictions


### **Question** :

For the first project, Cam wants to work with the weather dataset he found online.

Cam's Machine Learning class was a ton of fun. Their professor let them choose a problem to solve to allow them to showcase what they've learned so far. 

Cam wants to predict the probability of snowing based on four factors: the date, the air temperature, the location, and the air pressure. 

**Which of the following is the most appropriate algorithm that Cam should use to solve this problem?**


### **Choices** :

- Cam should use linear regression.
- Cam should use logistic regression.
- Cam should use K-means.
- Cam should use DBSCAN.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Cam's problem has two possible outcomes: it will snow or it won't, and Cam wants his model to return the probability of it. Logistic regression is an excellent fit for any problem with a binary outcome.

Logistic regression estimates the probability of an event occurring based on a dataset of independent variables. Linear regression, on the other hand, predicts the continuous dependent variable using a dataset of independent variables. 

K-means and DBSCAN are clustering algorithms and are not a good approach for this problem.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Linear Regression vs Logistic Regression"](https://www.javatpoint.com/linear-regression-vs-logistic-regression-in-machine-learning) does a full comparison between linear and logistic regression.
* Check ["8 Clustering Algorithms in Machine Learning that All Data Scientists Should Know"](https://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/) for an explanation of K-Means and DBSCAN.</p></details>

-----------------------

## Date - 2022-11-05


## Title - Precision is more important


### **Question** :

Here is the Fβ score formula:

![FBeta-Score](https://user-images.githubusercontent.com/1126730/193036770-0a37f6f7-9a5d-4be3-9b89-7018931140eb.jpg)

The Fβ score lets us combine precision and recall into a single metric.

Let's say you want to use this formula to measure a model where higher precision is more important.

**What's the correct value for the β parameter to achieve this?**


### **Choices** :

- β = 0
- β = 0.5
- β = 1
- β = 2


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The Fβ score lets us combine precision and recall into a single metric. When using β = 1, we place equal weight on precision and recall. For values of β > 1, recall is weighted higher than precision; for values of β < 1, precision is weighted higher than recall.

When β = 0, the Fβ score only takes precision into account; therefore, the correct answers to this question are β = 0 and β = 0.5.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What is the F-Score?"](https://deepai.org/machine-learning-glossary-and-terms/f-score) is a short introduction to this metric.
* ["Micro, Macro & Weighted Averages of F1 Score, Clearly Explained"](https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f) is a great article covering how to compute a global F1-Score metric in multi-class classification problem.
* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2022-11-06


## Title - Low training


### **Question** :

There's not a lot of context for you other than the following chart showing the training loss of a machine learning model:

![Training Loss Chart](https://user-images.githubusercontent.com/1126730/188471836-1959ae8e-44dd-4811-9606-2d1c1f9ded0c.jpg)

As you can see, after finishing training, the loss is very low.

**What's a reasonable conclusion about this machine learning model?**


### **Choices** :

- The model may be overfitting, but we can't say for sure.
- The model may be underfitting, but we can't say for sure.
- The model may be well-fit, but we can't say for sure.
- The model is either overfitting or underfitting, but it's not well-fit.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A good model should capture valuable patterns in the data and discard any noise that doesn't help with predictions. An overfitting model will fit that noise. An underfitting model will not capture the relevant patterns in the dataset. 

An overfitting model should not have any problems with the training data, so we should expect a low training loss. An underfitting model should struggle with the training data, so its training loss will be high.

This model shows a low training loss, which we expect from an overfitting model, but this is not enough information to conclude that this model is overfitting. The model might be well-fit, but we can't say unless we evaluate it on a separate dataset. Therefore, the first and third options are correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.</p></details>

-----------------------

## Date - 2022-11-07


## Title - Salary leak


### **Question** :

There was a massive data leak, and for some mysterious reason, you came across a dataset full of compensation data from top tech companies in Silicon Valley.

You thought it wouldn't hurt to play around with the data for a little bit. You could finally build a model to predict future compensation based on the different attributes of each employee.

But one thing becomes apparent from the start: You need to cut down useless features to build something useful.

Dimensionality reduction to the rescue. You haven't done it before and want to ensure you are doing it correctly.

**How should you apply dimensionality reduction to your data?**


### **Choices** :

- Reduce the dimensions of the training dataset. It's unnecessary to use dimensionality reduction on the test dataset.
- Reduce the dimensions of the entire dataset. Split your data into training and test right after.
- Reduce the dimensions of the training dataset, then reduce the dimensions of the test dataset. We can use different dimensionality reduction techniques as long as both splits end up with the same features.
- Reduce the dimensions of the training dataset, then apply the same transformations to the test dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>You found there are too many features to create a useful model. The Curse of Dimensionality states that, as the dimensionality of the data increases, the amount of data needed to train a learning algorithm grows exponentially. 

There are different techniques to reduce the dimensionality of a dataset. They all follow the same principle: you start with a dataset, reduce its dimensionality, and obtain a new dataset with fewer features. Depending on the technique, the final dataset may contain a subset of the initial features or even have entirely different columns not present in the initial dataset.

The first choice argues that you only need to worry about reducing the dimension of the training dataset. That's incorrect. How can you test a model trained with a dataset containing different features? 

The second choice argues for reducing the dimensionality of the entire dataset and splitting the data right after that. Dimensionality reduction algorithms like PCA will use information about the whole dataset to produce new features. If we apply this algorithm to all of our data—including the test data, which we aren't supposed to know about— we'll leak details from the test data into the training set. 

The third choice is also incorrect. You need to apply dimensionality reduction separately to the training and test datasets and make sure you use the same transformations from the training data on the test data.

For example, imagine your dimensionality reduction technique creates a new feature based on the mean of another two columns. If you compute this mean separately on the train and test data, the resulting columns will come from different mean values. You need to avoid this problem by using what the fourth choice suggests: apply the same transformations and use the same information from the training and testing sets.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["What is the Curse of Dimensionality?"](https://deepai.org/machine-learning-glossary-and-terms/curse-of-dimensionality) for an introduction to the Curse of Dimensionality.
* For an explanation of data leakages, check ["Data Leakage in Machine Learning"](https://machinelearningmastery.com/data-leakage-machine-learning/).
* ["Introduction to Dimensionality Reduction for Machine Learning"](https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/) covers different examples of dimensionality reduction.</p></details>

-----------------------

## Date - 2022-11-08


## Title - Two experiments. Round two.


### **Question** :

Lena ran two experiments training a neural network on a sample dataset. Her goal is to understand how different batch sizes affect the training process.

One of the experiments used a very small batch size, and the other used a batch size equal to all existing training data.

After training and evaluating her models, Lena plotted the training and testing losses over 100 epochs of one of the experiments, and this is what she got:

![Learning Curves](https://user-images.githubusercontent.com/1126730/191095137-39c02bc1-a0f7-42bf-8c90-a331f9811ab9.jpg)

**Which of the following options is the most likely to be true?**


### **Choices** :

- The model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took less time to train.
- The model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took longer to train.
- The model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took less time to train.
- The model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took longer to train.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The amount of noise in the plot is crucial to answering this question.

The lower the batch size, the more noise we will get in the model's loss. When we only use a few samples, any instances that vary dramatically will cause the loss to swing wildly. When we use a larger batch, no individual sample would have the power to sway the loss too much, so we should expect less noise.

This plot shows a fair amount of noise, so this is likely the experiment that uses a small batch size.

The smaller the batch size, the more times we will need to compute the loss and update the model's weights during backpropagation. For example, if Lena's training set has 800 samples and uses one as her batch size, the algorithm will update the model's weights 800 times. However, if she uses a batch size of 32, the algorithm will update the model's weights 25 times (800 / 32 = 25.)

Updating the model during every iteration is computationally expensive, so using a smaller batch size will usually take longer. Therefore, the second option is probably the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["The wrong batch size is all it takes" ](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) explains how different batch sizes influence the training process of neural networks using gradient descent.
* Check ["An overview of gradient descent optimization algorithms"](https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants) for a deep dive into gradient descent and every one of its variants.</p></details>

-----------------------

## Date - 2022-11-09


## Title - Divisible numbers


### **Question** :

Lilly needs to pick a number divisible by 3 or by 7.

She can multiply them and use 21, but she needs to pick one number that's not larger than 20.

**Assuming Lilly selects a random number between 1 and 20, what's the probability it will be divisible by 3 or 7?**


### **Choices** :

- The probability is 12%
- The probability is 35%
- The probability is 40%
- The probability is 56%


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's assume that A is the event where Lilly selects a number divisible by 3, and B is where her number is divisible by 7. We want to find the probability of `A u B`, which denotes the event where Lilly selects a number divisible by 3 or 7.

The numbers divisible by 3 are 3, 6, 9, 12, 15, and 18. The numbers divisible by 7 are 7 and 14. Since the intersection of these two sets is empty, we know that events A and B are mutually exclusive, therefore:

```
P(A u B) = P(A) + P(B)
P(A u B) = 6/20 + 2/20
P(A u B) = 8/20
P(A u B) = 40%
```</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Cartoon Guide to Statistics_](https://amzn.to/3eg9iIo) is a fun and instructive introduction to probabilities and statistics.</p></details>

-----------------------

## Date - 2022-11-10


## Title - Increasing KNN's K


### **Question** :

Nyla started experimenting with her k-Nearest Neighbor (KNN) implementation. 

After trying many different things, she decided to increase the value of `K` until she landed in a good spot.

Unfortunately, Nyla can't explain why what she did work.

**What happened as Nyla increased the value of `K`?**


### **Choices** :

- As Nyla increased the value of K, she reduced the algorithm's variance and bias.
- As Nyla increased the value of K, she increased the algorithm's variance and bias.
- As Nyla increased the value of K, she increased the algorithm's variance and reduce its bias.
- As Nyla increased the value of K, she reduced the algorithm's variance and increase its bias.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The smaller the value of `K`, the more variance and less bias KNN will exhibit. For example, if we use `K = 1`, a single sample close to our observation will cause the algorithm to return the wrong prediction. Imagine an observation surrounded by many instances from class A and only one from class B that's closer than everything else. Since `K=1`, the algorithm will incorrectly predict the observation as class B. 

Conversely, the larger the value of `K`, the less variance and more bias KNN will exhibit. Since KNN uses an average or majority voting, no individual sample will cause the algorithm to return the wrong prediction. Setting `K` to a value that's too large will make the algorithm underfit because it won't capture the variance in the dataset.

Here is a quote from ["Why Does Increasing k Decrease Variance in kNN?"](https://towardsdatascience.com/why-does-increasing-k-decrease-variance-in-knn-9ed6de2f5061):

> If we take the limit as `K` approaches the size of the dataset, we will get a model that just predicts the class that appears more frequently in the dataset [...]. This is the model with the highest bias, but the variance is 0 [...]. High bias because it has failed to capture any local information about the model, but 0 variance because it predicts the exact same thing for any new data point.

In summary, the smaller the value of `K` is, the lower the bias and the higher the variance. The larger the value of `K` is, the higher the bias and the lower the variance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Why Does Increasing k Decrease Variance in kNN?"](https://towardsdatascience.com/why-does-increasing-k-decrease-variance-in-knn-9ed6de2f5061) is a great article diving into the relationship of `k` and the variance of KNN.
* For a more general introduction to the bias-variance trade-off, check ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/).</p></details>

-----------------------

## Date - 2022-11-11


## Title - Testing loss going nuts


### **Question** :

There's not a lot of context for you other than the following chart showing the training and testing losses of a machine learning model:

![Training and Testing Loss Chart](https://user-images.githubusercontent.com/1126730/188513097-b11e85a0-9bd2-436c-b8e4-9a0595c1cff9.jpg)

As you can see, the training loss is low after finishing training, but the testing loss stops decreasing at some point and starts climbing.

**What's a reasonable conclusion about this machine learning model?**


### **Choices** :

- Your model is underfitting.
- You are training a model for too many epochs.
- Your model is well-fit to the point where the testing loss increases. It overfits after that.
- Your model is well-fit to the point where the testing loss increases. It underfits after that.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A good model should capture valuable patterns in the data and discard any noise that doesn't help with predictions. An overfitting model will fit that noise. An underfitting model will not capture the relevant patterns in the dataset. 

An overfitting model should not have any problems with the training data but stumble with the testing data. Therefore, we should expect a low training loss and a high testing loss. An underfitting model should struggle with the training and testing datasets, so both of its losses should be high. A well-fit model, however, should have low training and testing losses.

A couple of things happen in this chart. First, up to the point the testing loss starts increasing, the model is well-fit. Then, the testing loss suddenly starts increasing, which means we trained the model for too long. Right when this happens, the model starts overfitting. It's memorizing the training data, it's not generalizing anymore.

A simple technique to fix this issue is [Early stopping](https://articles.bnomial.com/early-stopping). In short, we want to stop training right before the model overfits.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.
* Check ["Early stopping"](https://articles.bnomial.com/early-stopping) for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models.</p></details>

-----------------------

## Date - 2022-11-12


## Title - Increasing lambda


### **Question** :

It's time for London to regularize her Linear Regression model. 

She is getting great results on the training data, but her model is overfitting: London's validation error is too high.

But London doesn't know exactly how to use the lambda (λ) parameter that controls the regularization on her model, so she decided to increase it and see what happens.

**Which of the following will eventually happen as London continuously increases the value of λ?**


### **Choices** :

- London's model will eventually underfit, and the validation error will increase.
- London's model will eventually overfit, and the validation error will increase.
- London's model will eventually underfit, and the validation error will decrease.
- London's model will eventually overfit, and the validation error will decrease.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>London can control the model's regularization using the lambda (λ) parameter. She can tune this parameter to decide how much she wants to penalize the model's flexibility. 

Increasing the value of λ will reduce the value of the coefficients, thus reducing the variance of the model. 

Since London's model is overfitting, reducing the variance is a good step. As the value of λ increases, the model will start losing essential information, and its bias will increase, leading to underfitting and a higher validation error.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Regularization in Machine Learning"](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a) for a detailed explanation of how regularization works.
* ["Fighting Overfitting With L1 or L2 Regularization: Which One Is Better?"](https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization) will give you a complete introduction to L1 and L2 regularization.</p></details>

-----------------------

## Date - 2022-11-13


## Title - Off-the-shelf encoder


### **Question** :

Marley used an off-the-shelf encoder to process a dataset of images and create embeddings for each. The next step for her is to try and interpret the latent space of her embeddings.

A specific question she has is related to the dimensionality of the space. She can control the encoder's output, so Marley wonders whether a high-dimensional space would be better than a lower-dimensional one.

**Which of the following statements about the dimensionality of a latent space are correct?**


### **Choices** :

- A high-dimensional latent space is more sensitive to specific features from the input object than a low-dimensional latent space.
- A high-dimensional latent space is less sensitive to specific features from the input object than a low-dimensional latent space.
- A high-dimensional latent space is more prone to overfitting than a low-dimensional latent space.
- A low-dimensional latent space is more prone to overfitting than a high-dimensional latent space.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In machine learning, we use "latent space" to refer to a multi-dimensional space containing a meaningful internal representation of objects and where similar points appear closer together. 

Latent spaces usually have a lower dimensionality than the feature space used to draw specific data points. Because of this, projecting an object in a latent space is typically a dimensionality reduction exercise.

The more dimensions in the latent space, the more sensitive it is to specific features from the input objects. In other words, small changes in the input data could cause significant variations in their representation in latent space. This makes high-dimensional spaces more likely to overfit than low-dimensional spaces.

On the other hand, the lower the latent space's dimensionality, the less sensitive to small changes in the input data. The more the encoder compresses the data, the fewer details will make it into the latent space. Low-dimensional spaces capture the essential features of the input data and are more robust to overfitting.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check the [Latent space's Wikipedia page](https://en.wikipedia.org/wiki/Latent_space) for more information.</p></details>

-----------------------

## Date - 2022-11-14


## Title - Grid competition


### **Question** :

Sutton wanted to win the competition. She knew the only path forward was to dedicate as much time as possible to tune her model and squeeze as much performance as possible from it.

Sutton used a library to tune her model's hyperparameters on the test set. She used Grid Search to try to cover as many combinations as possible and chose the hyperparameters that gave her the lowest training error.

**What do you think about Sutton's chances of winning the competition?**


### **Choices** :

- Sutton's approach gives her a fighting chance.
- Sutton's approach will produce a model that fails to run.
- Sutton's approach will likely result in an overfit model.
- We need more information to determine whether Sutton has a chance of winning the competition.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Sutton used the test set to choose the best hyperparameters, which could lead to a model that overfits to the testing dataset. 

This model will have trouble with unseen data because Sutton optimized it to work well on the test dataset. Instead, she should have used a validation set for hyperparameter tuning.

The test set is helpful for a final evaluation of the model, but anytime we use it to make changes to the model, we risk overfitting it.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Overview of hyperparameter tuning"](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview) is a great introduction to hyperparameters and the process of finding their optimal value.
* ["Train, Validation, Test Split for Machine Learning"](https://blog.roboflow.com/train-test-split/) goes into detail about the importance of each split.</p></details>

-----------------------

## Date - 2022-11-15


## Title - The model's f1-score


### **Question** :

A team built a binary classification model. They named the classes `A` and `B`.

After finishing training, they evaluated the model on a validation set, and here is the confusion matrix with the results:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186248358-1b39a042-5725-408b-84aa-8dd915a6d99d.jpg)

**Given the above confusion matrix, what is the f1-score of this binary classification model at predicting class `A`?**


### **Choices** :

- The f1-score of the model at predicting class `A` is 52%.
- The f1-score of the model at predicting class `A` is 80%.
- The f1-score of the model at predicting class `A` is 84%.
- The f1-score of the model at predicting class `A` is 88%.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To compute the model's f1-score at predicting class `A`, we can use the following formula:

```
f1-score = 2 * (precision * recall) / (precision + recall)
```

We can simplify this formula in the following way:

```
f1-score = TP / (TP + 1/2 * (FP + FN))
```

In this example, we have 52 true positives, 13 false positives, and 7 false negative samples. Substituting these values in our formula:

```
f1-score = TP / (TP + 1/2 * (FP + FN))
f1-score = 52 / (52 + 1/2 * (13 + 7))
f1-score = 52 / (52 + 1/2 * 20)
f1-score = 52 / (52 + 10)
f1-score = 52 / 62
f1-score = 0.84
```

Therefore, the model's f1-score at predicting class `A` is 84%.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.
* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.</p></details>

-----------------------

## Date - 2022-11-16


## Title - Loss up and down


### **Question** :

The worst thing that could happen to Thea is happening: she is training a neural network, and her training loss goes up and down and never settles:

![Learning Curves](https://user-images.githubusercontent.com/1126730/191616550-37c5cea8-1bc6-4008-b30e-ec9eeceb9c86.jpg)

Up to this point, she is using mini-batch gradient descent to train the network. She has talked to people and collected some feedback. She put it all together in the list below.

**Which of the following will help Thea solve the problem?**


### **Choices** :

- Use a lower learning rate.
- Use a higher learning rate.
- Decrease the batch size to train the network.
- Increase the batch size to train the network.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Increasing the learning rate will allow the model to take more significant steps in the gradient's direction, but it may miss the local minima. If it does, it will have to go back, and the same will happen again. A learning rate that's too high could be causing the oscillation, so Thea shouldn't keep increasing it. Conversely, reducing the learning rate is an excellent next step that could solve the problem.

Thea is using mini-batch gradient descent to train her model. The more she decreases the batch size, the more every sample will influence the shape of the loss. In a small batch, one instance could swing the loss significantly. Therefore, Thea should consider increasing the batch size to smooth the loss.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["The wrong batch size is all it takes" ](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) explains how different batch sizes influence the training process of neural networks using gradient descent.
* ["What could an oscillating training loss curve represent?"](https://ai.stackexchange.com/questions/14079/what-could-an-oscillating-training-loss-curve-represent) is a StackExchange question that will help you answer this question.
* Check ["Why is my training loss fluctuating?"](https://www.researchgate.net/post/Why_is_my_training_loss_fluctuating) for another set of answers covering this problem.</p></details>

-----------------------

## Date - 2022-11-17


## Title - Compact activation function


### **Question** :

Here is a simple and compact implementation of a neural network in Python:

![Neural network](https://user-images.githubusercontent.com/1126730/196750241-0a53d7bf-d821-43e9-bf5c-560e6ca842d4.png)

**Which of the following is the activation function used in this network?**


### **Choices** :

- This implementation uses a ReLU activation function.
- This implementation uses a sigmoid activation function.
- This implementation uses a softmax activation function.
- This implementation doesn't use an activation function.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The code defines the output layer in Line 9. 

To compute the output of this second layer, we need to multiply the second set of weights (`W2`) by the output of the previous layer (`layer1`). Then we wrap this result with the activation function.

In this case, the output layer uses a sigmoid activation function:

![Sigmoid](https://user-images.githubusercontent.com/1126730/196755518-311dd425-676e-4c85-be1f-467694879c30.jpg)</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Neural Networks and Deep Learning_](http://neuralnetworksanddeeplearning.com/index.html) is a free online book written by [Michael Nielsen](https://twitter.com/michael_nielsen) with a great introduction to neural networks.
* ["A Neural Network in 13 lines of Python"](https://github.com/ijustloveses/machine_learning/blob/master/gred_descent_and_dropout_with_numpy/A%20Neural%20Network%20in%2013%20lines%20of%20Python%20(Part%202%20-%20Gradient%20Descent).pdf) was the inspiration for the code in this question.</p></details>

-----------------------

## Date - 2022-11-18


## Title - Sparse labels


### **Question** :

Evie was reading the code in an [online article](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) and noticed that the author used Keras' `SparseCategoricalCrossentropy` as the loss function to train the neural network:

```
model.compile(
    optimizer=SGD(learning_rate=0.01), 
    loss="sparse_categorical_crossentropy", 
    metrics=["accuracy"]
)
```

Evie was familiar with the general concept, but she wasn't sure how this was different from the regular `CategoricalCrossentropy.`

**When should you use `SparseCategoricalCrossentropy` instead of `CategoricalCrossentropy`?**


### **Choices** :

- When the labels in the dataset are integer values.
- When the labels in the dataset are one-hot encoded.
- When the labels in the dataset are categorical values.
- When the labels in the dataset have a lot of sparse values.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Keras' `SparseCategoricalCrossentropy` computes the cross-entropy loss between the labels and predictions. It works when the labels in the dataset are integer values, for example, `1`, `2`, and `3`.

Keras' `CategoricalCrossentropy`, on the other hand, has the same function but works when the labels in the dataset are one-hot encoded, for example, `[1, 0, 0]`, `[0, 1, 0]`, and `[0, 0, 1]`.

The [article](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) that Evie saw generates a random dataset and assigns an integer to each of the three classes. That's the reason the network uses a `sparse_categorical_crossentropy` loss.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Here is Keras' [SparseCategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) and [CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) documentation.
* ["The wrong batch size is all it takes"](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) is the article that inspired this question.</p></details>

-----------------------

## Date - 2022-11-19


## Title - False Positives


### **Question** :

Here is the picture of a confusion matrix we generated after evaluating a model in a dataset with 100 samples:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186962046-7c076163-5b58-442f-9641-c09f5cf71003.jpg)

Assume that class A represents the outcomes of the model we are interested in finding.

**What's the total of False Positives on this evaluation round?**


### **Choices** :

- False Positives are class A samples the model predicted as class B, so the answer is 7.
- False Positives are class B samples the model predicted as class A, so the answer is 13.
- False Positives are class A samples the model predicted as class A, so the answer is 52.
- False Positives are class B samples the model predicted as class B, so the answer is 28.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We are interested in samples from class A which means we will treat class A as our "Positive" samples and class B as our "Negative" samples.

If we replace the classes in the confusion matrix with Positive and Negative instead of "A" and "B," it's much easier to reason about the model's number of False Positives:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186963826-ec29e2b5-c065-4569-9542-712acab129da.jpg)

False Positives are those samples that we expect to be Negative (class B), but the model predicted as Positive (class A.) Therefore, the correct answer to the question is 13. You can see every combination in the image above.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.
* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2022-11-20


## Title - Fastest hit


### **Question** :

Freya finished her first trial competition for the Olympics. Her hit was the fastest she's ever run!

After finishing every hit, the judges computed the final scores. Freya's trainer mentioned that the mode time was 25 seconds!

**Which of the following is the correct interpretation of the trainer's comment?**


### **Choices** :

- None of the runners ran faster than 25 seconds.
- Anyone who ran in 26 seconds was below average.
- More runners ran in 25 seconds than any other time.
- Anyone who ran in 24 seconds was above average.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The mean, median, and mode are different ways to measure the center of a dataset. Each of these metrics tries to summarize a set of values using a single number.

The mode of a dataset represents the most frequent number. The correct interpretation of Freya's trainer comment is that 25 seconds was the most common time among all runners.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Mean Median Mode: What They Are, How to Find Them"](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/) for a complete explanation of the Mean, Median, and Mode.</p></details>

-----------------------

## Date - 2022-11-21


## Title - Nuanced conversations


### **Question** :

Twitter is not a great place to have nuanced discussions.

I don't think anyone was surprised to see people picking apart Journey's tweet.

She tried to explain Linear Regression in less than 280 characters and, unsurprisingly, had to leave many details out.

But not every comment she received was correct. Turns out that you can't listen to every person online!

**Which of the following comments are factual statements about Linear Regression?**


### **Choices** :

- Linear Regression is an Unsupervised Learning technique useful for solving Regression problems.
- You can use Linear Regression to predict a continuous dependent variable with the help of independent variables.
- Linear Regression aims to find the best sigmoid curve that can accurately predict the output for the continuous dependent variable.
- In Linear Regression, the relationship between the dependent and independent variables must be linear.


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Linear Regression is probably the most popular Supervised Learning technique in machine learning. Its goal is to fit the best line through the data to predict a continuous output. 

The algorithm uses a set of independent variables to predict a continuous dependent variable. For example, a person's age, salary, or home price. 

Finally, for Linear Regression to work, we must ensure that the relationship between the inputs and the output is linear. A Linear Regression model won't give us good predictions if the relationship isn't linear. Sometimes, this condition means we must transform the input features before using Linear Regression. For example, if you have a variable with an exponential relationship with the target variable, you can use log transform to turn the relationship linear.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Linear Regression for Machine Learning"](https://machinelearningmastery.com/linear-regression-for-machine-learning/) is an introduction to Linear Regression.</p></details>

-----------------------

## Date - 2022-11-22


## Title - Same distribution


### **Question** :

Kehlani suspected that her training and test data didn't come from the same distribution.

To prove it, she mixed all the data, removed the target variable, and added a new binary target that contained a value of 1 for each training sample and a value of 0 for each test sample.

She then trained a new binary classification model using this new dataset to see whether she could separate the samples based on whether they belonged to the training or test sets.

Kehlani used a ROC curve to evaluate the results of the model. She looked at the area under the curve to make her final determination.

**Based on Kehlani's strategy, which of the following is the correct way to evaluate her model?**


### **Choices** :

- If the area under the curve is close to 1.0, the training and test samples come from the same distribution, and if it's close to 0.5, they come from different distributions.
- If the area under the curve is close to 0.5, the training and test samples come from the same distribution, and if it's close to 1.0, they come from different distributions.
- If the area under the curve is close to 1.0, the training and test samples come from the same distribution, and if it's close to 0.0, they come from different distributions.
- The area under the curve is not a good metric to determine whether this new model performs accurately.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Kehlani's strategy is called ["Adversarial validation"](https://essays.bnomial.com/adversarial-validation), and it's a clever technique to determine whether your training and test data come from the same distribution.

After she created the new model, she decided to use a ROC curve and evaluate the results based on the area under the curve (AUC.) This metric measures the area underneath the ROC curve, which ranges between 0.0 and 1.0.

The more accurate the model results are, the higher the area under the curve will be. Therefore, if the model can tell the training and test samples apart, the closer the AUC will be to 1.0, and Kehlani can conclude that both sets don't come from the same distribution.

Conversely, the closer the model performs to random chance, the worse it is at telling training samples apart from test samples. In this case, Kehlani will see the AUC close to 0.5, which means that both sets come from the same distribution.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Adversarial validation"](https://articles.bnomial.com/adversarial-validation) for a quick introduction to this technique.
* Check ["Classification: ROC Curve and AUC"](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) for an explanation of how to create and interpret a ROC curve.</p></details>

-----------------------

## Date - 2022-11-23


## Title - Something wrong


### **Question** :

"There's something wrong with your network."

That was the start of a message Camille received from her friend. She posted the plot of a neural network training loss online, and Camilla's friend reached out to let her know.

Camille is using gradient descent for the first time, so she appreciated the help. 

"Do you see what happened around the ninety epoch? The loss increases for a moment before coming back down again until the end. You don't want that,"—concluded the message.

**What would you do if you were in Camille's shoes?**


### **Choices** :

- Camille should decrease the learning rate. That should stop the loss from increasing during training.
- Camille should increase the learning rate. That should stop the loss from increasing during training.
- Camille should use Early Stopping at around the ninety epoch.
- Camille shouldn't do anything because her network has no problem.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Gradient descent moves downhill on average, so a network that learns appropriately should see the loss decrease over the training session. However, individual updates can move in the opposite direction, causing the loss to fluctuate up and down.

Camille's plot shows the loss increasing momentarily, but it immediately starts decreasing. That's normal, and Camille shouldn't worry about it.

Since the training process seems to be working correctly, modifying the learning loss might improve the results, but there's nothing Camille needs to fix. If she uses [Early Stopping](https://articles.bnomial.com/early-stopping), she will prevent the network from improving further.

In summary, Camille shouldn't do anything at this point.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["An overview of gradient descent optimization algorithms"](https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants) for a deep dive into gradient descent and every one of its variants.
* Check ["Early Stopping"](https://articles.bnomial.com/early-stopping) for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models.</p></details>

-----------------------

## Date - 2022-11-24


## Title - KNN's runtime


### **Question** :

Evangeline is working with a dataset with a single column of data. 

She wants to run k-Nearest Neighbors (KNN), but only if the algorithm is fast enough when making predictions.

**Assuming there are _n_ samples in the dataset, which of the following will be the runtime of Evangeline's KNN at prediction time?**


### **Choices** :

- The runtime that Evangeline should expect is _O(1)_
- The runtime that Evangeline should expect is _O(n)_
- The runtime that Evangeline should expect is _O(log n)_
- The runtime that Evangeline should expect is _O(n²)_


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>k-Nearest Neighbors' runtime is _O(nd)_ because we need to compute the distance to each feature of every sample. Here _n_ represents the number of instances, and _d_ the number of features. 

Evangeline is working with a single feature, so _d_ = 1. Therefore, in this case, the runtime of KNN is _O(n)_.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Here is a [Stack Exchange answer](https://stats.stackexchange.com/q/219664) that covers KNN's runtime complexity in detail.</p></details>

-----------------------

## Date - 2022-11-25


## Title - Basic components


### **Question** :

Dakota is trying to understand how neural networks work from their most basic components. She has one neuron with two input values, `x1` and `x2`, and uses the tanh activation function. 

Dakota initialized the network with weights `w1 = -3`, `w2 = 1`, and the bias term with `6` and used `x1 = 2` and `x2 = 0` as the input values.

**Assuming Dakota wants to increase the value of the output, how should she modify the weights?**


### **Choices** :

- Dakota should increase the value of `w1`
- Dakota should decrease the value of `w1`
- Dakota should increase the value of `w2`
- Dakota should decrease the value of `w2`


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The first important thing we should notice is that we are passing the value 0 as the second input (`x2`) of the neuron. That means that `w2` will not influence this neuron's output.

To determine whether we should increase or decrease the value of `w1` we could look at the expression to compute the output of the neuron:

```
output = tanh(x1 * w1 + x2 * w2 + b)
 ```

Since `x2 = 0`, we can simplify the expression:

```
output = tanh(x1 * w1 + b)
```

Let's substitute the initial values:

```
output = tanh(2 * -3 + 6)
output = 0
```

We know that the neuron's output is zero with the initial values, and we want to increase it by modifying the value of `w1`. 

The tanh function returns a positive value for any number greater than 0 and a negative value for any number less than 0. Therefore, we want to modify the value of `w1` to make the input of tanh a positive value, which we can accomplish by increasing `w1.`

We can also arrive at the same result by computing the gradients of the weights. Here is how you could do that using PyTorch:

![Example](https://user-images.githubusercontent.com/1126730/196778961-5d44438f-de9c-446a-8274-e03eb27beff4.png)

If you run the above code, the `w1`'s gradient will be `2.0`, and `w2`'s gradient will be zero, corroborating that we need to modify `w1` with a positive value, and `w2` will not influence the output.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The inspiration for this question comes from Andrej Karpathy's video ["The spelled-out intro to neural networks and backpropagation"](https://www.youtube.com/watch?v=VMj-3S1tku0).</p></details>

-----------------------

## Date - 2022-11-26


## Title - A few achievements


### **Question** :

We have seen Deep Learning become a force over the last two decades.

A few critical algorithmic improvements allowed the community to start training deep neural networks reliably.

**Which of the following are some achievements we have made since the early 2000s?**


### **Choices** :

- We proposed the idea of neural networks to replace multi-layer perceptrons.
- We created better activation functions for neural layers
- We came up with better weight-initialization schemes
- We created better optimization schemes


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In 1944, Warren McCulloch and Walter Pitts proposed the idea of neural networks, so they have been around for quite some time. But it wasn't until the late 2000s when the community started making significant improvements that enabled the popularity of Deep Learning. 

In [Deep Learning with Python, Second Edition](https://amzn.to/3K3VZoy), François Chollet writes referring to the history of Deep Learning:

> This changed around 2009-2010 with the advent of several simple but important algorithmic improvements (...):
>
> * Better activation functions for neural layers
> * Better weight-initialization schemes
> * Better optimization schemes

These were some of the algorithm improvements that allowed the community to start using deep learning.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check [_Deep Learning with Python, Second Edition_](https://amzn.to/3K3VZoy) for a great introduction to Deep Learning.</p></details>

-----------------------

## Date - 2022-11-27


## Title - Denoising images


### **Question** :

Olive is playing with the MNIST dataset of handwritten digits.

She decided to build a denoising autoencoder. To train it, she takes every image, generates a copy with random salt and pepper noise, and uses it as the input to the autoencoder. She compares the output of the network with the clean image to compute the reconstruction error.

**Which category of learning better represents Olive's autoencoder?**


### **Choices** :

- This autoencoder is an example of a supervised learning algorithm.
- This autoencoder is an example of a semi-supervised learning algorithm.
- This autoencoder is an example of a self-supervised learning algorithm.
- This autoencoder is an example of a simple-supervised learning algorithm.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>One of the applications of [Autoencoders](https://essays.bnomial.com/autoencoders) is removing noise from images. 

An autoencoder is a neural network that we can split into three sections: an encoder, a bottleneck, and a decoder. The encoder compresses the original input into an intermediate representation, and the decoder reverses the process to reconstruct the original data. The bottleneck sits between the encoder and decoder and is the section that stores the compressed representation of the data. 

In this example, Olivia is training the autoencoder without needing to label the dataset manually. The input to the network is an image with autogenerated noise, and the "label" is the same clean image. The lack of manual annotations pushes this autoencoder outside of the supervised-learning domain.

Semi-supervised learning problems require at least a few annotations we use to train and decide on what other annotations we should procure. Simple-supervised learning is not an existing learning approach.

We can classify this particular autoencoder as self-supervised learning. Notice that the literature also classifies autoencoders as unsupervised, which is also correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Autoencoders"](https://articles.bnomial.com/autoencoders) for an introduction to a learning technique to represent data efficiently using neural networks.
* Read the accepted answer under ["What is self-supervised learning in machine learning?"](https://ai.stackexchange.com/questions/10623/what-is-self-supervised-learning-in-machine-learning) Stack Exchange question.</p></details>

-----------------------

## Date - 2022-11-28


## Title - Porting dropouts


### **Question** :

Valentina is porting her PyTorch code over to TensorFlow. 

Her new company bought Valentina's startup, and the first order of business was to migrate the code base and integrate it into the new company's ecosystem.

The deep learning model they built uses Dropout on a few hidden layers. Valentina knows that both TensorFlow's and Pytorch's implementations are the same: they zero out some of the nodes and scale the remaining. However, she doesn't remember exactly how these work.

**Assuming Valentina is looking at a Dropout with a rate of `0.2`, which of the following are correct?**


### **Choices** :

- A Dropout rate of `0.2` will set 80% of the nodes to zero.
- A Dropout rate of `0.2` will set every node to zero with a probability of 20%.
- A Dropout rate of `0.2` will scale the value of every remaining node—those not set to zero—by multiplying them by `1.25`.
- A Dropout rate of `0.2` will scale the value of every remaining node—those not set to zero—by multiplying them by `0.2`.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Dropout is a regularization method that works well and is vital for reducing overfitting.

Sometimes, the nodes in a neural network create strong dependencies on other nodes, which may lead to overfitting. An example is when a few nodes on a layer do most of the work, and the network ignores all the other nodes. Despite having many nodes on the layer, you only have a small percentage of those nodes contributing to predictions. We call this phenomenon "[co-adaptation](https://machinelearning.wtf/terms/co-adaptation/)," and we can tackle it using Dropout.

During training, Dropout randomly removes a percentage of the nodes, forcing the network to learn in a balanced way. Now every node is on its own and can't rely on other nodes to do their work. They have to work harder by themselves. 

Valentina is right. Both [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) and [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) use the same implementation of Dropout that follows this process:

1. It zeros out every node of the layer with the specified probability. In this case, a Dropout with a `0.2` rate will zero out every node with a 20% probability.
2. It scales the remaining nodes to account for the missing values. The scaling factor is `1/(1-rate)`. In this case, if we can substitute the rate by `0.2`, we get that it will scale nodes by `1.25`.

Therefore, the second and third choices answer this question correctly.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For more information about co-adaptation and how to use Dropout, check ["Improving neural networks by preventing
co-adaptation of feature detectors"](https://arxiv.org/pdf/1207.0580.pdf).
* ["A Gentle Introduction to Dropout for Regularizing Deep Neural Networks"](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) is an excellent introduction to Dropout.
* Here is [TensorFlow's implementation of Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout).
* Here is [PyTorch's implementation of Droput](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html).</p></details>

-----------------------

## Date - 2022-11-29


## Title - Three gradients


### **Question** :

Here is a chart showing the gradient of three different functions:

![Gradients](https://user-images.githubusercontent.com/1126730/198387377-cee47720-8729-409f-a119-7280d547f73d.jpg)

These gradients correspond to Sigmoid, Tanh, and ReLU, but we don't know exactly the order.

**Which of the following correctly identifies the three functions?**


### **Choices** :

- Function 1 is the gradient of Tanh, function 2 is the gradient of Sigmoid, and function 3 is the gradient of ReLU.
- Function 1 is the gradient of Sigmoid, function 2 is the gradient of Tanh, and function 3 is the gradient of ReLU.
- Function 1 is the gradient of ReLU, function 2 is the gradient of Tanh, and function 3 is the gradient of Sigmoid.
- Function 1 is the gradient of Sigmoid, function 2 is the gradient of ReLU, and function 3 is the gradient of Tanh.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Here is the Python code you can use to plot the gradient of the three functions:

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def deriv_sigmoid(x):
    return sigmoid(x) * (1 - sigmoid(x))

def tanh(x):
    return 2 / (1 + np.exp(-2 * x)) - 1

def deriv_tanh(x):
    return 1 - tanh(x) ** 2

def deriv_relu(x):
    return (x > 0) * 1

x = np.arange(-5., 5., 0.2)
fn1 = deriv_tanh(x)
fn2 = deriv_sigmoid(x)
fn3 = deriv_relu(x)

plt.figure(figsize=(8, 6), dpi=80)

plt.plot(x, fn1, label = "function 1")
plt.plot(x, fn2, label = "function 2")
plt.plot(x, fn3, label = "function 3")

plt.legend()
plt.show()
```

As you see, function 1 is the gradient of Tanh, function 2 is the gradient of Sigmoid, and function 3 is the gradient of ReLU.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For a complete description of the Sigmoid function, check the ["Logistic function"](https://en.wikipedia.org/wiki/Logistic_function) Wikipedia page.
* For a complete description of the Tanh function, check the ["Hyperbolic Functions"](https://en.wikipedia.org/wiki/Hyperbolic_functions) Wikipedia page.
* For a complete description of the ReLU function, check the ["Rectifier (neural networks)"](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) Wikipedia page.</p></details>

-----------------------

## Date - 2022-11-30


## Title - Dataset of applicants


### **Question** :

Annabelle wanted to understand who was applying to her company's open jobs.

What do these people have in common? 

Annabelle had access to the entire dataset of applicants and had plenty of information about them.

**What would be your recommendation for Annabelle?**


### **Choices** :

- Annabelle should use a supervised learning algorithm.
- Annabelle should use an unsupervised learning algorithm.
- Annabelle should use a reinforcement learning algorithm.
- Annabelle should use a semi-supervised learning algorithm.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Annabelle doesn't know what characteristics the applicants share, so a clustering algorithm should be a good initial step.

For example, she could use [K-Means](https://en.wikipedia.org/wiki/K-means_clustering) to find interesting patterns and group the applicants that share them. A critical distinction is that you don't need to consider these groups preemptively; the clustering algorithm will find them for you. 

Clustering algorithms are part of unsupervised learning, so the second choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Customer Segmentation with Machine Learning"](https://towardsdatascience.com/customer-segmentation-with-machine-learning-a0ac8c3d4d84) for a quick introduction to Customer Segmentation.
* ["10 Clustering Algorithms With Python"](https://machinelearningmastery.com/clustering-algorithms-with-python/) will introduce you to 10 different clustering algorithms.</p></details>

-----------------------

## Date - 2022-12-01


## Title - The model's precision


### **Question** :

A team built a binary classification model. They named the classes `A` and `B`.

After finishing training, they evaluated the model on a validation set, and here is the confusion matrix with the results:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186248358-1b39a042-5725-408b-84aa-8dd915a6d99d.jpg)

**Given the above confusion matrix, what is the precision of this binary classification model at predicting class `B`?**


### **Choices** :

- The precision of the model at predicting class `B` is 28%.
- The precision of the model at predicting class `B` is 52%.
- The precision of the model at predicting class `B` is 80%.
- The precision of the model at predicting class `B` is 88%.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To compute the model's precision at predicting class `B`, we can use the following formula:

```
precision = TP / (TP + FP)
```

In this example, we have 28 true positive samples and 7 false positive samples. Substituting these values in our formula:

```
precision = TP / (TP + FP)
precision = 28 / (28 + 7)
precision = 28 / 35
precision = 0.8
```

Therefore, the model's precision at predicting class `B` is 80%.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.
* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.</p></details>

-----------------------

## Date - 2022-12-02


## Title - Every city


### **Question** :

Camilla was feeling the Curse of Dimensionality in her bones.

Her dataset had a column representing each city in the U.S. It was an important feature, and she couldn't eliminate it. Camilla needed that feature, but one-hot encoding produced thousands of new columns, making building her binary classifier with that dataset cumbersome.

Fortunately, she found a way around her issue: she replaced each column's category with the posterior probability of the target being positive on the presence of that category.

And it worked! 

**What's the name of this encoding technique?**


### **Choices** :

- Camilla used Label encoding.
- Camilla used Ordinal encoding.
- Camilla used Target encoding.
- Camilla used Posterior encoding.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Label encoding replaces each category with a consecutive number starting from 0. It's useful when the order of the categories doesn't matter. On the other hand, Ordinal encoding works similarly to Label encoding, but we use it when the order of categories matters. Camilla didn't use any of these two encoding techniques. 

Target encoding is another technique that helps process categorical features with high cardinality. In a dataset where the target is binary, you replace each category with the posterior probability of the target value being positive or 1.

Finally, Posterior encoding is not an encoding technique at the time of this writing.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["6 Ways to Encode Features for Machine Learning Algorithms"](https://towardsdatascience.com/6-ways-to-encode-features-for-machine-learning-algorithms-21593f6238b0) to see some of these encoding techniques in action.
* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.
* [_Feature Engineering for Machine Learning_](https://amzn.to/3SsnLAc) is an excellent book covering feature engineering.</p></details>

-----------------------

## Date - 2022-12-03


## Title - High training and testing


### **Question** :

There's not a lot of context for you other than the following chart showing the training and testing losses of a machine learning model:

![Training and Testing Loss Chart](https://user-images.githubusercontent.com/1126730/188513012-1ec37e8e-8c1d-46b9-bff7-d01bedf269a0.jpg)

As you can see, after finishing training, both losses are high.

**What's a reasonable conclusion about this machine learning model?**


### **Choices** :

- Your model is overfitting.
- Your model is underfitting.
- Your model is either overfitting or underfitting, but you can't tell.
- Your model is working fine.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A good model should capture valuable patterns in the data and discard any noise that doesn't help with predictions. An overfitting model will fit that noise. An underfitting model will not capture the relevant patterns in the dataset. 

An overfitting model should not have any problems with the training data but stumble with the testing data. Therefore, we should expect a low training loss and a high testing loss. An underfitting model should struggle with the training and testing datasets, so both of its losses should be high.

This model shows high training and testing losses, which we expect from an underfitting model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.</p></details>

-----------------------

## Date - 2022-12-04


## Title - Confusion Matrix


### **Question** :

Most people summarize the performance of their model using a single high-level metric. For example, accuracy is a popular way to explain how the model is doing.

While helpful, this doesn't give us enough information about the quality of the predictions and the mistakes the model makes.

A confusion matrix is a tool we can use to zoom into a model and surface important information.

**Which of the following sentences is true about a confusion matrix?**


### **Choices** :

- A confusion matrix helps analyze the performance of a binary classification model, but it doesn't work for multi-class classification models.
- A confusion matrix helps analyze the performance of multi-class classification models, and it doesn't work for binary classification models.
- A confusion matrix helps analyze the performance of classification models, including binary and multi-class models.
- A confusion matrix helps analyze the performance of a regression model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A confusion matrix is one of the simplest and most popular tools to analyze the performance of a classification model. It breaks down each class and the number of correct and incorrect predictions the model makes. It gives us immediate access to the model's errors and their type.

We use confusion matrices with binary and multi-class classification models, but we don't use them in regression problems.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.
* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2022-12-05


## Title - Live audience


### **Question** :

Piper started building a simple neural network from scratch in front of a live audience.

Her talk focused on the fundamental principles of neural networks, and writing the code wasn't too hard.

But there was a problem, and it took Piper an eternity to find and fix it: she forgot to initialize the network weights, so they were all zeroes.

**Which of the following was the clue for Piper to realize there was a problem?**


### **Choices** :

- The network learned, but it took a significantly long time.
- The network learned, but it started overfitting quickly.
- The network's loss started oscillating up and down.
- The network neurons evolved symmetrically, learning the same features.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Correctly initializing a neural network can have a significant impact on convergence.

Initializing the network weights with zeros will lead to every neuron learning the same features. Here is an excerpt from ["Initializing neural networks"](https://www.deeplearning.ai/ai-notes/initialization/) explaining the consequences of using the same weight values:

> Thus, both hidden units will have an identical influence on the cost, leading to identical gradients. Thus, both neurons will evolve symmetrically throughout training, effectively preventing different neurons from learning different things.

Since none of the other choices are possible, this symmetry tipped Piper about the problem.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Initializing neural networks"](https://www.deeplearning.ai/ai-notes/initialization/) is an excellent summary of the importance of weight initialization.
* Check ["Weight Initialization Techniques in Neural Networks"](https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78) to learn about different initialization schemes.</p></details>

-----------------------

## Date - 2022-12-06


## Title - Halting training


### **Question** :

Kim is recording a YouTube video where she wants to explain how early stopping works when training a neural network.

The fundamental insight she wants her audience to understand is how the technique decides the exact moment it needs to halt the training process.

**Assuming that Kim will use the validation loss as the metric to watch, which of the following is the correct way to configure early stopping?**


### **Choices** :

- The training process should stop once the validation loss has increased for several consecutive iterations.
- The training process should stop once the validation loss has decreased for several successive iterations.
- The training process should stop once the validation loss hits its maximum value.
- The training process should stop once the validation loss decreases from its immediate previous value.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Kim is using the validation loss of the model to explain how [early stopping](https://articles.bnomial.com/early-stopping) works. When training a model, its validation loss will decrease as the model learns, but it will increase as soon as it starts overfitting. The goal when using early stopping is to catch this reversal and halt the training process.

The second and third choices argue about the trigger happening when the validation loss decreases, but that's the opposite of what we want. While the validation loss drops, everything is fine, and we should let the model train. When the validation loss hits its maximum value, it's too late to stop the training process, so the second choice is also incorrect. 

The correct answer is the first choice: early stopping should halt the training process as soon as the validation loss has increased for several consecutive iterations.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check ["Early stopping"](https://articles.bnomial.com/early-stopping) for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models.</p></details>

-----------------------

## Date - 2022-12-07


## Title - Amazon's warehouse


### **Question** :

Faith works in an Amazon warehouse. 

Every day, thousands of packages leave the facility to their destination, and Faith has been working on a few machine learning models to predict workload over time.

When building one of the models, she found out that her dataset had too many categorical features. Faith doesn't want to deal with a sparse dataset, so she needs your help to select the appropriate way to encode those columns.

**How can Faith encode the categorical columns and avoid getting a sparse dataset?**


### **Choices** :

- Faith should use One-Hot encoding.
- Faith should use Target encoding.
- Faith should leave the categorical features as they are.
- Faith should remove every categorical feature from the dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Faith cannot leave the features as they are because many different machine learning models can't process categorical features directly. She can't remove them because she will lose all their predictive capabilities.

Faith has to encode these features.

One-Hot encoding creates a new feature for each unique value of the original categorical variable. For example, a "weather" feature with three values will get Faith three new features, one for each value of the original "weather" column. Unfortunately, using One-Hot encoding with many categorical features will result in a sparse dataset.

Target encoding is another technique that helps process categorical features with high cardinality. Target encoding replaces the categories of a column with the average target value of all data points belonging to that category. In other words, Faith will convert each categorical feature into a numerical one and won't have to deal with a sparse dataset.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["6 Ways to Encode Features for Machine Learning Algorithms"](https://towardsdatascience.com/6-ways-to-encode-features-for-machine-learning-algorithms-21593f6238b0) to see some of these encoding techniques in action.
* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.
* [_Feature Engineering for Machine Learning_](https://amzn.to/3SsnLAc) is an excellent book covering feature engineering.</p></details>

-----------------------

## Date - 2022-12-08


## Title - Choosing a project


### **Question** :

Elsie wants to practice what she studied about Recurrent Neural Networks (RNN).

She is a practical person, so she wants to choose a project where she can use an RNN. There are several options.

**Which of the following projects do you think Elsie could choose to practice what she learned about RNNs?**


### **Choices** :

- Predict the sales of her company's main product.
- Determining the safety of attachments sent by customers in their email correspondence.
- Transcribing handwritten correspondence from customers.
- Transcribing voicemails from customers.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A [recurrent neural network](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNN) is an artificial neural network that uses sequential or time series data. Their main difference from traditional networks is their ability to take information from prior inputs to influence the current input and output.

Because of the nature of written text and speech, Elsie should pick any of the transcription problems. Handwritten text and speech are sequential, making them ideal for an RNN. RNN models are heavily used in natural language processing and speech recognition. 

Predicting sales and classifying email attachments don't seem like a good application of RNNs.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Recurrent Neural Networks"](https://www.ibm.com/cloud/learn/recurrent-neural-networks) for a description of what they are and how they work.
* ["An Introduction To Recurrent Neural Networks And The Math That Powers Them"](https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/) is a deeper dive into RNNs.</p></details>

-----------------------

## Date - 2022-12-09


## Title - Trigger metric


### **Question** :

Rhea is building a multi-class classification neural network and doesn't have too much time to regularize it.

Instead, she wants to keep her model as unconstrained as possible and use early stopping as her regularization technique. She knows early stopping is simple to implement and very effective.

Rhea needs to decide how to configure it correctly. Specifically, she must decide which metric to watch to trigger early stopping.

**From the list below, what options could Rhea consider to configure early stopping for her model?**


### **Choices** :

- Use the training loss as the metric to watch.
- Use the accuracy on the training set as the metric to watch.
- Use the validation loss as the metric to watch.
- Use the accuracy on the validation set as the metric to watch.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Early stopping](https://articles.bnomial.com/early-stopping) halts the training process as soon as overfitting starts. It watches a pre-configured metric to determine the appropriate time.

Rhea shouldn't use the training set to decide whether her model is overfitting. As she trains for longer, the loss and accuracy on her training set will move continuously in the right direction. Instead, measuring the model's performance on a separate, hold-out validation set is the right approach.

Rhea could use the validation loss to halt the training process. When training a model, its validation loss will decrease as the model learns, but it will increase as soon as it starts overfitting. The model's accuracy could also work as a trigger because it will increase while the model learns but decrease as quickly as it begins overfitting.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check ["Early stopping"](https://articles.bnomial.com/early-stopping) for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models.</p></details>

-----------------------

## Date - 2022-12-10


## Title - Recall is more important


### **Question** :

Here is the Fβ score formula:

![FBeta-Score](https://user-images.githubusercontent.com/1126730/193036770-0a37f6f7-9a5d-4be3-9b89-7018931140eb.jpg)

The Fβ score lets us combine precision and recall into a single metric.

Let's say you want to use this formula to measure a model where a higher recall is more important.

**What's the correct value for the β parameter to achieve this?**


### **Choices** :

- β = 0
- β = 0.5
- β = 1
- β = 2


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The Fβ score lets us combine precision and recall into a single metric. When using β = 1, we place equal weight on precision and recall. For values of β > 1, recall is weighted higher than precision; for values of β < 1, precision is weighted higher than recall.

Therefore, the correct answer to this question is β = 2.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What is the F-Score?"](https://deepai.org/machine-learning-glossary-and-terms/f-score) is a short introduction to this metric.
* ["Micro, Macro & Weighted Averages of F1 Score, Clearly Explained"](https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f) is a great article covering how to compute a global F1-Score metric in multi-class classification problem.
* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2022-12-11


## Title - Bag of chips


### **Question** :

Amira brought a bag to the event. She wanted people to relax before a long day, so she thought of lighting the mood with a fun exercise.

"There are 15 red chips, 25 blue chips, and only 5 black chips in this bag,"—she told the audience. "If I put my hand inside and pick a chip randomly, what's the probability it's neither red nor black?"

**Select the correct probability from the following options:**


### **Choices** :

- The probability is 3/9
- The probability is 3/25
- The probability is 5/8
- The probability is 5/9


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Amira wants to compute the probability of picking a blue chip since she doesn't want it to be red or black.

There are 45 chips in the bag, 25 of which are blue. Therefore, the probability of picking a blue chip is 25/45, which simplifies down to 5/9.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Cartoon Guide to Statistics_](https://amzn.to/3eg9iIo) is a fun and instructive introduction to probabilities and statistics.</p></details>

-----------------------

## Date - 2022-12-12


## Title - KNN's summary


### **Question** :

Kaia has never worked with k-Nearest Neighbors (KNN) before, but that was her teammates' suggestion. 

She decided to do some research and look into how KNN works.

**Here is the summary she put together. Which of the following are correct statements about KNN?**


### **Choices** :

- KNN is considered an unsupervised learning algorithm.
- KNN stores the entire dataset in memory to make a prediction.
- KNN does not create a predictive model during training time.
- KNN makes predictions on the fly by calculating the similarity between a sample and the observations in the dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>k-Nearest Neighbors (KNN) is a supervised learning algorithm that doesn't create a predictive model from a training dataset to make predictions. In KNN, there's no need for a training phase. Instead, the algorithm computes a prediction during inference time.

KNN uses the entire dataset and looks for a pre-determined number of instances closest to the observation we want to classify to determine to which group the sample belongs. To do this, KNN stores the entire dataset in memory and computes new predictions on the fly.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Understand the Fundamentals of the K-Nearest Neighbors (KNN) Algorithm"](https://heartbeat.comet.ml/understand-the-fundamentals-of-the-k-nearest-neighbors-knn-algorithm-533dc0c2f45a) for an introduction to KNN.</p></details>

-----------------------

## Date - 2022-12-13


## Title - Underlying function


### **Question** :

Harlow has access to a simple dataset with only one numerical feature and a target value.

She wants to build a couple of models to predict the target value using her dataset. She will try two different functions:

* Model A: `y = mx + b`
* Model B: `y = x² + mx + b`

Harlow split her dataset in two: 80% for training both models and 20% for testing them.

**Which model will get the best results on Harlow's testing set?**


### **Choices** :

- Model A will have the best results on the testing set.
- Model B will have the best results on the testing set.
- Both Model A and Model B will have similar results on the testing set.
- We need more information to decide which model will do better.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We don't know anything about the dataset, so we can't decide which model will do better.

For example, if the data comes from a linear model, Model A may have an advantage, while Model B will be better if the data comes from a quadratic function.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Linear Regression for Machine Learning"](https://machinelearningmastery.com/linear-regression-for-machine-learning/) is an introduction to Linear Regression.</p></details>

-----------------------

## Date - 2022-12-14


## Title - Function differences


### **Question** :

Angela needs an activation function for her neural network and wants to decide between Sigmoid and Tanh.

She knows the functions are different but needs a reminder about their specific differences.

**Which of the following are correct statements when comparing Sigmoid and Tanh?**


### **Choices** :

- Sigmoid is an s-shaped-looking function. Tanh is not.
- Around the value 0, Sigmoid has a larger gradient than Tanh.
- Around the value 0, Tanh has a larger gradient than Sigmoid.
- Tanh produces values centered around 0. Sigmoid's output is always positive.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The Sigmoid function takes an input "x" and squeezes it between 0 and 1. When plotted, Sigmoid is an s-shape curve:

![Sigmoid Plot](https://user-images.githubusercontent.com/1126730/197406367-7a06343e-f5ed-4409-b11d-6ee92eef7171.jpg)

The Tanh function also has a similar-looking s-shape. The function squeezes the input "x" between -1 and 1. Notice how Tanh looks like a stretched and shifted version of Sigmoid:

![Tanh plot](https://user-images.githubusercontent.com/1126730/197406179-9f80cd61-2241-489e-98a7-bd63bf74beba.jpg)

When training neural networks, we use the derivative of the activation function. If we compute the gradient of Sigmoid and Tanh and plot them, we will see the following:

![Gradients](https://user-images.githubusercontent.com/1126730/198387224-79c11611-f666-4df2-9b58-c661fa5900e8.jpg)

Notice how the gradient of Tanh around 0 is around four times larger than the gradient of Sigmoid.

Finally, Tanh produces values centered around 0—between -1 and 1—, while Sigmoid's output is always positive—between 0 and 1.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Activation Functions: Sigmoid vs. Tanh"](https://www.baeldung.com/cs/sigmoid-vs-tanh-functions) for a comparison between these two functions.
* For a complete description of the Sigmoid function, check the ["Logistic function"](https://en.wikipedia.org/wiki/Logistic_function) Wikipedia page.
* For a complete description of the Tanh function, check the ["Hyperbolic Functions"](https://en.wikipedia.org/wiki/Hyperbolic_functions) Wikipedia page.</p></details>

-----------------------

## Date - 2022-12-15


## Title - Counting leaf nodes


### **Question** :

Myla built a decision tree that counts the number of features in a dataset with the value 1.

The dataset only contains binary features that are either zero or one. Myla's decision tree works with any number of features.

**Assuming a dataset with 8 binary features, how many leaf nodes would Myla's decision tree have?**


### **Choices** :

- Her decision tree needs 8 leaf nodes.
- Her decision tree needs 16 leaf nodes.
- Her decision tree needs 64 leaf nodes.
- Her decision tree needs 256 leaf nodes.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Myla's decision tree will need 256 leaf nodes to work with a dataset of 8 features. As a general rule, for a dataset with _d_ features, Myla will require a decision tree with 2ᵈ leaf nodes.

As an example, imagine we have a dataset with a single feature. The decision tree will need 2 leaf nodes: one to capture the case where the feature has a value of 0 and another to capture when the feature has a value of 1:

![Decision tree with one feature](https://user-images.githubusercontent.com/1126730/197406448-5dda78a5-0011-4525-aa6b-084ed46805a4.jpg)

For 2 features, the possible combinations are four: both features are 0, the first feature is 0 and the second is 1, the first feature is 1 and the second is 0, and both features are 1. The possible resultant count is 0, 1, or 2:

![Decision tree with two features](https://user-images.githubusercontent.com/1126730/197406460-edb7b72f-246f-4c53-8598-d5da4fd7c01c.jpg)

Generally, for _d_ features, the decision tree will need 2ᵈ leaf nodes.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Decision tree learning"](https://en.wikipedia.org/wiki/Decision_tree_learning) is the Wikipedia page where you can read about decision trees.
- ["How to tune a Decision Tree?"](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680) is a great article talking about different ways to tune a decision tree, including the effects of setting the maximum depth hyperparameter.</p></details>

-----------------------

## Date - 2022-12-16


## Title - Hard to read


### **Question** :

Zara tried to read Stable Diffusion's paper but realized there were too many terms that she didn't understand.

Early on, she stumbled upon the idea of a "latent space." Zara had never heard about that before, so she decided to do some research.

By the end of the day, Zara had a good idea of what was a latent space and wrote a summary with her understanding.

**Which of the following statements are correct regarding the concept of a latent space as we use it in machine learning?**


### **Choices** :

- We can find similar items nearby in latent space.
- Latent spaces are always linear, which makes them easier to interpret.
- The distance between objects in latent space lacks physical units, so every application needs to interpret these values.
- Usually, the data points we project in the latent space have higher dimensionality than the space itself.


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In machine learning, we use "latent space" to refer to a multi-dimensional space containing a meaningful internal representation of objects and where similar points appear closer together. 

It's common for a latent space to be high-dimensional and nonlinear. There are no predefined units to measure the distance between data points in latent space, so interpreting the contents of a latent space is an application-specific and complex task. 

Finally, latent spaces usually have a lower dimensionality than the feature space used to draw specific data points. Because of this, projecting an object in a latent space is typically a dimensionality reduction exercise.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check the [Latent space's Wikipedia page](https://en.wikipedia.org/wiki/Latent_space) for more information.</p></details>

-----------------------

## Date - 2022-12-17


## Title - Leadership meeting


### **Question** :

Annie received the email from her data science manager early in the morning. She needs to show up to the leadership meeting with the results of the latest iteration of their model but trying to decipher technical jargon was never her strong suit.

Her manager sent her a picture of the confusion matrix the team created after validating the model:

![Confusion matrix](https://user-images.githubusercontent.com/1126730/186957993-9bf7a320-0c86-455e-bd42-6a67777beaee.jpg)

The sum of every element in the confusion matrix is 190, but Annie doesn't have much experience here.

**What is the sum of the elements in a confusion matrix?**


### **Choices** :

- The sum of the elements in a confusion matrix always equals the number of correct predictions the model made on the validation set.
- The sum of the elements in a confusion matrix always equals the number of mistakes the model made on the validation set.
- The sum of the elements in a confusion matrix always equals the number of samples of the training set.
- The sum of the elements in a confusion matrix always equals the number of samples of the validation set.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A [confusion matrix](https://articles.bnomial.com/confusion-matrix) is one of the simplest and most popular tools to analyze the performance of a classification model. It breaks down each class and the number of correct and incorrect predictions the model makes. It gives us immediate access to the model's errors and their type.

The sum of every element in a confusion matrix gives us the number of samples in the dataset we used to create the matrix. Since the team built it during the validation process, we can assume it represents the validation set.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.
* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.</p></details>

-----------------------

## Date - 2022-12-18


## Title - Too many features


### **Question** :

Rowan's team set up a plan to build a new model. 

Since they had too many features, the first step was taking the entire dataset and performing feature selection. Then, they split the dataset in two: 80% for training the model and 20% for testing it. 

Finally, the team trained a Decision Tree on the training set and used the test data to find the best hyperparameter values for the model.

**Which of the following statements are true regarding this situation?**


### **Choices** :

- The team's approach should work as expected.
- The way the team performed feature selection is problematic.
- The way the team split the dataset is problematic.
- The way the team tuned the model's hyperparameters is problematic.


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>There are two problems with the team's approach.

First, they used the entire dataset for feature selection. They should have split the dataset first and performed feature selection on the training and testing data separately. When doing feature selection on the entire dataset, the team risks leaking information from the soon-to-be test data into the model. A leak will lead to a model that performs too well on the existing data but will do poorly on future unseen data.

The second problem is how the team tuned the model's hyperparameters. They used the test set to choose the best hyperparameters, which could lead to a model that overfits to the testing dataset. In other words, this model will have trouble with unseen data because the team optimized it to work well on the test dataset. Instead, the team should have used a validation set for hyperparameter tuning.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check [Data Leakage in Machine Learning](https://machinelearningmastery.com/data-leakage-machine-learning/) for an introduction to data leaks and how to prevent them. 
* ["Train, Validation, Test Split for Machine Learning"](https://blog.roboflow.com/train-test-split/) goes into detail about the importance of each split.</p></details>

-----------------------

## Date - 2022-12-19


## Title - Too many networks


### **Question** :

Why would Konrad want to look into recurrent neural networks?

He is always trying to minimize the amount of work he does. He is primarily a just-in-time person when it comes to learning new things.

And now that he started with deep learning, there are too many different types of neural networks, and he's getting overwhelmed.

**Let's help Konrad by selecting every correct statement regarding the differences between recurrent and traditional neural networks.**


### **Choices** :

- A recurrent neural network can process inputs of any length, unlike traditional networks that require a fixed-size input.
- A recurrent neural network has access to the information it processed at any time in the past, while traditional networks can't access any historical data.
- The model size of a recurrent neural network doesn't increase with the size of its input, unlike a traditional network where a larger input will require a bigger model.
- A recurrent neural network captures the sequential information present in the input data. Traditional neural networks don't have this ability.


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNN) are a type of artificial neural network that can process sequential or time series data. Their main difference from traditional networks is their ability to take information from prior inputs to influence the current input and output. This ability allows them to capture any sequential information present in the data. For example, an RNN is ideal for capturing the dependency between words of a sentence.

RNN processes the data sequentially so a model can process sequences of varying sizes. For example, an RNN can process a 5-word and 10-word sentence using the same input structure, unlike a traditional neural network that will need a different input size for each case. As the size of the input increases, a conventional network will need to accommodate it with a larger model size, while the size of an RNN will stay consistent.

The only incorrect option is the second: while an RNN can access historical information, they have difficulty accessing data from long ago. Their limited ability to process historical information is one of the most significant disadvantages of an RNN.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Recurrent Neural Networks"](https://www.ibm.com/cloud/learn/recurrent-neural-networks) for a description of what they are and how they work.
* ["An Introduction To Recurrent Neural Networks And The Math That Powers Them"](https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/) is a deeper dive into RNNs.</p></details>

-----------------------

## Date - 2022-12-20


## Title - Chest X-Rays


### **Question** :

In 2017, Andrew Ng's team published a paper on Deep Learning for pneumonia detection on chest X-Rays.

They used a dataset with 112,120 images belonging to 30,805 unique patients. They automatically labeled every sample with 14 different pathologies and randomly split the dataset into 80% training and 20% validation. Their process downscaled images to 224x224 pixels before inputting them into a neural network.

After publishing the paper and listening to the community's feedback, they had to redo their experiments.

**What do you think was wrong with their experiment?**


### **Choices** :

- A random split would get pictures from the same patient in training and the validation sets leading to leakage.
- Using too many images in their training dataset is too slow and wouldn't yield better results.
- Their dataset needed to be bigger to solve a problem with 14 classes.
- Downscaling X-Ray pictures to 224x224 is too aggressive and would destroy relevant image information.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The samples in the team's dataset are not independent. Different X-Ray images from the same patient will have similarities that a neural network could use to make a prediction.

For example, a patient might have a scar from a previous surgery or a specific bone density or structure. These clues will help the model make a prediction, so having X-Rays from the same patient in the training and validation sets will create a leaky validation strategy. Here is an excerpt from [The Kaggle Book](https://amzn.to/3kbanRb):

> In a leaky validation strategy, the problem is that you have arranged your validation strategy in a way that favors better validation scores because some information leaks from the training data.

The team fixed the experiment in the third version of their paper. Here is what they did:

> For the pneumonia detection task, we randomly split the dataset into training (28744 patients, 98637 images), validation (1672 patients, 6351 images), and test (389 patients, 420 images). There is no patient overlap between the sets.

Notice how they ensured that there was no overlap between sets.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.
* ["Target Leakage in Machine Learning"](https://www.youtube.com/watch?v=dWhdWxgt5SU) is a YouTube presentation that covers leakage, including during the partitioning of a dataset.
* Original paper showing the leaky validation strategy: ["CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning."](https://arxiv.org/pdf/1711.05225v1.pdf). The [third version](https://arxiv.org/pdf/1711.05225.pdf) of the paper fixes the problem.</p></details>

-----------------------

## Date - 2022-12-21


## Title - Decreasing lambda


### **Question** :

Ariel inherited a Linear Regression model. She didn't build the first version, so she isn't sure everything is working correctly.

Although the validation error looks fine, Ariel wants to experiment to determine whether every individual piece makes sense. She wants to start by looking into the lambda (λ) parameter that controls the model's regularization.

**Which of the following will eventually happen as Ariel continuously decreases the value of λ?**


### **Choices** :

- Ariel's model will eventually underfit, and the validation error will increase.
- Ariel's model will eventually overfit, and the validation error will increase.
- Ariel's model will eventually underfit, and the validation error will decrease.
- Ariel's model will eventually overfit, and the validation error will decrease.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Ariel can control the model's regularization using the lambda (λ) parameter. She can tune this parameter to decide how much she wants to penalize the model's flexibility. 

Decreasing the value of λ will increase the value of the coefficients, thus increasing the variance of the model. As this variance increases, the model will eventually overfit, leading to a higher validation error.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Regularization in Machine Learning"](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a) for a detailed explanation of how regularization works.
* ["Fighting Overfitting With L1 or L2 Regularization: Which One Is Better?"](https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization) will give you a complete introduction to L1 and L2 regularization.</p></details>

-----------------------

## Date - 2022-12-22


## Title - Restaurant trip


### **Question** :

Sloane took her friends to a restaurant.

They ordered two dozen raw oysters. Sloane, Elliana, and Yasmin had five oysters each, while Ana ate the other 9.

**Which of the following represents the mean of the oysters they ate?**


### **Choices** :

- The mean is 2.
- The mean is 4.
- The mean is 5.
- The mean is 6.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The mean, median, and mode are different ways to measure the center of a dataset. Each of these metrics tries to summarize a set of values using a single number.

The mean of a dataset represents its average. We can find it by adding all data points and dividing by the number of values in the set.

This example has four data points: `5`, `5`, `5`, and `9`. Adding these values and dividing the result by `4` gives us a mean of `6`.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Mean Median Mode: What They Are, How to Find Them"](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/) for a complete explanation of the Mean, Median, and Mode.</p></details>

-----------------------

## Date - 2022-12-23


## Title - Incompatible shapes


### **Question** :

Juliet started by creating a fake dataset. She used Scikit-Learn's `make_blobs()` function:

```
X, y = make_blobs(
    n_samples=1000, 
    centers=3, 
    n_features=2
)
```

One thousand samples were enough data points for Juliet to experiment with neural networks. She used Keras to create a simple model and compiled it as follows:

```
model.compile(
    optimizer=SGD(learning_rate=0.01), 
    loss="categorical_crossentropy", 
    metrics=["accuracy"]
 )
```

But when she tried to fit the model using 32 samples as the `batch_size`, she got the following error: 

> Shapes (32, 1) and (32, 3) are incompatible.

**Which of the following should solve Juliet's problem?**


### **Choices** :

- Juliet should not use 32 samples as her batch size. Instead, she should use a smaller value.
- Juliet should use the value 3 for the `n_features` parameter when calling the `make_blobs()` function.
- Juliet should use `sparse_categorical_crossentropy` as her loss function.
- Juliet should one-hot encode the labels returned by the `make_blobs()` function.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Keras' `SparseCategoricalCrossentropy` computes the cross-entropy loss between the labels and predictions. It works when the labels in the dataset are integer values, for example, 1, 2, and 3.

Keras' `CategoricalCrossentropy`, on the other hand, has the same function but works when the labels in the dataset are one-hot encoded, for example, `[1, 0, 0]`, `[0, 1, 0]`, and `[0, 0, 1]`.

Juliet is using `make_blobs()` to create a dataset, and this function returns integer values to enumerate the different clusters. Therefore, Juliet should one-hot encode these values, or she needs to replace the loss function with `sparse_categorical_crossentropy`. Either option works.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Here is Keras' [SparseCategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) and [CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) documentation.
* And here is [Scikit-Learn's make_blobs()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) function documentation.</p></details>

-----------------------

## Date - 2022-12-24


## Title - The clinical trial


### **Question** :

With more than five years of experience, Alani was an expert in dealing with tabular data. 

In her last consulting gig, the company gave her access to a large dataset with patients' information from a clinical trial. Alani needs to predict which patients are more likely to drop off, and she decided to use XGBoost to create her model.

There's only one problem. The dataset has a few columns with missing values. Not enough for Alani to drop the columns, but she still needs to decide what to do.

**Which of the following should be the best way to deal with the missing values?**


### **Choices** :

- Alani should replace every missing value with the column's mean.
- Alani should replace every missing value with the column's median.
- Alani should replace every missing value with the column's mode.
- Alani should keep the missing values untouched.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Fortunately for Alani, she uses XGBoost, an algorithm that supports missing values and has a clever way to deal with them.

In ["XGBoost: A Scalable Tree Boosting System,"](https://arxiv.org/pdf/1603.02754v3.pdf) the authors explain the algorithm's process to handle missing values. They call it "Sparsity-aware Split Finding":

> (...) we propose to add a default direction in each tree node (...). When a value is missing in the sparse matrix x, the instance is classified into the default direction. There are two choices of default direction in each branch. The optimal default directions are learnt from the data.

In other words, XGBoost learns how to classify samples with missing values by learning the best possible replacement for those values. This approach is better than replacing the value with an arbitrary choice like the feature's mean, median, or mode. Unless Alani knows something specific about those features, she should keep the missing values and let the algorithm deal with them.

Although XGBoost has a clever approach for missing values, it's still generic and won't be better than a specific strategy that tackles each feature individually. A complete understanding of the problem and using that information to modify the data is always the best strategy.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["XGBoost: A Scalable Tree Boosting System"](https://arxiv.org/pdf/1603.02754v3.pdf) is the XGBoost paper that explains the "Sparsity-aware Split Finding" approach.</p></details>

-----------------------

## Date - 2022-12-25


## Title - Muscle memory


### **Question** :

Brooke is working on a machine learning multi-class classification neural network and used a softmax activation function on the output layer.

But Brooke did it because her muscle memory kicked in. She isn't sure why softmax is the right way to go.

**Which of the following statements is true about the softmax activation function when used in the output layer of a neural network?**


### **Choices** :

- The softmax function turns the network's input into a vector of probabilities that sum to 1.
- The softmax function turns a vector of real values into a sorted vector of probabilities that sum to 1.
- The softmax function turns a vector of real values into a vector of probabilities that sum to 1.
- The softmax function turns the network's input into a vector of probabilities that sum to 0.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The softmax function turns a vector of real values into another vector of probabilities that sum to 1. These probabilities are proportional to the relative scale of each value in the vector.

When used as the activation function of the output layer of a neural network, softmax converts the scores from the previous layer to a normalized probability distribution. This is a convenient way to interpret the results of a multi-class classification model.

Noticed that the first and fourth choices argue about softmax converting the network's input. This would only be true if the network doesn't have hidden layers, and the input layer is connected directly to the output. Moreover, the fourth choice claims that the sum of the vector values will be zero, which is incorrect.

Finally, the second choice argues about a "sorted vector of probabilities." Softmax doesn't sort the output vector, so this option is also incorrect.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check ["Softmax Activation Function with Python"](https://machinelearningmastery.com/softmax-activation-function-with-python/) for more information about the Softmax function and a way to implement it.
- ["What is the Softmax Function"](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer) is a great summary of this function.</p></details>

-----------------------

## Date - 2022-12-26


## Title - First chapter


### **Question** :

Adelaide was about to finish the first chapter of her book. It was about neural networks, and Adelaide wanted to write a few exercises before closing it.

She found that an excellent way to summarize the chapter was with a multi-choice question:

**Which of the following sentences are true about neural networks?**


### **Choices** :

- We can only optimize neural networks using the Gradient Descent algorithm.
- Neural networks can only find the optimal solution for convex problems.
- Neural networks can use a mix of activation functions.
- Neural networks can approximate any function when using non-linear activations.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Gradient descent is an iterative optimization algorithm used to find the local minimum of a function. The algorithm works by taking steps proportional to the negative of the function's gradient at the current point. Gradient descent is an excellent choice to optimize neural networks, but it's not the only way. We can use, for example, the [Adam algorithm](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning), a combination of the AdaGrad and RMSProp algorithms.

Using Gradient descent, we can find the optimal solution for a concave problem, but it's not guaranteed. The algorithm might converge to a local minimum instead of the global minimum.

When setting up a neural network, we can use a mix of activation functions. For example, ReLU in the hidden layers and Softmax in the output layer of a classification model.

Finally, thanks to the [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem), we can turn a two-layer neural network into a universal function approximator when using non-linear activation functions.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Gradient Descent For Machine Learning"](https://machinelearningmastery.com/gradient-descent-for-machine-learning/) for a description of the algorithm.
* Check the ["Universal approximation theorem"](https://en.wikipedia.org/wiki/Universal_approximation_theorem) on Wikipedia for more information about the power of neural networks.
* ["Gentle Introduction to the Adam Optimization Algorithm for Deep Learning"](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning) is an excellent explanation of the Adam algorithm.</p></details>

-----------------------

## Date - 2022-12-27


## Title - Using log loss


### **Question** :

Wren learned about the log loss for the first time.

Log loss, also known as cross-entropy loss, indicates how close a prediction probability comes to the target value.

Wren wants to use what she learned right away.

**Which of the following problems is a good candidate for Wren to practice the log loss?**


### **Choices** :

- Predict the house price given a dataset with the number of bedrooms of every house in the neighborhood.
- Determine a person's location in an image, given a dataset of pictures from a security camera.
- Determine whether tomorrow will rain, given a dataset with the weather patterns over the last few years.
- Group customers from a store depending on their buying patterns.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Log loss is a function that we commonly use in classification problems. It returns the negative logarithm of the product of probabilities.

Log loss indicates how close a prediction probability is to the target value. The more the predicted probability differs from the target value, the higher the log loss.

From the list of options, the only classification problem is determining whether it will rain on any given day: it's a binary choice. The other three options are regression, object detection, and clustering.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Understanding binary cross-entropy / log loss: A visual explanation"](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) is an excellent introduction to binary cross-entropy.
- Another article that helps understand binary cross-entropy is ["Binary Cross Entropy/Log Loss for Binary Classification"](https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/).</p></details>

-----------------------

## Date - 2022-12-28


## Title - Sigmoid's interpretation


### **Question** :

Lennon is starting to understand the need for activation functions when building neural networks.

She is still relatively new and doesn't grasp the details of some of the most popular activation functions, especially because some formulas aren't straightforward to understand.

For example, here is the Sigmoid formula:

![Sigmoid](https://user-images.githubusercontent.com/1126730/196755518-311dd425-676e-4c85-be1f-467694879c30.jpg)

**Which of the following is a correct interpretation of this function?**


### **Choices** :

- The output of the Sigmoid function could be any real number.
- The output of the Sigmoid function could be any integer number.
- The output of the Sigmoid function is a real number between `-1` and `1`.
- The output of the Sigmoid function is a real number between `0` and `1`.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The Sigmoid function takes a value as input and outputs another real value between `0` and `1`. We say that Sigmoid "squeezes" the input into that range.

Sigmoid is continuously differentiable, and its derivative is simple to compute. This, together with its fixed output range, make the Sigmoid function one of the most popular activation functions.

Here is the plot of the Sigmoid function:

![Sigmoid Plot](https://user-images.githubusercontent.com/1126730/197406367-7a06343e-f5ed-4409-b11d-6ee92eef7171.jpg)</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For a complete description of the Sigmoid function, check the ["Logistic function"](https://en.wikipedia.org/wiki/Logistic_function) Wikipedia page.</p></details>

-----------------------

## Date - 2022-12-29


## Title - Throwing the towel


### **Question** :

Delaney was ready to throw the towel.

She's been paying careful attention to her machine learning professor. After they reviewed gradient descent in detail, Delaney knew something important: Gradient descent moves downhill, so she should expect the loss to decrease over time.

But here she is, looking at a training loss that goes up in several places. The first neural network she trains, and she can't shake the feeling she is doing something wrong:

![Learning Curves](https://user-images.githubusercontent.com/1126730/192155439-4a4a9f25-ed44-4b72-994b-59a6f2743fd5.jpg)

**How can Delaney fix this problem?**


### **Choices** :

- Delaney should decrease the batch size. That should stop the loss from increasing during training.
- Delaney should increase the batch size. That should stop the loss from increasing during training.
- Delaney should use Early Stopping to stop the process before the loss increases.
- Delaney shouldn't do anything because her network has no problem.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Gradient descent moves downhill on average, so a network that learns appropriately should see the loss decrease over the training session. However, individual updates can move in the opposite direction, causing the loss to fluctuate up and down.

Delaney's plot shows the loss increasing a few times momentarily, but it immediately starts decreasing. That's normal, and Delaney shouldn't worry about it.

Delaney can modify the batch size to improve her results, but just by looking at the information from this question, there's nothing to fix. If she uses [Early Stopping](https://articles.bnomial.com/early-stopping), she will prevent the network from improving further.

In summary, Delaney shouldn't do anything at this point.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["An overview of gradient descent optimization algorithms"](https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants) for a deep dive into gradient descent and every one of its variants.
* Check ["Early Stopping"](https://articles.bnomial.com/early-stopping) for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models.</p></details>

-----------------------

## Date - 2022-12-30


## Title - Just in time


### **Question** :

Lila had to turn in an implementation of a machine-learning technique to solve a toy exercise. She had to write the code from scratch.

She had a simple dataset: only a few features and rows of data. 

Lila decided to implement a solution that wouldn't require training and would classify new samples just in time.

**Which of the following was the technique that Lila used?**


### **Choices** :

- Lila used Linear Regression
- Lila used a Decision Tree
- Lila used k-Nearest Neighbors
- Lila didn't use any of the above.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Unlike Linear Regression and Decision Trees, k-Nearest Neighbors (KNN) is an algorithm that doesn't create a predictive model from a training dataset to make predictions. In KNN, there's no need for a training phase. Instead, the algorithm computes a prediction during inference time.

KNN uses the entire dataset and looks for a pre-determined number of instances closest to the observation we want to classify to determine to which group the sample belongs.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Understand the Fundamentals of the K-Nearest Neighbors (KNN) Algorithm"](https://heartbeat.comet.ml/understand-the-fundamentals-of-the-k-nearest-neighbors-knn-algorithm-533dc0c2f45a) for an introduction to KNN.</p></details>

-----------------------

## Date - 2022-12-31


## Title - The model's accuracy


### **Question** :

A team built a binary classification model. They named the classes `A` and `B`.

After finishing training, they evaluated the model on a validation set, and here is the confusion matrix with the results:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186248358-1b39a042-5725-408b-84aa-8dd915a6d99d.jpg)

**Given the above confusion matrix, what is the accuracy of this binary classification model?**


### **Choices** :

- The accuracy of the model is 28%.
- The accuracy of the model is 52%.
- The accuracy of the model is 80%.
- The accuracy of the model is 88%.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To compute the model's accuracy, we must divide the number of correct predictions by the number of total predictions.

In this example, the model made 100 predictions on the validation set. 80 of those predictions were correct: 52 corresponding to class `A`, and 28 to class `B`. Therefore, the accuracy of the model is 80%.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.
* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.</p></details>

-----------------------

## Date - 2023-01-01


## Title - Class notes


### **Question** :

It's been almost eight years since Dakota's first machine learning class, and she decided to dust her old notes and share them online.

Surprisingly, while reviewing one of her introductory classes, she found one mistake in her description of Linear Regression.

**Below, you have four bullets that Dakota wrote. Which of them is not correct?**


### **Choices** :

- Linear Regression is a Supervised Learning technique useful for solving Regression problems.
- We use Linear Regression for predicting a categorical dependent variable with the help of independent variables.
- Linear Regression aims to find the best line that can accurately predict the output for the continuous dependent variable.
- In Linear Regression, the relationship between the dependent and independent variables must be linear.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Linear Regression is probably the most popular Supervised Learning technique in machine learning. Its goal is to fit the best line through the data to predict a continuous output. 

The algorithm uses a set of independent variables to predict a continuous dependent variable. For example, a person's age, salary, or home price. 

Finally, for Linear Regression to work, we must ensure that the relationship between the inputs and the output is linear. A Linear Regression model won't give us good predictions if the relationship isn't linear. Sometimes, this condition means we must transform the input features before using Linear Regression. For example, if you have a variable with an exponential relationship with the target variable, you can use log transform to turn the relationship linear. 

Therefore, Dakota's second bullet point is the only one that's not correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Linear Regression for Machine Learning"](https://machinelearningmastery.com/linear-regression-for-machine-learning/) is an introduction to Linear Regression.</p></details>

-----------------------

## Date - 2023-01-02


## Title - Line 10 of the network


### **Question** :

Here is a simple and compact implementation of a neural network in Python:

![Neural network](https://user-images.githubusercontent.com/1126730/196750241-0a53d7bf-d821-43e9-bf5c-560e6ca842d4.png)

Line 10 computes the update we will add to the weights connecting the hidden and output layers.

**Which of the following explains what happens on that line?**


### **Choices** :

- The code computes the update by multiplying the derivative of the error by the derivative of the network's output.
- The code calculates the update by multiplying the derivative of the output by the target value.
- The code computes the update by multiplying the error by the network's output.
- There's an error in how the code calculates the update.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We need to update the weights of the network during the backpropagation process. We start from the final layer of the network and move sequentially to the beginning.

We can apply the chain rule to calculate the update to the second set of weights: The multiplication between the derivative of the error and the output's layer derivative.

The error represents how far the output is from the target values. In this example, the error is the square of the difference between `layer2` and the target values `y`. The derivative of the error is `2 * (layer2 - y)`.

Notice that the output layer (`layer2`) uses a sigmoid function; hence its derivative is `layer2 * (1 - layer2)`.

Therefore, Line 10 computes the update by multiplying the derivative of the error by the derivative of the network's output.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Neural Networks and Deep Learning_](http://neuralnetworksanddeeplearning.com/index.html) is a free online book written by [Michael Nielsen](https://twitter.com/michael_nielsen) with a great introduction to neural networks.
* ["A Neural Network in 13 lines of Python"](https://github.com/ijustloveses/machine_learning/blob/master/gred_descent_and_dropout_with_numpy/A%20Neural%20Network%20in%2013%20lines%20of%20Python%20(Part%202%20-%20Gradient%20Descent).pdf) was the inspiration for the code in this question.</p></details>

-----------------------

## Date - 2023-01-03


## Title - Neighborhood groceries


### **Question** :

A neighborhood grocery store wants to segment its online customers to send personalized advertising to their inboxes.

Alivia is in charge of the team that will implement a solution, but she has no previous experience building a system like this.

**How should Alivia approach this problem?**


### **Choices** :

- Alivia should use a decision tree to find and group similar customers.
- Alivia should use a decision tree to group customers into a predefined list of categories.
- Alivia should use a clustering algorithm to find and group similar customers.
- Alivia should use a clustering algorithm to group customers into a predefined list of categories.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Customer segmentation](https://towardsdatascience.com/customer-segmentation-with-machine-learning-a0ac8c3d4d84) is a popular field where you try to find similar characteristics among your customers. It's the perfect opportunity to use unsupervised learning: a clustering algorithm.

Alivia doesn't know what characteristics the applicants share, so she can't predefine the categories on which she wants to segment the customers. Instead, she needs to use a clustering algorithm to find those groups.

For example, she could use [K-Means](https://en.wikipedia.org/wiki/K-means_clustering) to find interesting patterns and group the customers that share them. A critical distinction is that you don't need to consider these groups preemptively; the clustering algorithm will find them for you.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Customer Segmentation with Machine Learning"](https://towardsdatascience.com/customer-segmentation-with-machine-learning-a0ac8c3d4d84) for a quick introduction to Customer Segmentation.
* ["10 Clustering Algorithms With Python"](https://machinelearningmastery.com/clustering-algorithms-with-python/) will introduce you to 10 different clustering algorithms.</p></details>

-----------------------

## Date - 2023-01-04


## Title - Noisy training loss


### **Question** :

Lucia finished training her deep learning model. She got the training history and plotted the train and validation loss of the model.

Unfortunately, she didn't like what she saw. 

It was tough to look at the chart and draw conclusions from it: The training loss was too noisy, and Lucia would have to fix it if she wanted a better understanding of her model.

**Which strategies should Lucia follow to reduce the noise in the training loss? Select all that apply.**


### **Choices** :

- Lucia should increase her learning rate.
- Lucia should decrease her learning rate.
- Lucia should increase her batch size.
- Lucia should decrease her batch size.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>If Lucia increases the learning rate, her model will take larger steps in the gradient direction, but it may miss the local minima. When it misses the local minima, it will start oscillating contributing to the noise Lucia sees in the chart. Reducing the learning rate will help with this problem.

If Lucia uses a small batch size, the optimizer will only see a small portion of the data during every cycle. This introduces noise in the training process because the gradient of the batch may take the model in entirely different directions. Here is Jason Brownlee on ["How to Control the Stability of Training Neural Networks With the Batch Size"](https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/): "Smaller batch sizes are noisy, offering a regularizing effect and lower generalization error."

Based on this idea, if Lucia decreases the batch size, the noise will get worse, so she wants to increase the size to reduce the oscillations of the training loss.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>*  Check ["How to Control the Stability of Training Neural Networks With the Batch Size"](https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/) for a full description of how the batch size affects the stability of the training process.
* ["What could an oscillating training loss curve represent?"](https://ai.stackexchange.com/questions/14079/what-could-an-oscillating-training-loss-curve-represent) is a StackExchange question covering different strategies to reduce the noise in the training loss.
* ["Why is my training loss fluctuating?"](https://www.researchgate.net/post/Why_is_my_training_loss_fluctuating) is another public question covering the same topic.</p></details>

-----------------------

## Date - 2023-01-05


## Title - Purchasing products


### **Question** :

Collins was working on a machine learning model to predict the likelihood of a customer purchasing a product. She was hired by an e-commerce company that wanted to use her model to personalize their sales promotions.

After several weeks of development, Collins had two models that performed well on the validation data. However, she noticed that the models had different strengths and weaknesses. One model had a higher precision but a lower recall than the other.

Collins wanted to find the best overall model to deploy in production, but she wasn't sure how to choose between them.

**What is the best way for Collins to decide which model is the best overall?**


### **Choices** :

- Collins should fix the precision at 95% and choose the model with the higher recall.
- Collins should fix the recall at 95% and choose the model with the higher precision.
- There's no objective way to decide which model is best. Collins should pick either one of them.
- Collins should compute the area under the ROC curve for both models and choose the one with the higher value.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Which model is better usually depends on the application. A high recall is more important than precision for some use cases, while in others, it is not. 

The first two options suggest fixing one particular metric and choosing the model that performs the best on the other one. This is a valid approach, but it's not what Collins needs. She wants to determine which model is the best, but fixing either recall or precision won't return the best overall model since we would always prioritize one of the metrics.

For example, imagine that we tune both models to a recall above `95%` and then pick the one with the higher precision. There's no guarantee that the model we choose is the best possible overall—the one that better balances recall and precision. Instead, we ensured that the model we picked was the best, with a recall above `95%`.

To find the best overall model, Collins should compute the area under the ROC curve (Receiver Operating Characteristic) and choose the model with the higher value. 

A ROC curve shows the True Positive and False Positive Rates at different classification thresholds. The area under this curve measures the performance of the model. A perfect model will have an area of `1.0`, while a model that only makes mistakes will have an area of `0.0`. Therefore, choosing the model with the higher area will give Collins the best overall model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["ROC and AuC"](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) for an explanation of how the Area under the Curve on a ROC works.
* ["ROC metrics"](https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics) will give you more information about the different metrics you can compute from a ROC.</p></details>

-----------------------

## Date - 2023-01-06


## Title - Robotics department


### **Question** :

Alayah works in the robotics department at a tech company. She has been tasked with building a model that recognizes and identifies different objects.

Alayah decides to use a transformer model for this task. She knows that transformers are a type of deep learning model that has been proven effective in object recognition tasks.

**Which of the following are characteristics of transformers?**


### **Choices** :

- Transformers use attention mechanisms to allow the model to focus on specific parts of the input.
- Transformers can handle variable-length output sequences but only fixed-size input sequences.
- Transformers are based on convolutional neural networks and use filters to extract features from the input data.
- Transformers can only work with numeric data.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Transformers are a type of deep learning model that has been widely used in Natural Language processing tasks, such as machine translation and text summarization. They were introduced in the 2017 paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al.

One of the key characteristics of transformers is their use of attention mechanisms. Attention allows the model to focus on specific input parts, which is useful when processing long data sequences. This differs from traditional recurrent neural networks, which use feedback loops to process the input data.

Another characteristic of transformers is their ability to handle variable-length input and output sequences. This is useful when dealing with natural language data, which often has varying lengths.

Transformers are not based on convolutional neural networks (CNNs.) They are based on self-attention mechanisms, while CNNs are based on convolutional layers, which apply a set of filters to the input data to extract features. 

Transformers are not limited to working with numerical data. They can also process categorical data.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check the ["Attention is All You Need"](https://arxiv.org/abs/1706.03762) paper by Vaswani et al. for an introduction to Transformers.
* The [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) is a fantastic visual guide that explains the inner workings of the Transformer architecture in an easy-to-understand manner.</p></details>

-----------------------

## Date - 2023-01-07


## Title - Low training. High testing


### **Question** :

There's not a lot of context for you other than the following chart showing the training and testing losses of a machine learning model:

![Training and Testing Loss Chart](https://user-images.githubusercontent.com/1126730/188513143-7dc8ec67-17a3-4ce9-919c-7a2851bf4d49.jpg)

As you can see, after finishing training, the training loss is low, but the testing loss is high.

**What's a reasonable conclusion about this machine learning model?**


### **Choices** :

- Your model is overfitting.
- Your model is underfitting.
- Your model is either overfitting or underfitting, but we can't tell.
- Your model is well-fit.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A good model should capture valuable patterns in the data and discard any noise that doesn't help with predictions. An overfitting model will fit that noise. An underfitting model will not capture the relevant patterns in the dataset. 

An overfitting model should not have any problems with the training data but stumble with the testing data. Therefore, we should expect a low training loss and a high testing loss. An underfitting model should struggle with the training and testing datasets, so both of its losses should be high. A well-fit model, however, should have low training and testing losses.

The chart shows the model doing well with the training data but struggling with the testing set. Therefore, the model is overfitting.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.</p></details>

-----------------------

## Date - 2023-01-08


## Title - Auto industry


### **Question** :

Hallie works in the auto industry. She is tasked with building a model to predict the likelihood of a car accident based on various factors such as the driver's age, weather conditions, and the car's make and model.

Hallie decides to use a decision tree for this task. She knows that decision trees are powerful models that can help her understand the relationships between the input features and the target variable.

**Which of the following are characteristics of decision trees?**


### **Choices** :

- Decision trees are non-parametric models that make no assumptions about the data distribution.
- Decision trees can handle both numerical and categorical data.
- Decision trees split the data into subsets based on random features.
- Decision trees are difficult to interpret and explain to non-technical stakeholders.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Decision trees are a popular and powerful tool in machine learning and data science. They are a type of non-parametric model, which means that they make no assumptions about the distribution of the data. This allows them to work well with various data types, including numerical and categorical data.

One of the main strengths of decision trees is their ability to split the data into subsets based on the most important features, not random features. This allows them to identify the key relationships between the input features and the target variable. The resulting tree structure is easy to interpret and explain to non-technical stakeholders, making them a useful tool for making decisions in real-world scenarios.

One of the benefits of decision trees is that they are easy to interpret and explain to non-technical stakeholders. The tree structure shows the sequence of decisions made to classify the data, which can help us understand the relationships between the input features and the target variable.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Decision tree learning"](https://en.wikipedia.org/wiki/Decision_tree_learning) is the Wikipedia page where you can read about decision trees.
* The Scikit-Learn's ["Decision Trees"](https://scikit-learn.org/stable/modules/tree.html) page contains an extensive list of advantages and disadvantages of decision trees.</p></details>

-----------------------

## Date - 2023-01-09


## Title - Digit distribution


### **Question** :

The MNIST dataset is a collection of scanned images of handwritten digits. It’s a modified subset of two datasets collected by the United States National Institute of Standards and Technology and one of the most popular datasets out there.Here is a simple way you can load this dataset in a [Google Colab](https://colab.research.google.com/) notebook:```from keras.datasets import mnist(X_train, y_train), (X_test, y_test) = mnist.load_data()```The dataset contains every digit from 0 to 9.**Could you help us determine what of the following statements are correct about the dataset?**


### **Choices** :

- There are a total of 7,293 images of digit 7.
- The most popular digit in the dataset is 1, and the least popular in the train set is 9.
- The digit 5 represents a mere 9% of the images in the test set.
- There are 80,000 images among both train and test sets.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We need to write some code to answer this question.Assuming that we start with the code snippet provided in the problem statement, we have the dataset split into a train and a test set, so we need to [concatenate](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) them to make the rest of the code simpler:```import numpy as nplabels = np.concatenate((y_train, y_test))```Notice that we don't need to worry about `X_train` and `X_test` because those arrays contain the images. We can answer the question by looking at the labels only.[Numpy's `unique()` function](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) will allow us to group and count the labels:```count = np.unique(labels, return_counts=1)[1]count_train = np.unique(y_train, return_counts=1)[1] count_test = np.unique(y_test, return_counts=1)[1]```If we print the count corresponding to digit 7, we will find out that there are 7,293 images:```print(count[7])```To determine the most popular digit in the dataset and the least popular in the train set, we can use the `argmax` and `argmin` functions in the corresponding arrays:```print(np.argmax(count))print(np.argmin(count_train))```By running this code, you'll find out that 1 is indeed the most popular digit in the entire dataset, but 5 is the least popular in the train set.We can compute the percentage that digit 5 represents in the test set with the following line:```print(count_test[5] / y_test.shape[0])```The result is `0.089`.Finally, we can find the total number of images by printing `labels.shape[0]`. We have 70,000 images, not 80,000.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For more information, you can check Numpy's [concatenate](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) and [unique](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) functions.</p></details>

-----------------------

## Date - 2023-01-10


## Title - Two bad models


### **Question** :

Chloe wasn't confident about the results of her homework, and her professor confirmed her fears.

Two of the models she had to train weren't good enough. The first was overfitting, while the second was underfitting.

She got some feedback: the professor suggested analyzing the models from a bias and variance perspective and taking the necessary steps to fix them.

Chloe is still learning. She wasn't sure what to do.

**Which of the following is the correct conclusion about Chloe's models?**


### **Choices** :

- The overfitting model has a high bias, and the one underfitting has a high variance.
- The overfitting model has a high bias, and the one underfitting has a low variance.
- The overfitting model has a low bias, and the one underfitting has a high variance.
- The overfitting model has a low bias, and the one underfitting has a low variance.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To answer this question, we need to focus on the impact of bias on overfitting models and variance on underfitting models.

Let's start with the bias, and what [Jason Brownlee](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) has to say about it: "Bias are the simplifying assumptions made by a model to make the target function easier to learn."

In other words, bias refers to the assumptions the model makes to simplify the process of finding answers. The fewer assumptions it makes, the less biased the model is.

Take any linear model, for example. They are highly biased because these models assume a linear relationship between the observations and the target variable. Because of these assumptions, linear models can easily underfit the training data. On the other hand, decision trees have a high variance. They don't make too many assumptions about the target function, so they are more likely to overfit.

Therefore, high biased models are prone to underfitting, while high variance models are prone to overfitting. This narrows the correct answer to either the third or the fourth choice.

Here is what [Jason](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) has to say about variance: "Variance is the amount that the estimate of the target function will change if different training data was used."

Variance refers to how much the answers given by the model will change if we use different training data. The simpler the model is, the less likely the results will vary if we use different training data, while more complex models will have the opposite problem. Often, linear models are low-variance, and nonlinear models are high-variance.

Therefore, high variance models will be prone to overfit, while low variance models will be prone to underfit. Looking at the two candidate choices left, the fourth is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Here is Jason Brownlee's article I mentioned before: ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/).
* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).
* In case you like the simplicity of Twitter threads, here is one for you about this topic: ["Bias, variance, and their relationship with machine learning algorithms"](https://twitter.com/svpino/status/1390969728504565761).</p></details>

-----------------------

## Date - 2023-01-11


## Title - A tiny number


### **Question** :

Here is Phoebe's experiment:

She split her dataset into training and testing. She then set the testing data aside and created five other random splits from the training data, each with 20% of the samples.

Phoebe then trained a model on each of the five groups and tested them on the testing data to determine the accuracy of each model. She grabbed the results and computed the variance between them:

![Pseudocode](https://user-images.githubusercontent.com/1126730/199803699-e056bcea-133f-45b1-9c52-8e68d75f5bed.png)

The final result was a tiny number.

**What can you say about the model Phoebe used in her experiment?**


### **Choices** :

- Phoebe's model is high bias and low variance.
- Phoebe's model is high bias and high variance.
- Phoebe's model is low bias and low variance.
- Phoebe's model is low bias and high variance.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>After Phoebe trains each model and computes their accuracy on the testing set, a couple of things could happen:

1. The accuracy of each model is significantly different.
2. The accuracy of each model is relatively similar.

We know that Phoebe split the training dataset into five random groups, so we should expect each of these groups to be similar. If the model's performance on each differs significantly, we know Phoebe is using a high-variance model.

On the other hand, if the model's performance is very similar across all five groups, we know we are looking at a stable, high-bias model that doesn't change much with new data.

The variance between the five accuracies is tiny, so Phoebe must be using a high-bias and low-variance model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) for an introduction to bias and variance.
* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).</p></details>

-----------------------

## Date - 2023-01-12


## Title - Outstanding performance


### **Question** :

Cecilia was finally ready to start building the model supporting her research.

One of the largest companies in the world is sponsoring Cecilia's work, so she is not concerned about costs. As her sponsor mentioned, she has "unlimited power" at her disposal.

Their concern is for Cecilia to build the most robust model possible. After years of questionable evaluation practices from the research community, they want to ensure that Cecilia provides an accurate estimate of model performance.

**Which of the following approaches should Cecilia use to build her model?**


### **Choices** :

- Cecilia should use leave-one-out cross-validation to build her model.
- Cecilia should train the model on the entire dataset and later evaluate it on the same data.
- Cecilia should train the model on a portion of the data and use the rest to evaluate the model.
- Cecilia should use k-fold cross-validation with 5 to 10 folds.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Cecilia should use [leave-one-out cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation).

Leave-one-out cross-validation is a variant of cross-validation where the number of folds equals the number of samples in the dataset. 

To use leave-one-out cross-validation, we build one model for each sample in the dataset. We train each model using all data except one instance we later use to evaluate its performance. Finally, we compute the overall performance by averaging the result of each model.

Leave-one-out cross-validation is significantly more expensive than every other choice on this question. Assuming Cecilia will use leave-one-out cross-validation on a dataset with 10,000 samples, she will need to train 10,000 models. Compare this with 10-Fold cross-validation, where she will only need to build ten models. 

Fortunately, Cecilia doesn't care about costs, and leave-one-out cross-validation will give a more robust estimate of model performance. Each sample has an opportunity to represent the entire dataset and contribute to the final evaluation, and this will result in a reliable and unbiased estimate of model performance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["LOOCV for Evaluating Machine Learning Algorithms"](https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/) is an excellent introduction to leave-one-out cross-validation.
* Check ["A Quick Intro to Leave-One-Out Cross-Validation (LOOCV)"](https://www.statology.org/leave-one-out-cross-validation/) for a succinct introduction to leave-one-out cross-validation.</p></details>

-----------------------

## Date - 2023-01-13


## Title - Decreasing KNN's K


### **Question** :

Diana knows her k-Nearest Neighbor (KNN) implementation uses a value of `K` that's too high.

She wants to start experimenting with a lower value.

**What should Diana expect to happen as she decreases `K`?**


### **Choices** :

- As Diana decreases the value of `K`, she will reduce the algorithm's variance and bias.
- As Diana decreases the value of `K`, she will increase the algorithm's variance and bias.
- As Diana decreases the value of `K`, she will increase the algorithm's variance and reduce its bias.
- As Diana decreases the value of `K`, she will reduce the algorithm's variance and increase its bias.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The smaller the value of `K`, the more variance and less bias KNN will exhibit. For example, if we use `K = 1`, a single sample close to our observation will cause the algorithm to return the wrong prediction. Imagine an observation surrounded by many instances from class A and only one from class B that's closer than everything else. Since `K=1`, the algorithm will incorrectly predict the observation as class B. 

Conversely, the larger the value of `K`, the less variance and more bias KNN will exhibit. Since KNN uses an average or majority voting, no individual sample will cause the algorithm to return the wrong prediction. Setting `K` to a value that's too large will make the algorithm underfit because it won't capture the variance in the dataset.

Here is a quote from ["Why Does Increasing k Decrease Variance in kNN?"](https://towardsdatascience.com/why-does-increasing-k-decrease-variance-in-knn-9ed6de2f5061):

> If we take the limit as `K` approaches the size of the dataset, we will get a model that just predicts the class that appears more frequently in the dataset [...]. This is the model with the highest bias, but the variance is 0 [...]. High bias because it has failed to capture any local information about the model, but 0 variance because it predicts the exact same thing for any new data point.

In summary, the smaller the value of `K` is, the lower the bias and the higher the variance. The larger the value of `K` is, the higher the bias and the lower the variance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Why Does Increasing k Decrease Variance in kNN?"](https://towardsdatascience.com/why-does-increasing-k-decrease-variance-in-knn-9ed6de2f5061) is a great article diving into the relationship of `k` and the variance of KNN.
* For a more general introduction to the bias-variance trade-off, check ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/).</p></details>

-----------------------

## Date - 2023-01-14


## Title - Geometric transformations


### **Question** :

We can't understand how deep learning networks work without looking into different tensor operations like addition, multiplication, etc. In the end, the magic mostly boils down to many of these operations chained together.

Here is something fun.

Every tensor operation has a corresponding geometric interpretation. For example, the addition operation represents the action of [translating](https://en.wikipedia.org/wiki/Translation_(geometry)) an object. Adding a vector to a set of points representing a 2D object is equivalent to moving the object by a certain amount in a specific direction.

**Which of the following descriptions are correct geometric interpretations of tensor operations in a 2D plane?**


### **Choices** :

- We can rotate an object by multiplying it with a 2x2 matrix with the following structure: `[[cos(θ), -sin(θ)], [sin(θ), cos(θ)]]`.
- We can scale an object by a dot product with a diagonal 2x2 matrix with the following structure: `[[h, 0], [0, v]]`.
- We get a linear transform by adding an arbitrary matrix.
- An affine transform combines a linear transform and a translation.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When we talk about [rotating](https://en.wikipedia.org/wiki/Rotation_(mathematics)) an object, we can think of moving each point of that object circularly around a center. Assuming that we use a column vector to represent the coordinate of each point, we can use matrix multiplication to rotate the object.

First, we need a rotation matrix R. This is the matrix we will multiply with the object's coordinates to obtain the new set of rotated coordinates. The structure of this matrix R to rotate an object counterclockwise is `[[cos(θ), -sin(θ)], [sin(θ), cos(θ)]]` where θ represents the rotation angle. Therefore, the first choice is correct.

We can [scale](https://en.wikipedia.org/wiki/Scaling_(geometry)) an object using the dot product with a matrix S, but this time the matrix will have a different structure. Vertical and horizontal scaling of a 2D object requires a 2x2 matrix containing the horizontal and vertical factors—by how much we want to scale the object in each direction.

Let's assume we want to scale the object in half horizontally but keep it as-is vertically. The horizontal factor should be 0.5, and the vertical factor should be 1.0. The scaling matrix S will be `[[0.5, 0], [0, 1.0]]`. Notice how this is a diagonal matrix because it only contains non-zero values in the diagonal. Therefore, the second choice is correct.

The third choice argues that a [linear transform](https://en.wikipedia.org/wiki/Linear_transformation) is an addition operation with an arbitrary matrix, but this is incorrect. The core of a linear transformation is the dot product, not an addition operation. Notice that both scaling and rotation are linear transforms.

Finally, the fourth choice argues that an [affine transform](https://en.wikipedia.org/wiki/Affine_transformation) combines a linear transform and a translation, which is correct:

> If X is the point set of an affine space, then every affine transformation on X can be represented as the composition of a linear transformation on X and a translation of X. 

In summary, the first, second, and fourth choices are correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Deep Learning with Python, Second Edition_](https://amzn.to/3K3VZoy) covers these operations in detail.</p></details>

-----------------------

## Date - 2023-01-15


## Title - One or the other


### **Question** :

Lilith is working on a classification model and wants to find a way to evaluate it depending on whether precision or recall is more important to her client.

She is using the Fβ score.

Lilith understands how to give more weight to precision or recall, but she needs to consider the case where her client only cares about one of the metrics regardless of the value of the other.

**How can Lilith do this?**


### **Choices** :

- If Lilith wants to consider precision only, she should use β = 0.
- If Lilith wants to consider precision only, she should use β < 1.
- If Lilith wants to consider recall only, she should use β > 1.
- If Lilith wants to consider recall only, she should set β = infinity.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Here is the Fβ score formula:

![FBeta-Score](https://user-images.githubusercontent.com/1126730/193036770-0a37f6f7-9a5d-4be3-9b89-7018931140eb.jpg)

The Fβ score lets us combine precision and recall into a single metric. When using β = 1, we place equal weight on precision and recall. For values of β > 1, recall is weighted higher than precision; for values of β < 1, precision is weighted higher than recall.

If Lilith's client wants only to consider precision regardless of the recall value, Lilith can use β = 0. Notice how the Fβ score becomes precision when β = 0. Conversely, If Lilith's client wants only to consider recall regardless of the precision value, Lilith can use β = infinite.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What is the F-Score?"](https://deepai.org/machine-learning-glossary-and-terms/f-score) is a short introduction to this metric.
* ["Micro, Macro & Weighted Averages of F1 Score, Clearly Explained"](https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f) is a great article covering how to compute a global F1-Score metric in multi-class classification problem.
* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2023-01-16


## Title - Binary features


### **Question** :

Your mission is to build a decision tree.

You'll work with a dataset where every feature has a value of 0 or 1. The dataset can have any number of features.

You want the decision tree to learn a function that outputs how many features in a sample have a value of 0.

**Assuming the dataset has _n_ rows and _d_ features, how many leaf nodes would your decision tree have?**


### **Choices** :

- 2ⁿ leaf nodes
- 2ᵈ leaf nodes
- 2n leaf nodes
- 2d leaf nodes


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>For a dataset with d features, you will require a decision tree with 2ᵈ leaf nodes.

As an example, imagine we have a dataset with a single feature. The decision tree will need 2 leaf nodes: one to capture the case where the feature has a value of 0 and another to capture when the feature has a value of 1:

![Decision tree with one feature](https://user-images.githubusercontent.com/1126730/197406448-5dda78a5-0011-4525-aa6b-084ed46805a4.jpg)

For 2 features, the possible combinations are four: both features are 0, the first feature is 0 and the second is 1, the first feature is 1, and the second is 0, and both features are 1. The possible resultant count is 0, 1, or 2:

![Decision tree with two features](https://user-images.githubusercontent.com/1126730/197406460-edb7b72f-246f-4c53-8598-d5da4fd7c01c.jpg)

Generally, for _d_ features, the decision tree will need 2ᵈ leaf nodes.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Decision tree learning"](https://en.wikipedia.org/wiki/Decision_tree_learning) is the Wikipedia page where you can read about decision trees.
- ["How to tune a Decision Tree?"](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680) is a great article talking about different ways to tune a decision tree, including the effects of setting the maximum depth hyperparameter.</p></details>

-----------------------

## Date - 2023-01-17


## Title - Not enough experience


### **Question** :

Gabriela works for Microsoft and is trying to build a machine-learning model that can accurately classify emails as spam or not spam. 

She knows that choosing the right hyperparameters can greatly improve the model's performance, but she doesn't have much experience with hyperparameter tuning.

**Which of the following statements summarizes the core goal of hyperparameter tuning?**


### **Choices** :

- Hyperparameter tuning is about choosing the set of optimal samples from the data to train a model.
- Hyperparameter tuning is about choosing the set of optimal features from the data to train a model.
- Hyperparameter tuning is about choosing the set of hypotheses that better fit the goal of the model.
- Hyperparameter tuning is about choosing the optimal parameters for a learning algorithm to train a model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We use the term "hyperparameter" to refer to the settings we can use to control the learning process. We set these "knobs" and "levers" before training a model. In contrast, we use "parameters" to refer to variables internal to the model whose values we estimate (learn) during the learning process using data.

A good way of thinking about this:

* Parameters: We learn their values during training. We do not set their values manually.
* Hyperparameters: The settings we fix before the learning process. We cannot learn these values during training.

Each model has different hyperparameters. For example, you can control the depth of a decision tree or the step size during the optimization process of a neural network.

Understanding this should be enough to analyze the four choices for this question.

Hyperparameters have nothing to do with the data. They aren't about features or samples. A good set of hyperparameters will indirectly lead to a better-fitted model, but "tuning hyperparameters" is not about "choosing a better hypothesis."

The correct answer is choosing the best parameters to tune a learning algorithm.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Overview of hyperparameter tuning"](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview) is a great introduction to hyperparameters and the process of finding their optimal value.
* [Hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization)
* [What is the Difference Between a Parameter and a Hyperparameter?](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)</p></details>

-----------------------

## Date - 2023-01-18


## Title - Fraud detection


### **Question** :

Milani's team has been working on a fraud detection algorithm for months.

They used the best practices and followed all the necessary steps: they split their data into training and test sets, they used a variety of metrics to evaluate their model, they balanced their dataset, and they reviewed their examples regularly to ensure there were no labeling errors.

Finally, the model was ready, and Milani's team deployed it to production.

The algorithm was a huge success. The team received much praise, and the number of fraud cases detected increased by more than 50% in the first month.

However, Milani soon noticed something strange. All the fraud cases detected by the model were in the US.

**What is the most likely reason for this problem?**


### **Choices** :

- The model is too simple and couldn't learn the entire dataset of fraud cases, leaving out those from other countries.
- The model is suffering from data or concept drift.
- The model didn't train long enough to capture all the details of different fraud cases.
- The model suffers from sampling bias. It probably didn't include enough examples of fraud cases from other countries.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A common problem in machine learning is that a model that shows promising results during evaluation doesn't perform well when deployed in production.

While there may be various reasons for that, the story above points us in one particular direction.

The first choice, a simple model, can't be the correct answer. If that were the case, the model would be underfitting and give poor results across the board, not only in the US.

Data and concept drift are indeed common problems with models in production. However, they arise when the environment changes over time, and so does the input to the model. In this case, the problem appeared straight after deployment.

Training the model longer is unlikely to solve this problem. The model is already performing well, and this issue only appears in the US. If the model wasn't trained long enough, it would be underperforming in general, not just in a specific region.

This leaves us with the only correct answer: The most likely reason for this problem is that Milani's team didn't have enough data from other countries, so the model struggles to recognize fraud cases from those regions.

This issue is called "sampling bias." It explains why the problem occurred in one particular country. Sampling bias is difficult to detect during development because the data is missing from the training and test datasets, so we can't notice it while evaluating the model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Sampling bias"](https://en.wikipedia.org/wiki/Sampling_bias) for a complete explanation of this problem.</p></details>

-----------------------

## Date - 2023-01-19


## Title - Graphic designer


### **Question** :

Lena is a graphic designer who has been working on a new project for her client. 

She needs to determine the probability that the client will approve of the design she has created. 

To do this, she uses an algorithm that considers several factors, including the number of revisions the client has requested in the past and the overall design quality. The equation for the algorithm is:

```
probability = σ(a + 0.27b + 0.18c)
```

Where:

* `a`: Indicates the number of revisions the client has requested in the past.
* `b`: Indicates the overall quality of the design on a scale from 1 to 10.
* `c`: Indicates the length of time Lena has spent working on the design.
* `σ`: Is the sigmoid function.

**Assuming the client has requested two revisions in the past, the design has a quality rating of 8, and Lena has spent 10 hours working on the design, what would be the probability that the client will approve of the design?**


### **Choices** :

- The probability is `0.99`
- The probability is `0.81`
- The probability is `0.62`
- The probability is `0.45`


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We need to understand how to compute the Sigmoid function to answer this question. The Sigmoid function takes a value as input and outputs another real value between `0` and `1`. We say that Sigmoid "squeezes" the input into that range.

Here is the formula:

![Sigmoid](https://user-images.githubusercontent.com/1126730/196755518-311dd425-676e-4c85-be1f-467694879c30.jpg)

In Python, the Sigmoid function looks like this:

```
import math

def sigmoid(x):
    return 1/(1 + math.exp(-x))
```

The probability that the client will approve the design can be calculated by plugging the appropriate values for `a`, `b`, and `c` into the equation. In this case, `a` would be 2,`b` would be 8, and `c` would be 10. The equation would look like this:

```
probability = sigmoid(2 + 0.27 * 8 + 0.18 * 10)
probability = 0.9974267268461897
```

Therefore, the probability that the client will approve the design is approximately 0.99.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For a complete description of the Sigmoid function, check the ["Logistic function"](https://en.wikipedia.org/wiki/Logistic_function) Wikipedia page.</p></details>

-----------------------

## Date - 2023-01-20


## Title - Early stopping


### **Question** :

Train a model for too long, and it will stop generalizing appropriately. Don't train it long enough, and it won't learn.

That's a critical tradeoff when building a machine learning model, and finding the perfect number of iterations is essential to achieving the results we expect.

Early stopping is a technique that helps.

**Which of the following statements about early stopping is correct?**


### **Choices** :

- Early stopping is a regularization technique that increases the model's generalization error.
- Early stopping is a regularization technique that under-constraints your model.
- Early stopping is a regularization technique that prevents overfitting.
- Early stopping is a regularization technique that prevents underfitting.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Early stopping](https://articles.bnomial.com/early-stopping) looks for a specific metric and stops the training process when it realizes that the metric is going in the wrong direction. Its goal is to prevent the model from overfitting, but it doesn't do anything to avoid underfitting.

Many experts recommend leaving the model under-constrained when using early stopping. That way, we can find the exact number of epochs when the model starts overfitting without having any other regularization technique obscuring those results. That, however, doesn't mean that early stopping under-constrains a model, so the second choice is incorrect.

Finally, early stopping helps the model generalize to unseen data, opposite to the first choice argument.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Early stopping"](https://articles.bnomial.com/early-stopping) for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models.</p></details>

-----------------------

## Date - 2023-01-21


## Title - Random splits


### **Question** :

Saylor is building a classification model.

She labeled the dataset and randomly split the data into training, validation, and testing sets. 

After training and evaluating the model, Saylor realized the results were too good to be true. Deploying the model to production validated her assumption: the model was a failure.

After some research, Saylor discovered the problem was with the random split. 

**Which of the following kinds of data is susceptible to this problem?**


### **Choices** :

- Time series data.
- Data that doesn't change over time.
- Data that show up in clusters, like news articles.
- Data where individual samples are correlated.


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Any dataset with correlation or groupings between individual samples is not a good candidate for random splitting. When used, data from each group will be present in each set, leading to a leaky validation strategy. Here is an excerpt from [The Kaggle Book](https://amzn.to/3kbanRb):

> In a leaky validation strategy, the problem is that you have arranged your validation strategy in a way that favors better validation scores because some information leaks from the training data.

This problem can happen anytime the data is grouped. Data that doesn't change over time is usually fine for random splits.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.
* ["Target Leakage in Machine Learning"](https://www.youtube.com/watch?v=dWhdWxgt5SU) is a YouTube presentation that covers leakage, including during the partitioning of a dataset.</p></details>

-----------------------

## Date - 2023-01-22


## Title - Regular process


### **Question** :

Elise received a massive dataset for her new project.

Unfortunately, there were too many features, so Elise spent quite a bit of time performing feature selection and creating a new dataset containing only the new features.

After she finished, Elise started her regular process: split the data, set the testing aside, and train a classifier on the training set.

**What do you think about Elise's setup?**


### **Choices** :

- Elise's setup is not valid.
- Elise's setup is problematic.
- Elise's setup should work as expected.
- We need more information to decide.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>There's a problem with Elise's setup.

She used the entire dataset for feature selection. She should have split the dataset first and performed feature selection on the training and testing data separately. 

When doing feature selection on the entire dataset, Elise risks leaking information from the soon-to-be test data into the model. A leak will lead to a model that performs too well on the existing data but will do poorly on future unseen data.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check [Data Leakage in Machine Learning](https://machinelearningmastery.com/data-leakage-machine-learning/) for an introduction to data leaks and how to prevent them. 
* ["Train, Validation, Test Split for Machine Learning"](https://blog.roboflow.com/train-test-split/) goes into detail about the importance of each split.</p></details>

-----------------------

## Date - 2023-01-23


## Title - Fighting overfitting


### **Question** :

What worries Kamila more than anything else is dealing with models that overfit. She never had apparent issues with underfitting, but regularizing a model that doesn't work well on production data is always a chore.

But Kamila's problem wasn't the process of fixing the model but the underlying theory of why that process worked in the first place.

So here she is, staring at two different regularization techniques. The first is L1, while the second is L2 regularization.

**Which of the following illustrates how these two techniques work? Select all that apply.**


### **Choices** :

- L1 regularization uses the sum of absolute values of the weights to penalize the loss function.
- L1 regularization uses the sum of squares of the weights to penalize the loss function.
- L2 regularization uses the sum of absolute values of the weights to penalize the loss function.
- L2 regularization uses the sum of squares of the weights to penalize the loss function.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>L1 regularization helps with overfitting by shrinking the model's parameters toward zero, making certain features irrelevant and preventing the model from using them to make predictions. L1 regularization uses the sum of absolute values of the weights to penalize the loss function.

On the other hand, L2 regularization fights overfitting by forcing weights to be small but not exactly zero. That means that the model can still use irrelevant features to make predictions, but the overall impact of those features will be limited. L2 regularization uses the sum of squares of the weights to penalize the loss function.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Fighting Overfitting With L1 or L2 Regularization: Which One Is Better?"](https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization) will give you a complete introduction to L1 and L2 regularization.</p></details>

-----------------------

## Date - 2023-01-24


## Title - Dependent Nuclear reactor


### **Question** :

Joanna is the structural engineer in charge of designing a nuclear reactor. Her team plans to build the plant next to the coast, so they need to understand the probability and effects of natural disasters.After consulting with local experts, they know that the probability of a large earthquake in the area is `10^-4`, and the probability of a tsunami is also `10^-4`. However, they think a tsunami always happens whenever a large earthquake occurs.**What's the probability of having both an earthquake and a tsunami simultaneously?**


### **Choices** :

- The probability of having both events simultaneously is `10^-8`.
- The probability of having both events simultaneously is `10^-5`.
- The probability of having both events simultaneously is `10^-4`.
- The probability of having both events simultaneously is `1`.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We can compute the probability of having both an earthquake and a tsunami simultaneously using the following formula:```P(T and E) = P(E) * P(T|E)```We know the probability of an earthquake `P(E) = 10^-4`, and the probability of a tsunami `P(T)` is also `10^-4`. We also know these events are dependent, and the probability of a tsunami given that an earthquake happens `P(T|E)` is `1`.We can now substitute the values in our initial equation:```P(T and E) = P(E) * P(T|E)P(T and E) = 10^-4 * 1P(T and E) = 10^-4```</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Cartoon Guide to Statistics_](https://amzn.to/3eg9iIo) is a fun and instructive introduction to probabilities and statistics.</p></details>

-----------------------

## Date - 2023-01-25


## Title - Approaching a problem


### **Question** :

Alexandria is a university professor who wants to build an algorithm to predict whether a student will pass or fail a course based on their past academic performance. 

She has access to a labeled dataset with detailed academic information from thousands of students, including their grades and whether they passed or failed their courses.

Alexandria is not sure which approach to take and would like some guidance.

**Understanding that there are many ways to approach a problem, what would be your first recommendation to Alexandria?**


### **Choices** :

- The best way to approach this problem is with Unsupervised Learning by using a clustering algorithm.
- The best way to approach this problem is with Supervised Learning by using a regression algorithm.
- The best way to approach this problem is with Supervised Learning by using a classification algorithm.
- The best way to approach this problem is with Reinforcement Learning.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Alexandria is trying to answer a question with only two possible answers: Is a student going to pass or fail?

There could be many ways to approach this problem, but we can go with what we know in this case.

Alexandria has access to labeled data, so she could set some of her data aside for testing purposes and build a binary classification model to predict a binary target value. This target could be whether or not a student passes or fails a course.

This is likely the more straightforward way to approach Alexandria's problem: A Supervised Learning binary classification model.

We could also frame this as a regression problem depending on the data we have for each student, but nothing in the description points to that being a good strategy.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Supervised and Unsupervised Machine Learning Algorithms"](https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/) for an explanation of supervised and unsupervised learning algorithms.</p></details>

-----------------------

## Date - 2023-01-26


## Title - Critical solution


### **Question** :

Lola has been building machine learning systems for a decade, and her specialty is neural networks.

Lola maintains a critical solution that helps her company make money: a multi-class classification model that she and her team have worked on for a few years.

Almost every layer of Lola's model uses ReLU except the last layer, which uses softmax.

**Which of the following statements are true about the softmax activation function when used in the output layer of a neural network?**


### **Choices** :

- The softmax function is not differentiable, and that's why Lola can only use it in the output layer of the network.
- The softmax function is differentiable, and that's why Lola can use it in the output layer of the network.
- The softmax function turns a vector of real values into an integer value representing the correct class index.
- The softmax function turns a vector of real values into a vector of probabilities that sum to 1.


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The softmax function is differentiable, which is an essential property to use its result to optimize a cost function. The function turns a vector of real values into another vector of probabilities that sum to 1. These probabilities are proportional to the relative scale of each value in the vector.

While we can use the resultant vector to determine the correct class index, notice that softmax doesn't output this index directly. Instead, it outputs a vector where the index of the larger value will correspond to the correct class.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check ["Softmax Activation Function with Python"](https://machinelearningmastery.com/softmax-activation-function-with-python/) for more information about the Softmax function and a way to implement it.
- ["What is the Softmax Function"](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer) is a great summary of this function.</p></details>

-----------------------

## Date - 2023-01-27


## Title - Right after the tutorial


### **Question** :

Lainey wants to start practicing after reading a machine-learning tutorial.

Luckily for her, the instructor left a few ideas and shared some datasets students could use.

Lainey needs to be careful. She has limited experience and wants to focus on something she understands. For now, neural networks combined with a log loss should do the trick.

**Which of the following problems are good candidates for Lainey to practice?**


### **Choices** :

- Given a picture of shoes, predict their corresponding brand.
- Predict whether a subscriber will churn over the next month.
- Predict yearly revenue given the number of products sold over the past 24 months.
- Determine whether a notification message is valid or a spam message.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Log loss is a function that we commonly use in classification problems. It returns the negative logarithm of the product of probabilities.

Log loss indicates how close a prediction probability is to the target value. The more the predicted probability differs from the target value, the higher the log loss.

Lainey needs to focus on classification problems. From the list of options, the only problem that doesn't fit is predicting yearly revenue. That's a regression problem, while the other three options are classification problems.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Understanding binary cross-entropy / log loss: A visual explanation"](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) is an excellent introduction to binary cross-entropy.
- Another article that helps understand binary cross-entropy is ["Binary Cross Entropy/Log Loss for Binary Classification"](https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/).</p></details>

-----------------------

## Date - 2023-01-28


## Title - Reporting scores


### **Question** :

It was late, and Amara had to send the report of her last exam.

She didn't have time to do much with the test scores, so she decided to report the median of the scores and leave any further analysis for another day.

Her students' scores were: 5, 5, 3, 4, 5, 2, 5, 5, 2, 5, and 3.

**What is the median value that Amara should report?**


### **Choices** :

- The median value of the dataset is 2.
- The median value of the dataset is 3.
- The median value of the dataset is 4.
- The median value of the dataset is 5.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The mean, median, and mode are different ways to measure the center of a dataset. Each of these metrics tries to summarize a set of values using a single number.

Amara can find the median by looking at the middle number after sorting the scores. In case there are two middle numbers, then she can use the mean of those two numbers.

Amara has 11 scores, and the middle number after sorting the list is `5`.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Mean Median Mode: What They Are, How to Find Them"](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/) for a complete explanation of the Mean, Median, and Mode.</p></details>

-----------------------

## Date - 2023-01-29


## Title - Tanh's interpretation


### **Question** :

Alyssa is looking into activation functions trying to understand how they work.

Sigmoid and Tanh were popular choices, but Alyssa read about how Tanh was the preferred choice for many practitioners.

Here is the Tanh formula:

![Tanh](https://user-images.githubusercontent.com/1126730/197406160-92aa0f03-ede8-43bc-853a-546360bc2287.jpg)

**Which of the following is a correct interpretation of this function?**


### **Choices** :

- The output of the Tanh function could be any real number.
- The output of the Tanh function could be any integer number.
- The output of the Tanh function is a real number between `-1` and `1`.
- The output of the Tanh function is a real number between `0` and `1`.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The Hyperbolic tangent function, or Tanh, takes a value as input and outputs another real value between `-1` and `1`. We say that Tanh "squeezes" the input into that range.

Tanh is continuously differentiable, and its derivative is simple to compute. 

Here is the plot of the Tanh function:

![Tanh plot](https://user-images.githubusercontent.com/1126730/197406179-9f80cd61-2241-489e-98a7-bd63bf74beba.jpg)</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For a complete description of the Tanh function, check the ["Hyperbolic Functions"](https://en.wikipedia.org/wiki/Hyperbolic_functions) Wikipedia page.</p></details>

-----------------------

## Date - 2023-01-30


## Title - Daily question


### **Question** :

Alayna found Bnomial, an online website that posts one machine learning question daily.

Perfect! It was an excellent way for Alayna to practice consistently!

The first question she found was about neural networks:

**Which of the following sentences are true about neural networks?**


### **Choices** :

- We can optimize neural networks using the Gradient Descent algorithm.
- Neural networks can find the optimal solution for convex problems.
- Neural networks can use a mix of activation functions.
- Neural networks can approximate any function when using non-linear activations.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Gradient descent is an iterative optimization algorithm used to find the local minimum of a function. The algorithm works by taking steps proportional to the negative of the function's gradient at the current point. Gradient Descent is an excellent choice to optimize neural networks.

Using Gradient descent, we can find the optimal solution for a convex problem, assuming we configure the algorithm with an appropriate learning rate.

When setting up a neural network, we can use a mix of activation functions. For example, ReLU in the hidden layers and Softmax in the output layer of a classification model.

Finally, thanks to the [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem), we can turn a two-layer neural network into a universal function approximator when using non-linear activation functions.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Gradient Descent For Machine Learning"](https://machinelearningmastery.com/gradient-descent-for-machine-learning/) for a description of the algorithm.
* Check the ["Universal approximation theorem"](https://en.wikipedia.org/wiki/Universal_approximation_theorem) on Wikipedia for more information about the power of neural networks.</p></details>

-----------------------

## Date - 2023-01-31


## Title - Rotating beers


### **Question** :

Avianna works at a beer factory where she processes pictures of the bottles before they are shipped out. 

She noticed that the pictures were often taken with different degrees of rotation, and the Convolutional Neural Network she built wasn't ready to handle this.

**Which of the following approaches could Avianna propose to handle rotation in the pictures?**


### **Choices** :

- Adding a data preprocessing step to properly rotate every image before giving the data to the model.
- Add a layer to the model that can rotate the data to the correct position.
- Include rotated versions of the images in the training data to build some rotation invariability into the model.
- Correctly configure the network since Convolutional Neural Networks are inherently rotation invariant.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Convolutional Neural Networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) are [translation invariant but not rotation invariant](https://pyimagesearch.com/2021/05/14/are-cnns-invariant-to-translation-rotation-and-scaling/). They can recognize the same patterns independently of where they show in an image, but they don't have the same ability to recognize shapes with various degrees of rotation. 

There's also no out-of-the-box mechanism for a layer to recognize the orientation of an image and much less "rotate" it to its correct position.

Dealing with rotated images is a common problem, especially with user data. The solution usually boils down to the following two options:

1. Training a model that learns to recognize pictures regardless of their orientation.
2. Completely sidestepping the problem and always using images in the appropriate position to train the model.

A way to train our model to recognize pictures regardless of their orientation is to extend the dataset with rotated images. If we expect to see images at 0, 90, and 180 degrees, we must teach our model to recognize them. ["Correcting Image Orientation Using Convolutional Neural Networks"](https://d4nst.github.io/2017/01/12/image-orientation/) is an excellent article covering this approach.

Finally, we can extend the pipeline that feeds the model with a step that ensures images are always in the same position. That way, our model doesn't have to deal with rotation. This may not always be possible, but sometimes we have access to metadata that could help with that—for example, using accelerometer information to straighten the picture.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Correcting Image Orientation Using Convolutional Neural Networks"](https://d4nst.github.io/2017/01/12/image-orientation/) is a great article covering practical ways to get a network to recognize rotated pictures.
* ["Are CNNs invariant to translation, rotation, and scaling?"](https://pyimagesearch.com/2021/05/14/are-cnns-invariant-to-translation-rotation-and-scaling/) goes into more detail about whether convolutional neural networks are translation, rotation, and scale invariant.
* Check ["How Do Convolutional Layers Work in Deep Learning Neural Networks?"](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/) for an introduction to how convolutional layers work.</p></details>

-----------------------

## Date - 2023-02-01


## Title - Cross-entropy variations


### **Question** :

Fiona found a Keras example online training a neural network and using cross-entropy loss. 

Fiona was familiar with a couple of implementations of the cross-entropy loss: `CategoricalCrossentropy` and `SparseCategoricalCrossentropy.`

**When should you use every one of these implementations of the cross-entropy loss?**


### **Choices** :

- We should use `SparseCategoricalCrossentropy` when the labels in the dataset are one-hot encoded.
- We should use `SparseCategoricalCrossentropy` when the labels in the dataset are integer values.
- We should use `CategoricalCrossentropy` when the labels in the dataset are one-hot encoded.
- We should use `CategoricalCrossentropy` when the labels in the dataset are integer values.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Keras' `SparseCategoricalCrossentropy` computes the cross-entropy loss between the labels and predictions. It works when the labels in the dataset are integer values, for example, `1`, `2`, and `3`.

Keras' `CategoricalCrossentropy`, on the other hand, has the same function but works when the labels in the dataset are one-hot encoded, for example, `[1, 0, 0]`, `[0, 1, 0]`, and `[0, 0, 1]`.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Here is Keras' [SparseCategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) and [CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) documentation.</p></details>

-----------------------

## Date - 2023-02-02


## Title - Connections between units


### **Question** :

A "feedback loop" is when connections between units form a directed cycle, thus creating loops in a neural network. This feature allows networks to save information in the hidden layers.

Only some neural network types support the concept of feedback loops.

**Which of the following support feedback loops?**


### **Choices** :

- Multilayer Perceptron
- Feed Forward Neural Network
- Recurrent Neural Networks
- Convolutional Neural Networks


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Recurrent Neural Networks have an advantage over traditional feed-forward networks when working with time series or data points that depend upon previous samples.

The magic ingredient of Recurrent Neural Networks is the ability to store the information of previous inputs to generate the following sequence output. Recurrent Neural Networks do this by implementing the concept of a "feedback loop" or "feedback connection." These are connections feeding the hidden layers of the neural network back into themselves.

None of the other options support the concept of feedback loops. Only Recurrent Neural Networks do.

[Long Short-Term Memory](https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/) (LSTM) networks are a type of Recurrent Neural Network that is very popular for sequence prediction problems.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [An Introduction To Recurrent Neural Networks And The Math That Powers Them](https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/)
* [Recurrent neural network](https://en.wikipedia.org/wiki/Recurrent_neural_network)
* [A Gentle Introduction to Long Short-Term Memory Networks by the Experts](https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/)</p></details>

-----------------------

## Date - 2023-02-03


## Title - Dental office


### **Question** :

Arielle worked as a receptionist at a busy dental office, always looking for ways to improve the practice's efficiency. She had recently become interested in machine learning and was eager to learn more about how it could be applied in her work.Arielle was already busy, so she wanted to maximize her value by spending her study time on the more impactful algorithms that could help her at work.One of the first lessons she learned was the No Free Lunch Theorem.**How is the No Free Lunch Theorem relevant to Arielle in this situation?**


### **Choices** :

- It suggests that Arielle should focus on studying the most complex algorithms since they are likely to be the most impactful.
- It suggests that Arielle should focus on studying the simplest algorithms since they are likely to be the most effective.
- It suggests that Arielle should focus on studying supervised learning algorithms since they are the most widely used in machine learning.
- It suggests that Arielle should focus on studying a variety of different algorithms since there is no single algorithm that is universally the best for all possible tasks.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The [No Free Lunch Theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem) states that there is no single machine learning algorithm that is universally the best for all possible tasks. This means that the performance of a machine learning algorithm depends on the specific characteristics of the data and task at hand, and there is no one-size-fits-all solution.In this situation, Arielle is looking to improve the efficiency of the dental office and wants to use machine learning to help solve specific problems. She knows that the No Free Lunch Theorem means that she will need to carefully consider the specific needs and challenges of the practice when selecting a machine learning algorithm. This means that she should focus on studying various algorithms to be prepared to try out different approaches and find the best solution for each problem.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["No Free Lunch Theorem for Machine Learning"](https://machinelearningmastery.com/no-free-lunch-theorem-for-machine-learning/) for a complete explanation of this theorem.</p></details>

-----------------------

## Date - 2023-02-04


## Title - Fortune 500


### **Question** :

Of all the data that Lucille received, she must decide which features will help build a machine-learning model.

Lucille's team is focusing on Fortune 500 companies, and their model will predict what other companies have a shot at joining the list.

**Which of the following columns on the dataset are meaningful features for the model?**


### **Choices** :

- The phone number of the company.
- The company's profits from the previous year.
- The number of employees that work for the company.
- The company's zip code.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The companies' phone numbers and zip codes will probably not help classify them. Lucille should discard these features.

The number of employees and profits from the previous year should be helpful for the model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Feature Engineering for Machine Learning_](https://amzn.to/3SsnLAc) is an excellent book covering feature engineering.
* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.</p></details>

-----------------------

## Date - 2023-02-05


## Title - Classifying customers


### **Question** :

Ariyah was working on a machine learning project to predict the likelihood of a customer churning based on their past behavior. She had been using a Decision Tree model to classify customers.While researching Decision Trees, Ariyah read about pruning, but she didn't understand how it would help her work.**What is the primary purpose of pruning a Decision Tree?**


### **Choices** :

- To prevent overfitting or the phenomenon where the model fits the training data too closely and performs poorly on new data.
- To reduce the time required for testing the model on new data.
- To conserve space for storing the Decision Tree model on a computer or server.
- To decrease the model's error on the training set.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Decision Trees are a popular machine-learning model for classification and regression tasks. They build a tree-like decision model based on the data's features to predict the output label of a given input sample. However, if the Decision Tree is allowed to grow too large, it can become overly complex and may start to fit the training data too closely, a phenomenon known as overfitting.Pruning is a technique that can be used to reduce the size of a Decision Tree and prevent overfitting. It involves removing nodes from the tree that are not contributing significantly to the model's accuracy. This can improve the model's generalization performance and reduce the risk of overfitting. While pruning a Decision Tree can also save computing time and space for storing the model, the primary purpose of pruning is to avoid overfitting the training set.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Decision tree pruning"](https://en.wikipedia.org/wiki/Decision_tree_pruning) covers the process of pruning a tree to reduce overfitting.* ["How to tune a Decision Tree?"](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680) is a great article talking about different ways to tune a decision tree, including the effects of setting the maximum depth.</p></details>

-----------------------

## Date - 2023-02-06


## Title - Intensive Care Unit


### **Question** :

Briella was working at a hospital, where she was tasked with developing a machine learning model to predict whether a patient will be admitted to the intensive care unit (ICU). She had access to medical data for thousands of patients, with labels indicating whether each patient was admitted to the ICU or not. After studying the available techniques, Briella had several options for building a binary classification model. **Which of the following techniques can she use to build a binary classification model?**


### **Choices** :

- Briella can use logistic regression.
- Briella can use k-nearest neighbors.
- Briella can use neural networks.
- Briella can use decision trees.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The four choices are examples of supervised learning techniques that you could use to build a binary classification model.Logistic regression is a popular technique for binary classification that uses a linear model to predict the likelihood of an outcome. K-nearest neighbors is a non-parametric technique that uses a similarity measure to classify data points based on their neighbors. It is often used for classification tasks, including binary classification.Neural networks can learn complex patterns in the data and can be used for a wide range of tasks, including binary classification.Decision trees are a type of machine learning model that uses a tree-like structure to make predictions. They are simple to understand and can be effective for binary classification tasks.Although they all work, they are very different approaches, and some might lead to better results on this dataset than others.However, the problem doesn't give us any information about the characteristics of the data, so it's hard to disqualify any of the choices based on what we know.Therefore, every choice is a correct answer.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check out ["Binary classification"](https://en.wikipedia.org/wiki/Binary_classification) in Wikipedia for more information.* ["4 Types of Classification Tasks in Machine Learning"](https://machinelearningmastery.com/types-of-classification-in-machine-learning/) is a great article going over different ways to implement a classification model.</p></details>

-----------------------

## Date - 2023-02-07


## Title - Human-like language


### **Question** :

Madilyn works in the natural language processing department at a tech company. She has been tasked with building a model that can understand and generate human-like language.

Madilyn decides to use a transformer model for this task. She knows that transformers are a type of deep learning model that has been proven effective in natural language processing tasks.

**Which of the following are characteristics of transformers?**


### **Choices** :

- Transformers use attention mechanisms to allow the model to focus on specific parts of the input.
- Transformers can handle variable-length input and output sequences.
- Transformers are based on recurrent neural networks and use feedback loops to process the input data.
- Transformers can only work with categorical data.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Transformers are a type of deep learning model that has been widely used in Natural Language processing tasks, such as machine translation and text summarization. They were introduced in the 2017 paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al.

One of the key characteristics of transformers is their use of attention mechanisms. Attention allows the model to focus on specific input parts, which is useful when processing long data sequences. This differs from traditional recurrent neural networks, which use feedback loops to process the input data.

Another characteristic of transformers is their ability to handle variable-length input and output sequences. This is useful when dealing with natural language data, which often has varying lengths.

Transformers are not limited to working with categorical data. They can also process numerical data.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check the ["Attention is All You Need"](https://arxiv.org/abs/1706.03762) paper by Vaswani et al. for an introduction to Transformers.
* The [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) is a fantastic visual guide that explains the inner workings of the Transformer architecture in an easy-to-understand manner.</p></details>

-----------------------

## Date - 2023-02-08


## Title - Tom's definition


### **Question** :

Using Tom M. Mitchell’s definition of Machine Learning: > "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."**Which of the following is the correct breakdown for the problem of recognizing handwritten digits?**


### **Choices** :

- **Task T:** Classifying handwritten digits. **Performance Measure P:** Number of correctly classified digits. **Experience E:** A dataset of handwritten digits.
- **Task T:** Classifying handwritten digits. **Performance Measure P:** Percentage of correctly classified digits. **Experience E:** Number of classified digits.
- **Task T:** Classifying handwritten digits. **Performance Measure P:** Percentage of correctly classified digits. **Experience E:** A dataset of handwritten digits and their corresponding classifications.
- **Task T:** Classifying handwritten digits. **Performance Measure P:** Percentage of correctly classified digits. **Experience E:** A dataset of handwritten digits.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The correct breakdown for the problem of recognizing handwritten digits is "**Task T:** Classifying handwritten digits. **Performance Measure P:** Percentage of correctly classified digits. **Experience E:** A dataset of handwritten digits and their corresponding classifications."The **Performance Measure P** can't be the number of classified digits because that metric doesn't measure the quality of the model.The **Experience E** can't be the number of classified digits. Instead, it should be a dataset of digits, including their correct classification.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Machine Learning"](https://deepai.org/machine-learning-glossary-and-terms/machine-learning) for an introduction to machine learning, including Tom Mitchel's original framing.</p></details>

-----------------------

## Date - 2023-02-09


## Title - Large Models


### **Question** :

Gracelyn was a curious computer science student who had always been fascinated by natural language processing. When she heard about the new Large Language Models (LLMs) that had recently been developed, she knew she had to learn more.At the time, some of the most popular LLMs were BERT, with 110 million parameters, GPT-3, with 175 billion parameters, and PaLM, with 540 billion parameters.But Gracelyn wasn't sure what exactly do these parameters mean.**Which of the following is the meaning of "parameters" in the context of  Large Language Models (LLM)?**


### **Choices** :

- The different learning rate values that were used to train the model.
- The size of the input vocabulary used to train the model.
- The number of connections between the neurons of the model.
- The size of the attention mechanism used by the model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In the context of Large Language Models (LLM), "parameters" refers to the number of connections between the neurons in the model. The number of parameters determines the complexity of the model and the amount of information it can store and process.Something to keep in mind is that the larger the number of parameters, the more sophisticated the model is, but the more computationally expensive will be to train and use.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Neural Networks and Deep Learning_](http://neuralnetworksanddeeplearning.com/index.html) is a free online book written by [Michael Nielsen](https://twitter.com/michael_nielsen) with a great introduction to neural networks.</p></details>

-----------------------

## Date - 2023-02-10


## Title - Theater department


### **Question** :

Vivienne works in the data science department of a theater. She is tasked with building a model to predict which plays will be successful based on their genre, cast, and other factors.

Vivienne decides to use a decision tree for this task. She knows that decision trees are powerful models that can help her understand the relationships between the input features and the target variable.

**Which of the following are characteristics of decision trees?**


### **Choices** :

- Decision trees are non-parametric models that make no assumptions about the data distribution.
- Decision trees can handle both numerical and categorical data.
- Decision trees split the data into subsets based on the most important features.
- Decision trees are easy to interpret and explain to non-technical stakeholders.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Decision trees are a popular machine-learning technique that is used for classification and regression tasks. They are a type of non-parametric model, which means that they make no assumptions about the distribution of the data. This allows decision trees to capture complex patterns in the data without requiring us to make any assumptions.

Decision trees can handle numerical and categorical data, making them versatile tools for many real-world problems. They split the data into subsets based on the most important features, and this process continues until the data is fully classified or the tree reaches the maximum depth.

One of the benefits of decision trees is that they are easy to interpret and explain to non-technical stakeholders. The tree structure shows the sequence of decisions made to classify the data, which can help us understand the relationships between the input features and the target variable.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Decision tree learning"](https://en.wikipedia.org/wiki/Decision_tree_learning) is the Wikipedia page where you can read about decision trees.
* The Scikit-Learn's ["Decision Trees"](https://scikit-learn.org/stable/modules/tree.html) page contains an extensive list of advantages and disadvantages of decision trees.</p></details>

-----------------------

## Date - 2023-02-11


## Title - Playing Stratego


### **Question** :

Remington was always eager to try new strategies while playing Stratego. She believed that the key to success was to constantly adapt and improve.Remington had a strategy for winning her Stratego matches: Sometimes, she would make a random move. Other times, she would only make moves that she knew would give her an advantage. Over time, Remington became skilled at predicting her opponent's moves and choosing the best course of action.Remington, a reinforcement learning engineer, described her approach as a "0.6 epsilon greedy policy."**Which of the following correctly summarizes Remington's approach while playing Stratego?**


### **Choices** :

- Remington makes a random move 60% of the time and only makes moves she knows will give her an advantage 40% of the time.
- Remington makes a random move 40% of the time and makes moves she knows will give her an advantage 60% of the time.
- Remington emphasizes exploiting her knowledge more than trying new strategies.
- Remington usually makes moves that she knows will give her an advantage and otherwise makes random moves.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In reinforcement learning, an ["epsilon greedy policy"](https://developers.google.com/machine-learning/glossary#epsilon-greedy-policy) refers to a strategy that either follows a random policy with epsilon probability or a greedy policy otherwise.  A [greedy policy](https://developers.google.com/machine-learning/glossary#greedy-policy) always chooses the action with the highest expected return. In Remington's case, this would be executing the strategies she already knows. She explores and discovers new strategies when following a [random policy](https://developers.google.com/machine-learning/glossary#random_policy).Remington describes her approach as a "0.6 epsilon greedy policy" because she follows a random strategy about 60% of the time. In other words, Remington spends most of her time exploring rather than exploiting what she already knows.As an additional note, in reinforcement learning, after accumulating knowledge from exploiting, the algorithm reduces the value of epsilon to shift from following a random to a greedy policy.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Exploration vs. exploitation in reinforcement learning"](https://www.manifold.ai/exploration-vs-exploitation-in-reinforcement-learning) is a great introduction to the exploration vs. exploitation tradeoff.* Check Google's machine learning glossary definition of ["Epsilon greedy policy"](https://developers.google.com/machine-learning/glossary#epsilon-greedy-policy).</p></details>

-----------------------

## Date - 2023-02-12


## Title - Dataset statistics


### **Question** :

The MNIST dataset is a collection of handwritten digit images that has been modified from two datasets originally collected by the National Institute of Standards and Technology in the United States. 

It is a popular dataset and can be easily loaded in a [Google Colab](https://colab.research.google.com/) notebook using the provided code snippet:

```
from keras.datasets import mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()
```

The dataset contains every digit from 0 to 9.

**Could you help us determine what of the following statements are correct about the dataset?**


### **Choices** :

- The digit 1 has the most images in the dataset, totaling 7,877 images.
- The digit 0 has the least images in the test set, totaling 600 images.
- The train and test sets contain the same number of images.
- The digit 3 has 6,131 images in the train set.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We need to write some code to answer this question. Let's start by [concatenating](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) the train and test sets:

```
import numpy as np
labels = np.concatenate((y_train, y_test))
```

Notice that we don't need to worry about `X_train` and `X_test` because those arrays contain the images. We can answer the question by looking at the labels only.

[Numpy's `unique()` function](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) will allow us to group and count the labels:

```
digit, count = np.unique(labels, return_counts=1)
```

If we print the count corresponding to digit 1, we will find out that there are 7,877 images:

```
print(count[1])
```

To determine how popular each digit is, we can print the entire `count` array and get the following result: `[6903, 7877, 6990, 7141, 6824, 6313, 6876, 7293, 6825, 6958]`. Notice that digit 1 is indeed the most popular in the dataset.

We can check how many instances of digit 0 in the test set with the following code that will print 980:

```
print(np.where(y_test == 0)[0].shape[0])
```

We can find the total number of images on each set by printing `y_train.shape[0]` and `y_test.shape[0]`. The result will be 60,000 and 10,000, respectively.

Finally, we can check how many instances of digit 3 in the train set with the following code that will print 6,131:

```
print(np.where(y_train == 3)[0].shape[0])
```</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For more information, you can check Numpy's [concatenate](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) and [unique](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) functions.</p></details>

-----------------------

## Date - 2023-02-13


## Title - Returning a product


### **Question** :

Nina was working on a machine learning model to predict the likelihood of a customer returning a product. She was hired by a retailer that wanted to use her model to improve customer service.After several weeks of development, Nina had two models that performed well on the validation data, but she wanted to pick only one.**What is the best way for Nina to decide which model is the best overall?**


### **Choices** :

- Nina should compute the f1-score for both models and choose the higher value.
- Nina should compute the f1-score for both models and choose the lower value.
- Nina should compute the Recall for both models and choose the higher value.
- Nina should compute the Precision for both models and choose the higher value.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The best way for Nina to decide which model is the best overall is to compute the f1-score for both models and choose the higher value. The model's f1-score is the harmonic mean between precision and recall. We can use the following formula to compute it:![F1-Score](https://articles.bnomial.com/images/article-when-accuracy-doesnt-help-3.jpg)While precision and recall are important metrics, they do not provide a complete picture of its overall performance. Precision measures the proportion of true positive predictions made by the model, while recall measures the proportion of actual positive cases correctly predicted by the model. While a high precision indicates that the model is good at avoiding false positives, a high recall indicates that the model is good at finding all of the positive cases.In the case of Nina's models, a high precision might indicate that the model is good at predicting which customers are likely to return a product, but it does not necessarily mean that the model is good at finding all of the customers who are likely to return a product. Similarly, a high recall might indicate that the model is good at finding all the customers who are likely to return a product, but it does not necessarily mean that the model is good at avoiding false positives.Therefore, while precision and recall are important metrics to consider when evaluating a model's performance, they should not be used in isolation to determine which model is the best overall. The F1 score is a better metric to use because it provides a balanced view of a model's precision and recall and allows Nina to choose the model that has the best overall performance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Micro, Macro & Weighted Averages of F1 Score, Clearly Explained"](https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f) is a great article covering how to compute a global F1-Score metric in multi-class classification problem.* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2023-02-14


## Title - Stable suggestions


### **Question** :

When Vera read the suggestions from her teammates, it wasn't immediately clear what they were trying to tell her.

Vera deployed a model using k-Nearest Neighbors (KNN). She suggested exploring different values of `K` to improve the results. In response, her teammates sent a few suggestions.

Specifically, they mentioned that increasing the value of `K` will make the predictions more stable, while the opposite would happen if Vera decreased it.

**Vera wasn't sure what exactly they meant by "stable." What do you think they meant?**


### **Choices** :

- Vera's teammates were referring to the bias of the algorithm.
- Vera's teammates were referring to the variance of the algorithm.
- Vera's teammates were referring to the dimensionality of the algorithm.
- Vera's teammates were referring to the speed of the algorithm.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Vera's teammates were referring to the variance of the algorithm. More variance means the algorithm's results will be less stable, while less variance means the results will be more stable.

The smaller the value of `K`, the more variance and less bias KNN will exhibit. For example, if we use `K = 1`, a single sample close to our observation will cause the algorithm to return the wrong prediction. Imagine an observation surrounded by many instances from class A and only one from class B that's closer than everything else. Since `K=1`, the algorithm will incorrectly predict the observation as class B. 

Conversely, the larger the value of `K`, the less variance and more bias KNN will exhibit. Since KNN uses an average or majority voting, no individual sample will cause the algorithm to return the wrong prediction. Setting `K` to a value that's too large will make the algorithm underfit because it won't capture the variance in the dataset.

Here is a quote from ["Why Does Increasing k Decrease Variance in kNN?"](https://towardsdatascience.com/why-does-increasing-k-decrease-variance-in-knn-9ed6de2f5061):

> If we take the limit as `K` approaches the size of the dataset, we will get a model that just predicts the class that appears more frequently in the dataset [...]. This is the model with the highest bias, but the variance is 0 [...]. High bias because it has failed to capture any local information about the model, but 0 variance because it predicts the exact same thing for any new data point.

In summary, the smaller the value of `K` is, the lower the bias and the higher the variance. The larger the value of `K` is, the higher the bias and the lower the variance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Why Does Increasing k Decrease Variance in kNN?"](https://towardsdatascience.com/why-does-increasing-k-decrease-variance-in-knn-9ed6de2f5061) is a great article diving into the relationship of `K` and the variance of KNN.
* For a more general introduction to the bias-variance trade-off, check ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/).</p></details>

-----------------------

## Date - 2023-02-15


## Title - Every book is the same


### **Question** :

I have a ton of machine learning books at home.

Great books, most of them, but I can't help but notice that most start their story precisely at the same place:

Linear and Logistic Regression.

I get it. Understanding the fundamentals is essential, but at some point, most books should move on and cover different and more exciting topics.

**In the meantime, I have to ask you to select which of the following statements are true about these two techniques:**


### **Choices** :

- In Linear Regression, we find the best line that predicts the output. In Logistic Regression, we find the best sigmoid curve to classify the samples.
- We use Linear Regression for solving Regression problems. We use Logistic Regression for solving Classification problems.
- In Linear Regression, we predict the values of continuous variables. In Logistic Regression, we predict the values of categorical variables.
- Linear Regression uses a set of independent variables to predict a continuous dependent variable. Logistic Regression uses a set of dependent variables to predict a categorical independent variable.


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Linear Regression is probably the most popular Supervised Learning technique in machine learning. Its goal is to fit the best line through the data to predict a continuous output. Logistic Regression is also a Supervised Learning technique, with the difference that we use a sigmoid function to map values between 0 and 1. By doing this, we can use Logistic Regression to predict a categorical variable.

In Regression problems, we need to predict continuous outcomes, for example, a person's age, salary, or home price. Linear Regression helps with that. In Classification problems, we need to predict a categorical outcome, for example, whether the person is sick or whether they will get a job. We can use Logistic Regression for that.

Finally, Linear and Logistic Regression use a set of independent variables to predict a dependent variable. In Linear Regression, the dependent variable is continuous, while the dependent variable is categorical in Logistic Regression. Therefore, the final choice of this question is incorrect.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Linear Regression for Machine Learning"](https://machinelearningmastery.com/linear-regression-for-machine-learning/) is an introduction to Linear Regression.
* ["Logistic Regression for Machine Learning"](https://machinelearningmastery.com/logistic-regression-for-machine-learning/) is an introduction to Logistic Regression.</p></details>

-----------------------

## Date - 2023-02-16


## Title - Guessing the model


### **Question** :

Octavia had a problem with her model and decided to investigate.

She split her dataset into training and testing. Set the testing data aside and created four other random splits from the training data, each with 25% of the samples.

She then trained a model on each of the four groups and tested them on the testing data to determine the accuracy of each model. She grabbed the results and computed the variance between them.

Here is what Octavia's code looked like:

![Pseudocode](https://user-images.githubusercontent.com/1126730/199803699-e056bcea-133f-45b1-9c52-8e68d75f5bed.png)

After running her function, Octavia found out that the result was a value much higher than what she expected.

**If you had to guess, which model do you think Octavia is using?**


### **Choices** :

- Octavia could be using a Decision Tree.
- Octavia could be using k-Nearest Neighbors.
- Octavia could be using Linear Regression.
- Octavia could be using Logistic Regression.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>After Octavia trains each model and computes its accuracy on the testing set, a couple of things could happen:

1. The accuracy of each model is significantly different.
2. The accuracy of each model is relatively similar.

We know that Octavia split the training dataset into four random groups, so we should expect each of these groups to be similar. If the model's performance on each differs significantly, we know Octavia is using a high-variance model.

On the other hand, if the model's performance is very similar across all four groups, we know we are looking at a stable, high-bias model that doesn't change much with new data.

The variance between the four accuracies is high, so Octavia must be using a high-variance model. From the list, both Decision Trees and k-Nearest Neighbors are high-variance models.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) for an introduction to bias and variance.
* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).</p></details>

-----------------------

## Date - 2023-02-17


## Title - Juice packing


### **Question** :

Winter works at a juice-packing company where she uses a machine-learning model to predict the demand for different juice flavors.After training and evaluating the model, Winter was confident that it was ready for production. The model was deployed, and the company started using it to plan its production and distribution.During the first few months, everything was working as expected. But then, the company noticed that the model consistently overestimated the demand for certain flavors. **What could be the cause of the problem with the model?**


### **Choices** :

- The model is underfitting and needs more complexity.
- The model is overfitting and needs more regularization.
- The model is suffering from data drift.
- The model is suffering from sampling bias.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Machine learning models are trained on a historical dataset and make predictions based on that data. If the data distribution changes over time, the model can become less accurate and give poor predictions. This phenomenon is known as data drift.Data drift can occur for various reasons, such as changes in the underlying process that generates the data, changes in the environment, or changes in how the data is collected. In this case, it is likely that the demand for certain flavors of juice has changed over time, and the model is not capturing that change.Underfitting, overfitting, and sampling bias are not relevant in this scenario. Underfitting occurs when the model is too simple and cannot capture the underlying relationship in the data. Overfitting occurs when the model is too complex and captures noise in the data instead of the relationship. Sampling bias occurs when the training data does not represent the entire population. In this case, the problem appeared after a few months the model was deployed, so underfitting, overfitting, and sampling bias are not the cause.The most likely explanation for the problem is that the model suffers from data drift and needs to be retrained on more recent data to capture the changes in the demand for juice flavors.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Data Drift vs. Concept Drift: What Are the Main Differences?"](https://deepchecks.com/data-drift-vs-concept-drift-what-are-the-main-differences/) is a great introduction to data and concept drift.* ["Why You Should Care About Data and Concept Drift" ](https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift) is a great article from [Evidently AI](https://evidentlyai.com/) focusing on the importance of monitoring your models.</p></details>

-----------------------

## Date - 2023-02-18


## Title - Student performance


### **Question** :

Jayla is a data science student at a top university. She has been working on a project to build a model that predicts student success. She uses a deep learning model to analyze past student performance data and predict how well current students will do.Unfortunately, Jayla has struggled to get her model to perform well. She has tried different techniques and hyperparameters, but nothing seems to work.Jayla is starting to think that her model suffers from the exploding gradient problem. **Which of the following techniques will make Jayla's model more robust to the exploding gradient problem?**


### **Choices** :

- Gradient clipping
- Weight regularization
- Increasing the batch size
- Modifying the model architecture to include batch normalization


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The Exploding Gradient Problem is an issue that occurs in training artificial neural networks when large error gradients accumulate and result in very large updates to the network weights during training. This can cause the network to become unstable and prevent it from learning effectively.One way to overcome the exploding gradient problem is to use gradient clipping, where the gradients are capped at a maximum value to prevent them from growing too large. This can help to stabilize the network and improve its performance.Another technique is weight regularization, where the network weights are constrained to prevent them from becoming too large. This can help to prevent the gradients from exploding and improve the stability of the network.Increasing the batch size is not a valid solution to the exploding gradient problem. This technique has nothing to do with the gradients becoming large and exploding.Modifying the model architecture to include batch normalization can help with the vanishing gradient problem, but it does not address the exploding gradient problem.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Exploding Gradient Problem"](https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem) for an introduction to this problem.* ["A Gentle Introduction to Exploding Gradients in Neural Networks"](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/) is a great article covering Exploding Gradients in detail.</p></details>

-----------------------

## Date - 2023-02-19


## Title - Four rounds


### **Question** :

Four rounds of interviews later, Tessa felt so overwhelmed that she started questioning every one of her answers.As soon as she got to her hotel, she wrote down every question she could remember. She wanted to gain some confidence before going to bed.She remembered one particular question vividly but couldn't recall her answer. Tessa had to pick every example of an ensemble method from a list of choices.**Assuming the following are the choices, what of the following are examples of ensemble methods?**


### **Choices** :

- Random Forest
- Decision Trees
- AdaBoost
- Bootstrapping


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Ensembling is where we combine a group of models to produce a new model that yields better results than any initial individual models. Decision Trees is the only method from the list that's not an ensemble method. [Random Forest](https://en.wikipedia.org/wiki/Random_forest) is an algorithm that consists of many individual decision trees. It uses bootstrap aggregating to combine these trees to reach a solution much better than the one provided by any of the individual trees.[AdaBoost](https://en.wikipedia.org/wiki/AdaBoost) is another ensembling technique. It's also called Adaptive Boosting and, as the name implies, uses boosting.Finally, [Bootstrap aggregating](https://en.wikipedia.org/wiki/Bootstrap_aggregating), also called "bagging," is also a popular machine learning ensembling technique.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Bagging and Random Forest Ensemble Algorithms for Machine Learning"](https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/) is a great introduction to bagging and Random Forest.* Check out ["Understanding Random Forest"](https://towardsdatascience.com/understanding-random-forest-58381e0602d2) to understand how it works and why it's effective.* [_The Kaggle Book_](https://amzn.to/3kbanRb) is an excellent source for information about ensembling techniques.</p></details>

-----------------------

## Date - 2023-02-20


## Title - Outcome of matches


### **Question** :

Blair is the data scientist for a professional soccer team. She is working on a project to build a model that predicts the outcome of matches. Before she can train her model, she needs to understand her data. She decides to start by performing Exploratory Data Analysis (EDA).**Which of the following are some of the steps Blair takes during this process?**


### **Choices** :

- When doing EDA, Blair needs to evaluate the performance of her models on the data to understand how well they can make predictions.
- During EDA, Blair needs to understand the features in the dataset and the distribution of their values. This will help her identify potential problems or patterns that she can use to improve her model.
- During EDA, Blair needs to assess the data quality, including checking for missing or corrupt values. This is important because poor-quality data can lead to poor model performance.
- During EDA, Blair needs to learn the distribution of the target variable, which is the variable that she is trying to predict.


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Exploratory Data Analysis is an essential step in any machine-learning project. It allows us to explore and understand our data before we build a model. During this process, we investigate the dataset to discover valuable patterns, spot any potential anomalies and use statistics and plots to test different hypotheses and check assumptions. We also focus on understanding the distribution of the target variable and assessing the data quality, including checking for missing or corrupt values.Building a model is not part of the Exploratory Data Analysis process, so we don't evaluate the model's performance during this phase.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["What is Exploratory Data Analysis"](https://www.ibm.com/cloud/learn/exploratory-data-analysis) for an explanation of the EDA process.* The [_Feature Engineering for Machine Learning_](https://amzn.to/3JWZxZl) is a great book covering the entire Feature Engineering process.</p></details>

-----------------------

## Date - 2023-02-21


## Title - Image recognition


### **Question** :

Dahlia has been working on an image recognition app that uses a deep learning model to classify different objects. She's been struggling to get her model to perform appropriately.Dahlia knows her model suffers from the vanishing gradient problem. She decides to research every possible option to improve her model.**Which of the following techniques will help Dahlia's model overcome the vanishing gradient problem?**


### **Choices** :

- Dahlia should try using the ReLU activation function, which is known to reduce the vanishing gradient problem.
- Dahlia should consider introducing Batch Normalization to her model architecture to combat the vanishing gradient problem.
- Dahlia should increase the batch size to combat the vanishing gradient problem.
- Dahlia should carefully initialize the weights of her model, such as using He initialization, to reduce the vanishing gradient problem.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The [Vanishing Gradient Problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) occurs when the gradients of the loss function approach zero. This prevents the model from updating the weights and learning effectively. This problem is widespread when using the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) and [tanh](https://www.sciencedirect.com/topics/mathematics/hyperbolic-tangent-function) activation functions in deep neural networks.One way to overcome the vanishing gradient problem is to use the [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation function. ReLU is less likely to saturate, and its derivative is 1 for values larger than zero. This can help prevent the gradients from becoming too small and vanishing.Another technique that can help is introducing Batch Normalization to the model architecture. This normalizes the input to a layer, ensuring that the values don't reach the edges where the derivatives are too small. We can combat the vanishing gradient problem by modifying the input this way.Increasing the batch size is not a valid solution to the vanishing gradient problem. This technique has nothing to do with the gradients becoming small and vanishing.Carefully initializing the model weights can also help reduce the vanishing gradient problem. If we use sigmoid or tanh as our activation functions, and many of the weights are initialized with values too small or too large, we will end up with derivatives close to zero. Using [He initialization](https://arxiv.org/abs/1704.08863) can prevent this from happening.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["How to Fix the Vanishing Gradients Problem Using the ReLU"](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/) is a great explanation of how to approach this problem.* Check ["On weight initialization in deep neural networks"](https://arxiv.org/abs/1704.08863). It's an excellent paper covering weight initialization.</p></details>

-----------------------

## Date - 2023-02-22


## Title - Master's exam


### **Question** :

Millie is taking an exam for her Master's degree in machine learning.One of the questions tests her knowledge of Supervised Learning techniques. She needs to select every problem she can solve using Supervised Learning. **Which of the following problems should Millie select as examples of Supervised Learning?**


### **Choices** :

- Given a dataset of emails and their classification, build an application to determine whether an email is spam.
- Given a dataset of audio files and their text transcripts, build an application that turns any audio snippet into text.
- Given a dataset of translations between English and Spanish, build an application that turns any sentence written in English into Spanish.
- Given a dataset of images of circuit boards and whether they work, build an application that determines if a picture of a circuit board corresponds to a working board.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>There are multiple ways to approach these problems, but we can solve all of them using [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning).Supervised learning involves using labeled training data to make predictions or classify data. Every one of these examples gives us a dataset with examples of inputs and outputs.Transcription and translation are interesting examples because we can also solve these problems using Unsupervised Learning. For this particular question, Millie has access to labeled data, and she can frame the problem using Supervised Learning.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Supervised and Unsupervised Machine Learning Algorithms"](https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/) is an excellent introduction to the differences between supervised and unsupervised learning.</p></details>

-----------------------

## Date - 2023-02-23


## Title - News cycle


### **Question** :

Elaina works for a national pollster, and she is building a machine-learning model to classify the topic from the text of a news article.

She has access to a dataset containing every news article from the past year. Elaina's goal is to classify each piece into ten topics. Her team labeled the dataset, and Elaina will use a supervised learning model to make the predictions.

**How would you recommend Elaina split her dataset into training and validation?**


### **Choices** :

- Elaina should split her dataset based on the date of the story, ensuring every article around the same date goes into the same split.
- Elaina should split her dataset based on the classification of every story, ensuring every article from the same class goes into the same split.
- Elaina should split her dataset based on the classification of every story, ensuring every article from the same class goes into a different split.
- Elaina should split her dataset randomly, ensuring each split properly represents the overall dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The samples in Elaina's dataset are not independent. Multiple stories about the same topic will appear around the same time. For example, during a major upset win in sports, Elaina will find various news articles related to the same topic.

If Elaina splits her dataset randomly, the training and validation sets will likely contain the same stories. This will lead to a leaky validation strategy. Here is an excerpt from [The Kaggle Book](https://amzn.to/3kbanRb):

> In a leaky validation strategy, the problem is that you have arranged your validation strategy in a way that favors better validation scores because some information leaks from the training data.

Elaina's model can use the date as a clue to predict the topic of a news article: any story happening around the same date will likely belong to the same topic, which will make some of the samples from the validation set simple for the model to predict.

The solution is to group news articles by their date and ensure they go together into the same set.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.
* ["Target Leakage in Machine Learning"](https://www.youtube.com/watch?v=dWhdWxgt5SU) is a YouTube presentation that covers leakage, including during the partitioning of a dataset.</p></details>

-----------------------

## Date - 2023-02-24


## Title - Game theory


### **Question** :

Amari has been studying game theory and has learned about Nash equilibrium. She decides to test her understanding by designing a simple game and playing it with her friend, Jake.Each player can choose to either cooperate or defect. If both players cooperate, they each receive a reward of $3. If one player defects and the other cooperates, the defector receives a reward of $5, while the cooperator receives nothing. If both players defect, they each receive a reward of $1.**What is the Nash equilibrium in this game?**


### **Choices** :

- Amari and Jake cooperate, and both receive a reward of $3.
- Amari and Jake defect, and both receive a reward of $1.
- Amari defects and receives $5, while Jake cooperates and receives nothing.
- Amari cooperates and receives nothing, while Jake defects and receives $5.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A Nash equilibrium is a state in which all players in a game have made their best decisions given the decisions of the other players. In this game, the only Nash equilibrium is when both players defect because each receives the highest reward possible given the other player's decision.If Amari defects and Jake cooperates, Jake will receive a higher reward than if they both cooperate. This means that Jake has the incentive to change his decision to defect, and the game is not in a Nash equilibrium.Similarly, if Amari cooperates and Jake defects, Amari will receive nothing, which is less than the reward she would receive if they both cooperated. This means that Amari is incentivized to change her decision to cooperate, and the game is not in a Nash equilibrium.Therefore, the only Nash equilibrium in this game is when both players defect.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["An Introduction to the Nash Equilibrium"](https://builtin.com/data-science/nash-equilibrium) for a great explanation of one of the cornerstones of Game Theory.</p></details>

-----------------------

## Date - 2023-02-25


## Title - Candy flavors


### **Question** :

Jocelyn has been selling candy online for about a year, and after a lot of back and forth, she wants to introduce a second flavor. 

She is trying to run a tight ship, so she needs to ensure she doesn't order too much of the new flavor from her manufacturer. Jocelyn knows she needs to predict how much candy she will sell in the coming weeks.

After a few days, Jocelyn builds an algorithm to predict the probability of an existing customer buying the new flavor. The function looks like this:

```
probability = σ(a + 0.45b - 0.84)
```

Where:
* `a`: Indicates whether the customer bought candy during the last month.
* `b`: Indicates whether the customer bought more than one candy flavor before.
* `σ`: Is the sigmoid function.

**Assuming Patricia is a customer who bought two packages of different candy flavors last week, what would be the probability of her purchasing the new flavor?**


### **Choices** :

- The probability is `0.26`
- The probability is `0.65`
- The probability is `0.80`
- The probability is `1.0`


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We need to understand how to compute the Sigmoid function to answer this question. The Sigmoid function takes a value as input and outputs another real value between `0` and `1`. We say that Sigmoid "squeezes" the input into that range.

Here is the formula:

![Sigmoid](https://user-images.githubusercontent.com/1126730/196755518-311dd425-676e-4c85-be1f-467694879c30.jpg)

In Python, the Sigmoid function looks like this:

```
import math

def sigmoid(x):
    return 1/(1 + math.exp(-x))
```

We can now answer the question by solving the following equation:

```
probability = sigmoid(a + 0.45 * b - 0.84)
```

Patricia bought candy during the last month, so `a = 1`, and she bought different flavors, so `b = 1`:

```
probability = sigmoid(1 + 0.45 * 1 - 0.84)
probability = 0.6479408020806503
```

Therefore, Patricia's probability of buying the new flavor is `0.65`.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For a complete description of the Sigmoid function, check the ["Logistic function"](https://en.wikipedia.org/wiki/Logistic_function) Wikipedia page.</p></details>

-----------------------

## Date - 2023-02-26


## Title - Model recipes


### **Question** :

Gia's been working on a model to classify photos of food. 

Her company is building an application that will let users snap a picture of a plate at a restaurant and show them a potential recipe so they can cook it at home.

After a year of work, Gia's model was working great. The company launched the model worldwide and started monitoring user feedback.

Unfortunately, users from an Asian country complained because the model wasn't working for them.

**What is the most likely reason for the problem?**


### **Choices** :

- Gia's model didn't have enough complexity to learn all the data, so it's normal to have problems with certain regions.
- Gia needed to train the model for more time to fully capture the dataset's information.
- Gia's model is suffering from data drift.
- Gia's model is suffering from sampling bias.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A common problem in machine learning is that a model that shows promising results during evaluation doesn't perform well when deployed in production. 

Gia's model doesn't lack complexity. If that were the case, the model would be underfitting and give poor results across all regions where the application works. The training time is not a problem either for similar reasons.

Data and concept drift are common problems with models in production. However, they arise when the environment changes over time, and so does the input to the model. In this case, the problem appeared straight after deployment.

The most likely reason for this problem is that Gia didn't have enough data from the Asian country where the application is failing, so the model is struggling to recognize food from that region.

This issue is called "sampling bias." It explains why the problem occurred in one particular country. Sampling bias is difficult to detect during development because the data is missing from the training and test datasets, so we can't notice it while evaluating the model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Sampling bias"](https://en.wikipedia.org/wiki/Sampling_bias) for a complete explanation of this problem.</p></details>

-----------------------

## Date - 2023-02-27


## Title - Driving sales


### **Question** :

Wynter is working at a marketing agency and is trying to build a machine-learning model to predict which advertisements will be most effective in driving sales. Unfortunately, this is Wynter's first real project, and the model is not doing great. Her colleague recommended tuning the model's hyperparameters, but neither of them knows much about that.**Which of the following best describes how hyperparameter tuning impacts the performance of a model?**


### **Choices** :

- Hyperparameter tuning has no impact on the performance of a machine learning model.
- Hyperparameter tuning significantly improves the performance of a machine learning model by isolating the most relevant samples from the dataset.
- Hyperparameter tuning significantly improves the performance of a machine learning model by fine-tuning the model's ability to learn and generalize to new data.
- Hyperparameter tuning significantly improves the performance of a machine learning model by selecting the most relevant features from the dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We use the term "hyperparameter" to refer to the settings we can use to control the learning process. We set these "knobs" and "levers" before training a model. In contrast, we use "parameters" to refer to variables internal to the model whose values we estimate (learn) during the learning process using data.A good way of thinking about this:* Parameters: We learn their values during training. We do not set their values manually.* Hyperparameters: The settings we fix before the learning process. We cannot learn these values during training.Hyperparameter tuning is the process of adjusting the hyperparameters of a machine-learning model to achieve better performance. Hyperparameters have nothing to do with the data. They aren't about features or samples. Hyperparameter tuning can have a significant impact on the model's ability to learn and generalize to new data. We can fine-tune a model to improve its accuracy, precision, recall, and other performance metrics by choosing the values for the hyperparameters that result in the best performance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Overview of hyperparameter tuning"](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview) is a great introduction to hyperparameters and the process of finding their optimal value.* [Hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization)* [What is the Difference Between a Parameter and a Hyperparameter?](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)</p></details>

-----------------------

## Date - 2023-02-28


## Title - All-or-nothing affair


### **Question** :

For Charlee, multi-choice questions were always challenging.

She was much better with words and always found a way to reason while elaborating on her answers. But multi-choice questions were cold and to the point. They were an all-or-nothing affair. 

Charlee didn't have a choice. If she wanted to graduate, she had to answer the question:

**Which of the following sentences are true about neural networks?**


### **Choices** :

- We can only optimize neural networks using the Gradient Descent algorithm.
- Neural networks can find the optimal solution for convex problems.
- Neural networks can find the optimal solution for concave problems.
- Neural networks can approximate any function when using non-linear activations.


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Gradient descent is an iterative optimization algorithm used to find the local minimum of a function. The algorithm works by taking steps proportional to the negative of the function's gradient at the current point. Gradient descent is an excellent choice to optimize neural networks, but it's not the only way. We can use, for example, the Adam algorithm, a combination of the AdaGrad and RMSProp algorithms.

Using Gradient descent, we can find the optimal solution for convex and concave problems. The former depends on a suitable configuration of the learning rate, while the latter is not guaranteed: the algorithm might converge to a local minimum instead of the global minimum. However, finding optimal solutions for both types of objective functions is possible.

Finally, thanks to the [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem), we can turn a two-layer neural network into a universal function approximator when using non-linear activation functions.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Gradient Descent For Machine Learning"](https://machinelearningmastery.com/gradient-descent-for-machine-learning/) for a description of the algorithm.
* Check the ["Universal approximation theorem"](https://en.wikipedia.org/wiki/Universal_approximation_theorem) on Wikipedia for more information about the power of neural networks.
* ["Gentle Introduction to the Adam Optimization Algorithm for Deep Learning"](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning) is an excellent explanation of the Adam algorithm.</p></details>

-----------------------

## Date - 2023-03-01


## Title - Advertising spending


### **Question** :

Lily works at an advertising agency where she used a model to predict how much a client should spend on advertising.After training and testing her model, Lily felt confident it was ready for production: its performance was excellent. She deployed the model, and the agency started using it to make advertising recommendations to its clients.Hours later, a client complained that the model's predictions were consistently off by a large margin. **What could be the cause of the problem with the model?**


### **Choices** :

- The model is underfitting and needs more complexity.
- The model is overfitting and needs more regularization.
- The model is suffering from data drift.
- The model is suffering from sampling bias.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Overfitting and underfitting are common problems in machine learning. They occur when a model is not appropriately trained or has too much or too little capacity to learn from the data. Overfitting occurs when the model has learned the noise in the data and is unable to generalize to new data, while underfitting occurs when the model is unable to learn the underlying patterns in the data and performs poorly on both the training and test sets.In this example, Lily's model did well during training and testing, so assuming she had good data, it's unlikely the model was either overfitting or underfitting.Data drift is also unlikely. Data drift occurs when the data distribution changes over time, but in this case, Lily's model failed to work immediately after deployment, so there hasn't been enough time for the data to drift.Sampling bias is likely the cause of the problem. Sampling bias occurs when the training data does not represent the entire population. Lily might have trained the model on different data from the client's, and that's why the predictions are not correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Sampling bias"](https://en.wikipedia.org/wiki/Sampling_bias) for a complete explanation of this problem.</p></details>

-----------------------

## Date - 2023-03-02


## Title - Type I


### **Question** :

In statistics, the notion of a statistical error is integral to hypothesis testing. When testing the null hypothesis, there are two types of errors: **type I** and **type II**.Look at this confusion matrix of a hypothetical machine learning model that classifies spam emails. The "P" stands for "Positive" samples, and the "N" stands for "Negative" samples. Our default assumption is that emails are not spam.![Confusion Matrix](https://user-images.githubusercontent.com/1126730/188163753-6424565d-3c8c-421e-95fb-7d89aba1453a.jpg)**How many type I errors does this model have?**


### **Choices** :

- There are 75 type I errors.
- There are 22 type I errors.
- There are 12 type I errors.
- There are 121 type I errors.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Type I errors are the same as **false positives**. For example, if we mark a valid email as spam, we are in the presence of a false positive. Type I errors are the rejection of a true [null hypothesis](https://www.investopedia.com/terms/n/null_hypothesis.asp) by mistake.On the other hand, Type II errors are the same as **false negatives**. For example, if we let a spam message pass as a valid email, we are in the presence of a false negative. This is a type II error because we accept the conclusion of the email being good, even though it is incorrect. Type II errors are the acceptance of a false null hypothesis by mistake.Therefore, there are a total of 12 type I errors in this hypothetical model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.* Check out ["Type I and type II errors"](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors) for the definition and examples of each type of error. * ["Understanding Null Hypothesis Testing"](https://opentextbc.ca/researchmethods/chapter/understanding-null-hypothesis-testing/) is an excellent article about hypothesis testing.</p></details>

-----------------------

## Date - 2023-03-03


## Title - Classification partitions


### **Question** :

Morgan works at a research lab specializing in machine learning and data analysis. She is currently writing a paper on entropy in classification and wants to ensure she accurately describes the concept to her readers. She uses a Decision Tree as the foundation of her explanations.**In this context, what does high entropy say about the partitions in a classification problem?**


### **Choices** :

- High entropy means the partitions are pure.
- High entropy means the partitions are not pure.
- High entropy means the partitions are useful.
- High entropy means the partitions are not useful.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Entropy measures how random the information being processed is. The higher the entropy, the harder it is to draw conclusions from that information.Decision Trees use a "purity" metric to split at each node. Low entropy leads to pure nodes, where 100% of the data belongs to a single partition, while high entropy leads to impure nodes, where the data is split evenly between partitions.High entropy means that the different classes are mixed and not well separated. If a partition has high entropy, the class labels are mixed or not pure. This makes it hard for a classifier to predict the class label of new samples accurately. Low entropy means that the classes are more distinct and easier to predict.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Understanding Entropy: the Golden Measurement of Machine Learning"](https://towardsdatascience.com/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3) for an introduction to entropy.</p></details>

-----------------------

## Date - 2023-03-04


## Title - Presentation slides


### **Question** :

Vanessa is preparing a conference talk on different log functions and the best approach to use each.

She finished most of the slides, but the final one needs some work: Vanessa plans to end her presentation by talking about the Log loss.

Vanessa wants to mention a few problems where the log loss is helpful.

**Which of the following problems should Vanessa include in her presentation?**


### **Choices** :

- Predict whether a dog will bark in the middle of the night.
- Determine whether a credit card transaction is invalid.
- Predict the stock price at the time the market closes.
- Predict how much rain will fall the next day.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Log loss is a function that we commonly use in classification problems. It returns the negative logarithm of the product of probabilities.

Log loss indicates how close a prediction probability is to the target value. The more the predicted probability differs from the target value, the higher the log loss.

Vanessa should only include examples of classification problems. From the list of options, predicting whether a dog will bark and determining whether a credit card transaction is valid are classification problems. The other two problems are regression problems and not a good fit for the log loss.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Understanding binary cross-entropy / log loss: A visual explanation"](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) is an excellent introduction to binary cross-entropy.
- Another article that helps understand binary cross-entropy is ["Binary Cross Entropy/Log Loss for Binary Classification"](https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/).</p></details>

-----------------------

## Date - 2023-03-05


## Title - Administrative assistant


### **Question** :

Noelle is a student who takes college classes at night while she works full-time as an administrative assistant. As part of her Machine Learning course, she is learning about Perceptrons and how to implement them from scratch.One of the class assignments involves writing and running a Perceptron by hand. For this assignment, she is given a training sample `[-2.0, 4.0]` with a true label of `0`.**Assuming the initial weights of the Perceptron are initialized with zeros, and the predicted label is `1`, what does the weight vector look like after one pass?**


### **Choices** :

- The weight vector will be `[0.0, 0.0]`.
- The weight vector will be `[-2.0, 4.0]`.
- The weight vector will be `[2.0, -4.0]`.
- The weight vector will be `[-1.0, 2.0]`.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Since the true label is `0` but the Perceptron predicted `1`, the error will be `0 - 1 = -1`, therefore, the Perceptron will update the weight vector by the negative feature vector.As a reminder, the following snippet of code illustrates how to update the weights of a Perceptron:```import numpy as npx = np.array([-2.0, 4.0])w = np.array([0.0, 0.0])error = -1w = w + error * xprint(w)```The feature vector is `[-2.0, 4.0]`, and the weights were initialized with `[0.0, 0.0]`. The update will set them to `[2.0, -4.0].`</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The ["Perceptron Algorithm for Classification in Python"](https://machinelearningmastery.com/perceptron-algorithm-for-classification-in-python/) is a great introduction to the Perceptron.</p></details>

-----------------------

## Date - 2023-03-06


## Title - Robotic facility


### **Question** :

Makayla works at a robotics facility where she processes pictures of the robots before they are shipped out.

Makayla noticed that the pictures of the robots often had different degrees of rotation and realized that the Convolutional Neural Network she had built couldn't process these images properly.

**Which of the following approaches should Makayla use to handle rotation in the pictures?**


### **Choices** :

- Add a layer at the beginning of the model that rotates the data to the correct position.
- Add a layer at the end of the model that rotates the data to the correct position.
- Include rotated versions of the images in the training data to build some rotation invariability into the model.
- Replace the Convolutional Neural Network with a Multilayer Perceptron because they are rotation invariant.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Convolutional Neural Networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) are [translation invariant but not rotation invariant](https://pyimagesearch.com/2021/05/14/are-cnns-invariant-to-translation-rotation-and-scaling/). They can recognize the same patterns independently of where they show in an image, but they don't have the same ability to recognize shapes with various degrees of rotation. Multilayer Perceptrons are not rotation invariant either.

There's also no out-of-the-box mechanism for a layer to recognize the orientation of an image and much less "rotate" it to its correct position. 

A way to train a model to recognize pictures regardless of their orientation is to extend the dataset with rotated images. If we expect to see images at 0, 90, and 180 degrees, we must teach our model to recognize them. ["Correcting Image Orientation Using Convolutional Neural Networks"](https://d4nst.github.io/2017/01/12/image-orientation/) is an excellent article covering this approach.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Correcting Image Orientation Using Convolutional Neural Networks"](https://d4nst.github.io/2017/01/12/image-orientation/) is a great article covering practical ways to get a network to recognize rotated pictures.
* ["Are CNNs invariant to translation, rotation, and scaling?"](https://pyimagesearch.com/2021/05/14/are-cnns-invariant-to-translation-rotation-and-scaling/) goes into more detail about whether convolutional neural networks are translation, rotation, and scale invariant.
* Check ["How Do Convolutional Layers Work in Deep Learning Neural Networks?"](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/) for an introduction to how convolutional layers work.</p></details>

-----------------------

## Date - 2023-03-07


## Title - Filtering features


### **Question** :

Ruth is a software engineer who has recently joined a machine learning startup. Her team is working on a project that involves building a model to classify different types of flowers based on their physical characteristics.While reviewing the code, Ruth came across the line `X[y == 1, 0]`. She knows that `X` is the feature matrix and `y` is the target vector that holds the class labels for each sample. However, she is unsure what the line `X[y == 1, 0]` does.**Assuming the code uses the Numpy library, which of the following is the correct interpretation of `X[y == 1, 0]`?**


### **Choices** :

- The code returns the first feature of every sample that belongs to class 1.
- The code returns every sample that belongs to class 1.
- The code returns the second feature of every sample that belongs to class 0.
- The code returns the label for the first sample in the dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's break down `X[y == 1, 0]`:We have `y == 1`, which will return `True` for every sample that belongs to class 1, and `False` otherwise. We use this as a selection mask on the set `X`, which will return every row that belongs to class 1.Notice, however, that we don't return every feature from `X`. Instead, we only return the first feature (index = 0.)Therefore, the code returns the first feature of every sample that belongs to class 1.We can write a simple snippet of code to illustrate this:```import numpy as npX = np.array([[2, 1], [4, 3], [6, 5], [8, 7]])y = np.array([0, 1, 1, 0])print(X[y == 1, 0])```If we run the above code snippet, we will get:```[4 6]```Notice how the result is the first feature of the second and third rows, which correspond to the samples with class 1.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check the ["NumPy quickstart"](https://numpy.org/doc/stable/user/quickstart.html) tutorial for an introduction to Numpy.</p></details>

-----------------------

## Date - 2023-03-08


## Title - AND logic gate


### **Question** :

We want to use a Perceptron to represent the AND logic gate. As a reminder, here is how the AND gate works:* 0 and 0 = 0* 0 and 1 = 0* 1 and 0 = 0* 1 and 1 = 1Our Perceptron will have two inputs, two weights, and a bias parameter. **Which of the following parameters will make our Perceptron act as an AND gate?**


### **Choices** :

- `w1 = 0.6`, `w2 = 0.6`, `b = 0.0`
- `w1 = 0.6`, `w2 = 0.6`, `b = -0.8`
- `w1 = 1.0`, `w2 = 1.0`, `b = -1.5`
- You need more than one Perceptron to represent the AND gate.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We can represent an AND gate using a single Perceptron. Here is a simple implementation with two input values, `x1` and `x2`:```def perceptron(x1, x2, w1, w2, b):    return int((x1*w1 + x2*w2 + b) > 0)```Using this function, we can try the different configurations suggested in this question. Here is an example of running the Perceptron for the AND gate using a set of parameters:```w1 = 1.0w2 = 1.0b = -1.5assert perceptron(0, 0, w1, w2, b) == 0assert perceptron(0, 1, w1, w2, b) == 0assert perceptron(1, 0, w1, w2, b) == 0assert perceptron(1, 1, w1, w2, b) == 1```Notice how each `assert` validates a specific pair of inputs. If there are no errors, then we can conclude the parameters work.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Neural Representation of AND, OR, NOT, XOR and XNOR Logic Gates (Perceptron Algorithm)"](https://medium.com/@stanleydukor/neural-representation-of-and-or-not-xor-and-xnor-logic-gates-perceptron-algorithm-b0275375fea1) for a deep dive on how to set up a Perceptron to represent multiple logic gates.</p></details>

-----------------------

## Date - 2023-03-09


## Title - Excellent book


### **Question** :

Trinity found an excellent explanation of One-Hot Encoding in one of her books.She plans to use the technique with some features of her dataset, but she wanted to do some research before pulling the trigger.**Which of the following statements do you think Trinity found in the book?**


### **Choices** :

- One-Hot Encoding creates one additional column for each possible category.
- One-Hot Encoding encodes a numerical feature into its categorical representation.
- One-Hot Encoding transforms string variables into a single integer.
- One-Hot Encoding transforms string variables using a numerical representation.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Categorical data are variables that contain label values rather than numeric values. For example, a variable representing the temperature with values "hot," "warm," and "cold" is a categorical variable.Although some algorithms can use categorical data directly, most can't: they require the data to be numeric. One-Hot Encoding is one of the techniques we can use to turn categorical data into a numerical representation.For example, assume we have a dataset with a single feature called "temperature" that could have the values "hot," "warm," and "cold." Applying One-Hot Encoding will get us a new dataset with three features, one for each value of the original "temperature" column. A sample that had the value "warm" in the previous column will now have the value `0` for both "hot" and "cold" and the value `1` under the "warm" feature.Therefore, One-Hot Encoding creates one additional column for each possible category, and these columns use a numerical representation.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Why One-Hot Encode Data in Machine Learning?"](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) for an explanation of how One-Hot Encoding works.</p></details>

-----------------------

## Date - 2023-03-10


## Title - Shelter's headquarters


### **Question** :

Kailani is working with the government to classify the shelters in the city. They are interested in organizing each location by its impact in terms of help provided versus costs.She wants to start by organizing the data and preparing the features to help build a machine-learning model to predict future shelters' impact.**Which of the following columns on Kailani's dataset are meaningful numerical features for the model?**


### **Choices** :

- The shelter's headquarters phone number.
- The shelter's tax identifier.
- The number of employees that work at the shelter's headquarters.
- The number of years the shelter has been operating.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The phone number and the tax identifier of the shelters are numeric features, but they will probably not help classify shelters. Kailani should discard these features.The number of employees and years a shelter has been operating seems like good candidates for Kailani's model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Feature Engineering for Machine Learning_](https://amzn.to/3SsnLAc) is an excellent book covering feature engineering.* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.</p></details>

-----------------------

## Date - 2023-03-11


## Title - Basketball team


### **Question** :

Elianna worked as a data scientist in the analytics department of a professional basketball team. As part of her job, she used machine learning algorithms to analyze data and predict player performance and team strategy.Elianna was well aware of the importance of the No Free Lunch Theorem in her work, so she always kept repeating it to hear teammates. **How is the No Free Lunch Theorem relevant to Elianna?**


### **Choices** :

- There is no single machine learning algorithm that is universally the best for all possible tasks.
- The performance of machine learning algorithms is independent of the specific characteristics of the data and task at hand.
- The most complex machine learning algorithm for a given task is the best option.
- The simplest machine learning algorithm for a given task is the best option.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The [No Free Lunch Theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem) states that there is no single machine learning algorithm that is universally the best for all possible tasks. This means that the performance of a machine learning algorithm depends on the specific characteristics of the data and task at hand, and there is no one-size-fits-all solution.Elianna is using machine learning algorithms to analyze data and make predictions in the context of professional basketball. She knows that the No Free Lunch Theorem means she will need to carefully consider the specific needs and challenges of the task when selecting a machine learning algorithm. This means that she should be prepared to try out different approaches and find the best solution for each problem that arises.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["No Free Lunch Theorem for Machine Learning"](https://machinelearningmastery.com/no-free-lunch-theorem-for-machine-learning/) for a complete explanation of this theorem.</p></details>

-----------------------

## Date - 2023-03-12


## Title - Changing careers


### **Question** :

Cataleya wanted to change careers. 

From working as a project manager, she decided to follow her passion and start focusing on data analysis and machine learning.

Her first opportunity was doing some time-series analysis work. Cataleya had some experience with models but needed some help to get started.

**Which of the following would be your recommendation for Cataleya?**


### **Choices** :

- Multilayer Perceptron
- Decision Trees
- Convolutional Neural Networks
- Recurrent Neural Networks


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Recurrent Neural Networks have an advantage over traditional feed-forward networks when working with time series or data points that depend upon previous samples.

The magic ingredient of Recurrent Neural Networks is the ability to store the information of previous inputs to generate the following sequence output. Recurrent Neural Networks do this by implementing the concept of a "feedback loop" or "feedback connection." These are connections feeding the hidden layers of the neural network back into themselves.

None of the other options support the concept of feedback loops. Only Recurrent Neural Networks do.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [An Introduction To Recurrent Neural Networks And The Math That Powers Them](https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/)
* [Recurrent neural network](https://en.wikipedia.org/wiki/Recurrent_neural_network)</p></details>

-----------------------

## Date - 2023-03-13


## Title - A tree that's not doing great


### **Question** :

Adelynn is a data scientist at a startup, and she has been working on building a machine-learning model to classify customers into different categories. After some investigation, Adelynn discovers that the Decision Tree model she uses is not doing great. She remembers reading about a technique called pruning that could help.**What is the purpose of pruning a Decision Tree?**


### **Choices** :

- To improve the model's ability to generalize to new data.
- To reduce the complexity of the model and make it easier to interpret.
- To improve the model's training speed and efficiency.
- To improve the model's performance on a specific task or dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Decision Trees are a popular machine-learning model for classification and regression tasks. They build a tree-like decision model based on the data's features to predict the output label of a given input sample. However, if the Decision Tree is allowed to grow too large, it can become overly complex and may start to fit the training data too closely, a phenomenon known as overfitting.Pruning is a technique that can be used to reduce the size of a Decision Tree and prevent overfitting. It involves removing nodes from the tree that are not contributing significantly to the model's accuracy. By pruning a Decision Tree, the model's ability to generalize to new data is improved, as the model is less likely to be overly complex and overfit the training set. This is the primary purpose of pruning a Decision Tree.In addition to improving the model's ability to generalize to new data, pruning a Decision Tree reduces the complexity of the model and makes it easier to interpret. By removing unnecessary nodes from the Decision Tree, the model becomes simpler and more transparent, making it easier to understand and interpret.Pruning also reduces the number of nodes and decision points the model needs to consider during training, which can improve the model's training speed and efficiency.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Decision tree pruning"](https://en.wikipedia.org/wiki/Decision_tree_pruning) covers the process of pruning a tree to reduce overfitting.* ["How to tune a Decision Tree?"](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680) is a great article talking about different ways to tune a decision tree, including the effects of setting the maximum depth.</p></details>

-----------------------

## Date - 2023-03-14


## Title - Crashing cars


### **Question** :

Palmer works for NASCAR, developing a machine-learning model to predict whether a car will crash in a race. She had access to data for thousands of races, with labels indicating whether each car crashed.After studying the available techniques, Palmer had several options for building a binary classification model. **Which of the following should be the best approach to build a binary classification model?**


### **Choices** :

- Palmer should use Reinforcement Learning.
- Palmer should use a Decision Tree, a Supervised Learning technique.
- Palmer should use k-Nearest Neighbors, a Supervised Learning technique.
- Palmer should use Unsupervised Learning.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Supervised learning is a type of machine learning where the model is trained on labeled data. In the case of Palmer's model, she could use supervised learning to train a model on the data for past races, where the data is labeled with the outcome (i.e., whether each car crashed or not). This would allow the model to learn from the data and predict future races.Either a Decision Tree or k-Nearest Neighbors would work for Palmer. Decision trees use a tree-like structure to make predictions. They are simple to understand and can be effective for binary classification tasks. K-nearest neighbors is a non-parametric technique that uses a similarity measure to classify data points based on their neighbors. It is often used for classification tasks, including binary classification.Reinforcement learning is not a good choice for Palmer's model. Reinforcement learning is a type of machine learning where the model learns by interacting with its environment and receiving feedback as rewards or penalties. This differs from binary classification, which involves making predictions based on input data.Unsupervised learning is also not a good choice for Palmer's model because is a type of machine learning where the model is trained on unlabeled data. This means that the model must find patterns and relationships in the data without any guidance from labels or outputs. In this example, Palmer has access to plenty of data and knows the target she wants to predict.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check out ["Binary classification"](https://en.wikipedia.org/wiki/Binary_classification) in Wikipedia for more information.* ["4 Types of Classification Tasks in Machine Learning"](https://machinelearningmastery.com/types-of-classification-in-machine-learning/) is a great article going over different ways to implement a classification model.</p></details>

-----------------------

## Date - 2023-03-15


## Title - Classification pseudocode


### **Question** :

Alana implemented an algorithm to classify new samples of data. Her approach uses a dataset that she collected and labeled.

Her algorithm uses a pre-determined value `N`, and does the following for every new sample Alana wants to classify:
1. She calculates the distance with every sample from the dataset.
2. She picks the `N` closest samples from the dataset.
3. She returns the mode of the labels of those `N` samples.

**Based on this description, which of the following is the technique that Alana implemented?**


### **Choices** :

- Alana implemented k-Nearest Neighbors (KNN)
- Alana implemented a Decision Tree
- Alana implemented Linear Regression
- None of the above techniques resemble Alana's implementation.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Alana's implementation resembles the way k-Nearest Neighbors (KNN) works.

KNN is an algorithm that doesn't create a predictive model from a training dataset to make predictions. In KNN, there's no need for a training phase. Instead, the algorithm computes a prediction during inference time.

KNN uses the entire dataset and looks for a pre-determined number of instances closest to the observation we want to classify to determine to which group the sample belongs. In Alana's implementation, this value is `N`. Since Alana wants to classify new samples, she can use the mode of the labels of the closest observations as the final prediction.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Understand the Fundamentals of the K-Nearest Neighbors (KNN) Algorithm"](https://heartbeat.comet.ml/understand-the-fundamentals-of-the-k-nearest-neighbors-knn-algorithm-533dc0c2f45a) for an introduction to KNN.</p></details>

-----------------------

## Date - 2023-03-16


## Title - Independent nuclear reactor


### **Question** :

Joanna is the structural engineer in charge of designing a nuclear reactor. Her team plans to build the plant next to the coast, so they need to understand the probability and effects of natural disasters.

After consulting with local experts, they know that the probability of a large earthquake in the area is 10⁻⁴, and the probability of a tsunami is also 10⁻⁴.

**What's the probability of having both an earthquake and a tsunami simultaneously, assuming both events are independent?**


### **Choices** :

- The probability of having both events simultaneously is 10⁻⁸.
- The probability of having both events simultaneously is 10⁻⁵.
- The probability of having both events simultaneously is 10⁻⁴.
- The probability of having both events simultaneously is 1.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We can compute the probability of having both an earthquake and a tsunami simultaneously using the following formula:

```
P(T and E) = P(E) * P(T|E)
```

We know the probability of an earthquake `P(E) = 10⁻⁴`, and the probability of a tsunami `P(T)` is also 10⁻⁴. Since both events are independent, then the probability of a tsunami given that an earthquake happens, is also 10⁻⁴:

```
P(T|E) = P(T) = 10⁻⁴
```

We can now substitute the values in our initial equation:

```
P(T and E) = P(E) * P(T|E)
P(T and E) = 10⁻⁴ * 10⁻⁴
P(T and E) = 10⁻⁸
```</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Cartoon Guide to Statistics_](https://amzn.to/3eg9iIo) is a fun and instructive introduction to probabilities and statistics.</p></details>

-----------------------

## Date - 2023-03-17


## Title - Advertising tensor


### **Question** :

Phoenix is a data scientist working at a marketing research firm. She wants to build a predictive model to identify which advertisements attract customer attention most.Phoenix gathered a dataset containing 100 customer responses to advertisements. 65 responses are labeled as "positive," and 35 as "negative." As Phoenix starts to build the model, she separates the features into a tensor "X" and the corresponding target values into a tensor "y."**Which of the following is the shape of the tensor "y"?**


### **Choices** :

- (35, 65)
- (65, 35)
- (100, 2)
- (100,)


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Phoenix wants to solve a binary classification problem with two classes: one representing every "positive" customer and the other representing the "negative" customers. The `y` tensor contains the target labels. The shape of this tensor depends on the number of samples in the dataset.In this scenario, there are 65 positive and 35 negative samples, so the total number of samples is 100. The shape of the tensor `y` should be (100,) with one dimension to represent the 100 samples.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Deep Learning with Python, Second Edition](https://amzn.to/3K3VZoy) covers the topic of tensors really well.* Check ["A Gentle Introduction to Tensors for Machine Learning with NumPy"](https://machinelearningmastery.com/introduction-to-tensors-for-machine-learning/) for a quick introduction to tensors.</p></details>

-----------------------

## Date - 2023-03-18


## Title - Power grid


### **Question** :

Blake is an engineer at a government organization responsible for maintaining and improving the power grid. She has been tasked with building a model to help predict the likelihood of power outages in certain areas.Blake has a dataset with the age of the power grid equipment, the number of power outages in the area, the average temperature, and the average wind speed.She builds a Perceptron and runs the forward pass using a vector containing the values of a single sample.**What is the size of the output if this vector looks like this [10, 5, 70, 10]?**


### **Choices** :

- The output will contain a single value.
- The output will contain two values.
- The output will contain three values.
- The output will contain four values.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The size of the output of a Perceptron will be a single value. A Perceptron will always output the predicted class of the sample. Regardless of how many features are in the sample, the output will always be a single value.This output comes from multiplying the input vector by the weights and passing the result through the activation function, a step function in the case of a Perceptron.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The ["Perceptron Algorithm for Classification in Python"](https://machinelearningmastery.com/perceptron-algorithm-for-classification-in-python/) is a great introduction to the Perceptron.</p></details>

-----------------------

## Date - 2023-03-19


## Title - Predicting prices


### **Question** :

Camila's first exercise in class is the Housing Price Prediction problem.She has access to a dataset with the size of thousands of houses measured in squared feet and their corresponding price. She needs to build a model that predicts the price of a house, given its size.Before working on the solution, she needs to answer an important question.**How can we classify the type of machine learning problem Camila needs to solve?**


### **Choices** :

- Supervised Learning problem.
- Regression problem.
- Unsupervised Learning problem.
- Classification problem.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>This is both a Supervised Learning and a Regression problem.A problem is considered Supervised Learning when the model is trained on a labeled dataset, where the input and output are known. This is the case for Camila's exercise. She has access to a dataset that contains features (the size of the house) and labels (the price of the house.)The goal of Camila's exercise is to predict a continuous value: the price of the house. Anytime we need to predict continuous values instead of a discrete class, we are looking at a regression model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Supervised and Unsupervised Machine Learning Algorithms"](https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/) is an excellent introduction to the differences between supervised and unsupervised learning.</p></details>

-----------------------

## Date - 2023-03-20


## Title - Your friend's model


### **Question** :

Imagine you run this experiment with a dataset:

You start by splitting your data into training and testing. Then, divide the training data into five random splits, each with 25% of the samples.

You then ask a friend to train a model on each of the five groups and test them on the testing data to determine the accuracy of each model. She then computes the variance between the performance of each model.

Here is what your friend's code looks like:

![Pseudocode](https://user-images.githubusercontent.com/1126730/199803699-e056bcea-133f-45b1-9c52-8e68d75f5bed.png)

You don't know anything about the model your friend used, but she says the result of the function above is a large value.

**What can you say about your friend's model?**


### **Choices** :

- Her model is high bias and low variance.
- Her model is high bias and high variance.
- Her model is low bias and low variance.
- Her model is low bias and high variance.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>After your friend trains each model and computes its accuracy on the testing set, a couple of things could happen:

1. The accuracy of each model is significantly different.
2. The accuracy of each model is relatively similar.

You split the training dataset into five random groups, so we should expect each of these groups to be similar. If the model's performance on each differs significantly, we know your friend is using a high-variance model.

On the other hand, if the model's performance is very similar across all four groups, we know we are looking at a stable, high-bias model that doesn't change much with new data.

The variance between the five accuracies is high, so your friend must have used a high-variance and low-bias model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) for an introduction to bias and variance.
* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).</p></details>

-----------------------

## Date - 2023-03-21


## Title - Flight delays


### **Question** :

Malia started her new job with one of the most important airlines in the country.

Her first project is to build a supervised learning model to predict flight delays. She has access to a large dataset containing information about her airline's arrival and departure flights. 

Early on, Malia discovered something interesting: a delayed flight impacts every flight that day. Any future flight on the same day will likely depart late when a delay occurs.

**Malia wants to split her dataset into a training and a test set. How would you recommend she does it?**


### **Choices** :

- Malia should use stratified splitting to ensure both sets contain the same ratio of arrival and departure flights.
- Malia should split her dataset so that flights from the same date go in the same split.
- Malia should split her dataset randomly to ensure both sets properly represent the overall dataset.
- Malia should use flight information before a specific date as her training set and any data after that as her test set.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The samples in Malia's dataset are not independent. Flights during the same day are connected, so having a delay will cascade and affect every flight after that.

If Malia splits her dataset randomly, she risks getting flights that happened on the same day on separate datasets, leading to a leaky validation strategy. Here is an excerpt from [The Kaggle Book](https://amzn.to/3kbanRb):

> In a leaky validation strategy, the problem is that you have arranged your validation strategy in a way that favors better validation scores because some information leaks from the training data.

Malia's model can use the flight date as a clue to predict delays: any flight happening after a delay during the same day will likely be late as well, which will make some of the samples from the test set simple for the model to predict.

The solution is to group flights by their date and ensure they go together into the same set.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.
* ["Target Leakage in Machine Learning"](https://www.youtube.com/watch?v=dWhdWxgt5SU) is a YouTube presentation that covers leakage, including during the partitioning of a dataset.</p></details>

-----------------------

## Date - 2023-03-22


## Title - The role of entropy


### **Question** :

Aliyah is a researcher who specializes in machine learning and data analysis. She is currently writing a paper on entropy and its role in data processing.As part of her research, Aliyah is exploring the different ways entropy can be used to understand the information being processed.**What does high entropy indicate about the information being processed in this context?**


### **Choices** :

- High entropy indicates the presence of a pattern in the information
- High entropy indicates the lack of presence of a pattern in the information
- High entropy indicates it's simple to draw conclusions from the information
- High entropy indicates the randomness in the information being processed


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Entropy is a concept used to quantify the randomness or uncertainty in a data set. In the context of information processing, entropy can be used to measure the randomness of the data being processed.A high entropy value indicates that the data being processed is random or uncertain, making it difficult to draw meaningful conclusions from the data. In contrast, a low entropy value indicates that the data is less random and more predictable, making it easier to draw conclusions from the data.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Understanding Entropy: the Golden Measurement of Machine Learning"](https://towardsdatascience.com/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3) for an introduction to entropy.</p></details>

-----------------------

## Date - 2023-03-23


## Title - Chain airport


### **Question** :

Molly works as an operations analyst at the airport. She is responsible for monitoring the performance of the airport's systems and identifying ways to improve efficiency. Molly has always been interested in machine learning and decided to take a course.One of the concepts she found most interesting in the course was the chain rule from Calculus. Molly realized it was a fundamental concept if she wanted to understand how neural networks work.**Which of the following statements is true about the chain rule in neural networks?**


### **Choices** :

- The chain rule is used to calculate the gradient of the loss function with respect to the network's weights.
- The chain rule is used to calculate the gradient of the loss function with respect to the network's inputs.
- The chain rule is used to calculate the gradient of the loss function with respect to the network's outputs.
- The chain rule is used to calculate the gradient of the loss function with respect to the layers of the network.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The chain rule in Calculus allows us to find the derivative of a composite function. In the context of neural networks, we use the chain rule as part of the backpropagation algorithm to calculate the gradient of the loss function with respect to the network's weights. This gradient tells us the direction and magnitude of the change we need to make to the weights to reduce the loss.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["The Chain Rule of Calculus for Univariate and Multivariate Functions"](https://machinelearningmastery.com/the-chain-rule-of-calculus-for-univariate-and-multivariate-functions) is a short introduction to the chain rule in machine learning.</p></details>

-----------------------

## Date - 2023-03-24


## Title - Exploratory Data Analysis


### **Question** :

Maggie is a data scientist at a company building an email system. They want to build a set of machine learning models to process spam and understand patterns from their customers.The first step in the process is for Maggie to perform Exploratory Data Analysis (EDA). She wants to give a presentation to her team about everything they will complete during this phase.**Which of the following steps does Maggie take during this process?**


### **Choices** :

- During the Exploratory Data Analysis phase, Maggie will evaluate the performance of her model.
- During the Exploratory Data Analysis phase, Maggie will monitor the performance of her model.
- During the Exploratory Data Analysis phase, Maggie will look into the distribution of her dataset's features to identify potential problems or patterns that she can use to improve her model.
- During the Exploratory Data Analysis phase, Maggie will assess the quality of her dataset, including checking for missing or corrupt values, to avoid poor model performance.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Exploratory Data Analysis is an essential step in any machine-learning project. It allows us to explore and understand our data before we build a model. During this process, we investigate the dataset to discover valuable patterns, spot any potential anomalies, and use statistics and plots to test different hypotheses and check assumptions. We also focus on understanding the distribution of the target variable and assessing the data quality, including checking for missing or corrupt values.Building a model is not part of the Exploratory Data Analysis process, so we don't evaluate the model's performance during this phase. We also don't monitor the model during this phase.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["What is Exploratory Data Analysis"](https://www.ibm.com/cloud/learn/exploratory-data-analysis) for an explanation of the EDA process.* The [_Feature Engineering for Machine Learning_](https://amzn.to/3JWZxZl) is a great book covering the entire Feature Engineering process.</p></details>

-----------------------

## Date - 2023-03-25


## Title - Twitter's data scientist


### **Question** :

Brooklynn is a data scientist at Twitter. She is working on a project to build a model to predict how well a tweet performs. She uses a deep learning model to analyze past tweets and their performance data to make predictions.Unfortunately, Brooklynn has been having trouble getting her model to perform well. She has tried different techniques and hyperparameters, but nothing seems to work.Brooklynn is starting to think that her model suffers from the exploding gradient problem.**Which of the following techniques will make Brooklynn's model more robust to the exploding gradient problem?**


### **Choices** :

- One technique that can help with the exploding gradient problem is gradient clipping, where the gradients are capped at a maximum value to prevent them from growing too large.
- Increasing the batch size can help to mitigate the exploding gradient problem by providing more data for the model to learn from.
- Weight regularization can help prevent the gradients from exploding and improve the stability of the network.
- Modifying the model architecture to include batch normalization can help. This will normalize the input to the network and prevent the values from reaching the edges where the gradients are too large.


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The Exploding Gradient Problem is an issue that occurs in training artificial neural networks when large error gradients accumulate and result in very large updates to the network weights during training. This can cause the network to become unstable and prevent it from learning effectively.One way to overcome the exploding gradient problem is to use gradient clipping, where the gradients are capped at a maximum value to prevent them from growing too large. This can help to stabilize the network and improve its performance.Another technique is weight regularization, where the network weights are constrained to prevent them from becoming too large. This can help to prevent the gradients from exploding and improve the stability of the network.Increasing the batch size is not a valid solution to the exploding gradient problem. This technique has nothing to do with the gradients becoming large and exploding.Modifying the model architecture to include batch normalization can help with the exploding gradient problem. Batch normalization gets rid of the extreme gradients that accumulate, leading to the elimination of the weight fluctuations that result from the increasing gradients.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Exploding Gradient Problem"](https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem) for an introduction to this problem.* ["A Gentle Introduction to Exploding Gradients in Neural Networks"](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/) is a great article covering Exploding Gradients in detail.</p></details>

-----------------------

## Date - 2023-03-26


## Title - Consulting firm


### **Question** :

Amina works at an HR consulting firm where she spends most days processing job descriptions. She has always been interested in using machine learning to streamline her work and make it more efficient. She has heard that transformers are a powerful type of deep learning model that can be used for natural language processing tasks.However, Amina is not very familiar with transformers and wants to know more about their fundamental characteristics before she decides whether to use them for her job.**Which of the following is not a key feature of transformers?**


### **Choices** :

- Transformers use self-attention mechanisms to weigh the input features differently.
- Transformers can process sequences of variable length in a parallel manner.
- Transformers can be trained to perform multiple tasks at once.
- Transformers use convolutional layers to process input data.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Transformers are a type of deep learning model widely used in Natural Language processing tasks, such as machine translation and text summarization. They were introduced in the 2017 paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al.One of the key characteristics of transformers is their use of attention mechanisms. Attention allows the model to focus on specific input parts, which is useful when processing long data sequences. Another characteristic of transformers is their ability to handle variable-length sequences in parallel. This is useful when dealing with natural language data, which often has varying lengths.We can train transformers to perform multiple tasks at once using a process known as multitask learning. In multitask learning, a single model is trained to perform multiple tasks simultaneously using a shared set of parameters. This allows the model to learn common features relevant to all tasks, which can improve the model's performance on each task.Transformers do not use convolutional layers to process input data. Convolutional layers work by sliding a small window, or kernel, across the input data and performing an element-wise multiplication between the values in the kernel and the values in the input data. The resulting values are then summed and passed through a nonlinear activation function before being output as the output of the convolutional layer.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check the ["Attention is All You Need"](https://arxiv.org/abs/1706.03762) paper by Vaswani et al. for an introduction to Transformers.* The [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) is a fantastic visual guide that explains the inner workings of the Transformer architecture in an easy-to-understand manner.</p></details>

-----------------------

## Date - 2023-03-27


## Title - Ensembling list


### **Question** :

Here is a list of four popular techniques that people use in machine learning. You've probably heard of them, maybe even used them.You are probably also familiar with ensembling, where we combine a group of models to produce a new model that yields better results than any initial individual models. **Can you select every technique from the list below that's related to ensembling?**


### **Choices** :

- Random Forest
- AdaBoost
- Bagging
- Boosting


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Ensembling is where we combine a group of models to produce a new model that yields better results than any initial individual models. Bagging and boosting are two popular ensemble techniques.Bagging trains a group of models in parallel and independently from each other. Each model uses a subset of the data randomly selected with replacement from the original dataset. In contrast, Boosting trains a group of learners sequentially, using the results from each model to inform which samples to use to train the next model.[Random Forest](https://en.wikipedia.org/wiki/Random_forest) is an algorithm that consists of many individual decision trees. It uses bagging to combine these trees to reach a solution much better than the one provided by any of the individual trees.[AdaBoost](https://en.wikipedia.org/wiki/AdaBoost) is another ensembling technique. It's also called Adaptive Boosting and, as the name implies, uses boosting.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check [Bagging vs. Boosting in Machine Learning: Difference Between Bagging and Boosting](https://www.upgrad.com/blog/bagging-vs-boosting/) for a detailed comparison between both techniques.* Check out ["Understanding Random Forest"](https://towardsdatascience.com/understanding-random-forest-58381e0602d2) to understand how it works and why it's effective.* [_The Kaggle Book_](https://amzn.to/3kbanRb) is an excellent source for information about ensembling techniques.</p></details>

-----------------------

## Date - 2023-03-28


## Title - The network's weights


### **Question** :

The following code trains a multilayer perceptron with a single layer to learn how to perform an OR operation. ```import torchX = torch.tensor([  [0.0, 0.0],  [0.0, 1.0],  [1.0, 0.0],  [1.0, 1.0],])y = torch.tensor([0.0, 1.0, 1.0, 1.0])learning_rate = 0.01W = torch.randn(2)for epoch in range(100):    y_hat = (X @ W).tanh()    L = torch.square(y_hat - y).sum()    print(f"Epoch {epoch} Loss: {L.data:.4}")    delta = 2 * L * (1 - torch.square(y_hat.tanh()))    # Right here, we should update the weights:    # W += ...```Notice one line is missing: the one that updates the network weights during backpropagation.**Which of the following is the correct way to update the weights?**


### **Choices** :

- `W += learning_rate * delta`
- `W += learning_rate * (X @ delta)`
- `W += learning_rate * (X.t() @ delta)`
- `W += learning_rate * torch.t(X) @ delta`


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To update the weights of the network, we need the dot product of the inputs with the value of `delta.` To do that, however, we must transpose the input `X` to perform the multiplication.We can do this in two different ways: Using the [`torch.t()`](https://pytorch.org/docs/stable/generated/torch.t.html) function or calling `t()` on the input because `X` is a tensor. This means that we can update the weights like this:```W += learning_rate * (X.t() @ delta)```Or like this:```W += learning_rate * torch.t(X) @ delta```</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Build a Simple Neural Network Using PyTorch"](https://towardsdatascience.com/build-a-simple-neural-network-using-pytorch-38c55158028d) for step-by-step directions on how to build a neural network from scratch.</p></details>

-----------------------

## Date - 2023-03-29


## Title - Writing a book


### **Question** :

Rosalie is writing a book about machine learning.One of the chapters covers Supervised Learning techniques. Rosalie knows this is important, so she wants to include some hands-on exercises for the reader.She has access to many different datasets and picks a few exciting problems with enough data for the reader to explore.**Which of the following problems should Rosalie pick to teach Supervised Learning?**


### **Choices** :

- Determine whether a website displays content for a mature audience.
- Learn the best way to split a group of car buyers into different categories based on their buying patterns.
- Given a dataset of images of circuit boards and whether they work, build an application that determines if a picture of a circuit board corresponds to a working board.
- Given a dataset of translations between English and Spanish, build an application that turns any sentence written in English into Spanish.


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>There are always multiple ways to approach these problems, but some are better for [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning), while others can benefit from [unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning).Given any website, Rosalie wants a "Yes" or "No" answer, depending on whether the site displays mature content. She could tackle this problem using a Supervised Learning technique, a binary classification algorithm. Something similar happens with classifying circuit board pictures to determine whether they work correctly.Splitting a group of car buyers into different categories requires a technique that helps Rosalie cluster buyers based on their buying patterns. We can't foresee these patterns beforehand, so any problem that involves finding out the best way of grouping samples is a good fit for clustering algorithms. [Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) is an unsupervised learning technique, so this option is unsuitable for supervised learning. Finally, translation is an interesting example because we can solve it using Supervised or Unsupervised Learning. For this particular question, Rosalie has access to labeled data, so she can frame the problem using Supervised Learning.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Supervised and Unsupervised Machine Learning Algorithms"](https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/) is an excellent introduction to the differences between supervised and unsupervised learning.</p></details>

-----------------------

## Date - 2023-03-30


## Title - Drowning in flu


### **Question** :

Charlotte's town is drowning in flu classes. 10,000 people live there, and many struggle to get a doctor's appointment. The city estimates that 5% of the population is sick.Charlotte has no symptoms, but she buys a test from the pharmacy, and the result returns positive! Reading the instructions, she discovers that the test is 90% accurate.**What do you think is Charlotte's probability of being sick?**


### **Choices** :

- Charlotte is likely sick with a 90% probability.
- Charlotte is likely sick with a 68% probability.
- Charlotte is likely healthy with a 68% probability.
- Charlotte is likely healthy with a 95% probability.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's break down the problem step by step.5% of the people that live in Charlotte's town are sick, which means that 500 individuals are sick and 9,500 are healthy. The test is 90% accurate. If all 500 sick individuals take the test, `500 * 0.9 = 450` will be correctly diagnosed as sick, but 50 will be incorrectly diagnosed as healthy. If all 9,500 healthy individuals take the test, `9,500 * 0.9 = 8,550` will be correctly diagnosed as healthy, but 950 will be incorrectly diagnosed as sick.Charlotte could be one of those 450 people that are sick and had a positive test, but she could also be one of the 950 healthy with an incorrect positive result. To determine the probability of Charlotte being sick, we can compute how likely she is to be in each group.Out of `450 + 950 = 1,400` positive results, Charlotte is `450 / 1,400 = 32%` likely to be sick and `950 / 1,400 = 68%` likely to be healthy.Despite what Charlotte's test suggests, she is likely to be healthy.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Cartoon Guide to Statistics_](https://amzn.to/3eg9iIo) is a fun and instructive introduction to probabilities and statistics.</p></details>

-----------------------

## Date - 2023-03-31


## Title - Game of chicken


### **Question** :

In the game of chicken, two players drive toward each other on a collision course. If both players swerve, they each receive a small reward. If one player swerves and the other doesn't, the swerving player receives a moderate reward while the other player receives a large reward. If both players refuse to swerve, they crash and receive a large negative reward. **Which of the following is the Nash equilibrium in this game?**


### **Choices** :

- Both players swerve, resulting in both players receiving a small reward.
- One player swerves, and the other doesn't, resulting in the swerving player receiving a moderate reward and the other player receiving a large reward.
- Both players refuse to swerve, resulting in a crash and a large negative reward.
- Neither of these options results in a Nash equilibrium.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A Nash equilibrium is a state in which all players in a game have made their best decisions given the decisions of the other players. In this game, the Nash equilibrium is when one player swerves while the other stays its course. In this case, each player will receive the highest reward possible, given the other player's decision.If both players refuse to swerve, they will crash and obtain the lowest reward. If both swerve, they will obtain a small reward. But whenever one stays the course and the other swerves, they will receive a better reward.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["An Introduction to the Nash Equilibrium"](https://builtin.com/data-science/nash-equilibrium) for a great explanation of one of the cornerstones of Game Theory.</p></details>

-----------------------

## Date - 2023-04-01


## Title - Weights vector


### **Question** :

Harmony is a computer science student, and during her free time decided to watch a video about a Perceptron.When the video finished, Harmony tried to reproduce everything she learned by implementing it using Python, but as soon as she started coding, she realized she had forgotten how to set up the weights vector.**Which option should Harmony choose to set up the number of weights in her Perceptron correctly?**


### **Choices** :

- Harmony should create as many weights as samples in her dataset.
- Harmony should create a random number of weights.
- Harmony should create as many weights as features in her dataset.
- Harmony should create as many weights as hidden layers in her implementation.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A perceptron is a machine learning algorithm that takes a set of inputs, known as features, and uses a set of weights to predict the class of a data point. The number of weights a perceptron uses equals the number of features.Each weight in the perceptron represents a feature in the data. The weights are used to compute a weighted sum of the inputs, which is then passed through an activation function to produce a prediction. The activation function is typically a step function that returns 0 or 1, indicating whether the sample belongs to one class or the other.Using a different weight for each feature, the perceptron captures the different relationships between the features and the target. We can learn the optimal weights that allow the model to make accurate predictions by training a perceptron.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The ["Perceptron Algorithm for Classification in Python"](https://machinelearningmastery.com/perceptron-algorithm-for-classification-in-python/) is a great introduction to the Perceptron.</p></details>

-----------------------

## Date - 2023-04-02


## Title - Supervised description


### **Question** :

Supervised learning, also known as supervised machine learning, is a subcategory of machine learning and artificial intelligence.Supervised learning is one of the most popular ways to build machine learning models.**Which of the following is the correct description of Supervised learning?**


### **Choices** :

- Supervised learning is a type of machine learning where the model is not given any labeled training data. Instead, the model is only given a set of unlabeled data and is expected to discover the relationships and patterns within the data on its own.
- Supervised learning is a type of machine learning where the model is trained on labeled data, which consists of input data and corresponding correct output labels. The model can then make predictions on new, unseen data by using the patterns it learned from the training data.
- Supervised learning is a type of machine learning that involves training an agent to take action in an environment to maximize a reward. This is done through a process of trial and error, where the agent receives feedback in the form of rewards or punishments based on its actions and uses this feedback to adjust its behavior and improve its performance over time.
- Supervised learning is a type of machine learning where the training data consists of a mixture of labeled and unlabeled examples. It leverages the availability of labeled examples to improve the model's performance while still making use of the vast amount of unlabeled data that is often available.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Supervised learning is a type of machine learning where the model is trained on labeled data, which consists of input data and corresponding correct output labels. The model can then make predictions on new, unseen data by using the patterns it learned from the training data.Unsupervised learning is a type of machine learning where the model is not given any labeled training data. Instead, the model is only given a set of unlabeled data and is expected to discover the relationships and patterns within the data on its own.Reinforcement learning is a type of machine learning that involves training an agent to take action in an environment to maximize a reward. This is done through a process of trial and error, where the agent receives feedback in the form of rewards or punishments based on its actions and uses this feedback to adjust its behavior and improve its performance over time.Finally, Semi-supervised learning is a type of machine learning where the training data consists of a mixture of labeled and unlabeled examples. It leverages the availability of labeled examples to improve the model's performance while still making use of the vast amount of unlabeled data that is often available.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Supervised and Unsupervised Machine Learning Algorithms"](https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/) is an excellent introduction to the differences between supervised and unsupervised learning.* ["What is reinforcement learning? The complete guide"](https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide/) covers an introduction to Reinforcement learning.* ["What Is Semi-Supervised Learning"](https://machinelearningmastery.com/what-is-semi-supervised-learning/) covers Semi-supervised learning.</p></details>

-----------------------

## Date - 2023-04-03


## Title - Complex architecture


### **Question** :

Talia is working on a machine-learning model for her thesis. She has been building a deep neural network that uses a very complex architecture.The problem is that her model is not generalizing well. Despite having a low error rate on the training set, the error rate on the validation set is much higher.Talia knows that her model is suffering from overfitting. She needs to find a way to improve her model's performance on the validation set.**Which of the following techniques will help Talia fix her model's overfitting problem?**


### **Choices** :

- Talia should try using a smaller network architecture.
- Talia should add more training data.
- Talia should increase the learning rate.
- Talia should add L1 and L2 regularization to the cost function.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Overfitting is a common problem in machine learning, where a model performs well on the training data but fails to generalize to new examples.One way to fix overfitting is to use a smaller network architecture. If the model is too complex, it will be able to learn the noise in the training data, and this will prevent it from generalizing well. By simplifying the network architecture, we will reduce the model's capacity, and it will be less likely to overfit.Another way to fix overfitting is by adding more training data. With more data, the model will be able to learn the true underlying pattern of the problem, and this will reduce overfitting.Increasing the learning rate will not fix overfitting. If we use a high learning rate, the optimization algorithm will make large weight updates, which won't help the model generalize.Finally, L1 and L2 regularization are ways to constrain the complexity of the model. These techniques add a penalty term to the cost function to prevent the model from fitting the noise in the training data.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.</p></details>

-----------------------

## Date - 2023-04-04


## Title - Candy observations


### **Question** :

Stevie works for a candy manufacturer and wants to build a machine-learning model to predict the best price for every new product they release.She has access to a large dataset with similar products in the market, but since every product is different, she applies a log transformation to the target column before training the model.**If Stevie's model for a single observation is `y = f(x)`, what are `x` and `y`?**


### **Choices** :

- `y` is a feature vector containing the variables that describe the product, and `x` is the product's price.
- `y` is a feature vector containing the variables that describe the product, and `x` is the logarithm of the product's price.
- `x` is a feature vector containing the variables that describe the product, and `y` is the product's price.
- `x` is a feature vector containing the variables that describe the product, and `y` is the logarithm of the product's price.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We can represent Stevie's model as a function `y = f(x)`.This function has two components: the function's output —the `y` value— and the function's input —the `x` value.The result of this function—`y`—is what Stevie wants to predict: the product's price. On the other hand, `x` is a feature vector containing all the variables that describe the product. An important detail: Stevie applied a log transformation to the product's price before training the model. This means the model will not output the price directly but a logarithm of the product's price.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Log Transformation: Purpose and Interpretation"](https://medium.com/@kyawsawhtoon/log-transformation-purpose-and-interpretation-9444b4b049c9) for an introduction to log transformation.* The [_Machine Learning Bookcamp_](https://amzn.to/3hCGbgo) is a great book that discusses the topic.</p></details>

-----------------------

## Date - 2023-04-05


## Title - The ice cream shop


### **Question** :

Bethany has been running her ice cream company for the last five years and currently owns half the market of her town. Brennen, her competitor, owns the other half.Bethany wants to get ahead and is thinking of reducing prices to steal around 20% of Brennen's customers. But if Brennen follows suit, they will both be back in the same place but now making lower profits.You can assume that every potential customer is already buying ice cream from either Bethany or Brennen, so the situation is a zero-sum game.**Which of the following do you think is Brennen's best strategy?**


### **Choices** :

- Brennen should be the first to reduce prices. Then never do it again regardless of what Bethany does.
- Brennen should not reduce prices regardless of what Bethany does.
- Brennen should not reduce prices unless Bethany does it first. Then start matching whatever Bethany does.
- Brennen should be the first to reduce prices. Then start matching whatever Bethany does.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>This problem is an example of the [prisoner's dilemma](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma), a classic from game theory. Let's analyze the situation carefully.If Brennen reduces his prices and Bethany doesn't, Brennan will emerge the winner. The extra market share will make up for the profit reduction per person. Of course, it's improbable that Bethany doesn't do anything, which will take them back to square one, but now each will be making less money—they will both be charging less with the same amount of customers.If Bethany goes first and Brennen doesn't follow suit, Bethany wins, and Brennen loses. Brennan will be forced to charge less money if Bethany goes first, or else he stands to lose the most. Therefore, the second choice is not Brennen's best strategy.The best outcome for both of them is to do nothing! If neither reduces prices, they will keep the same market share but profit more. Brennen should consider this and start by cooperating and letting Bethany take the first step. If she reduces her prices, Brennen will have no choice but to follow, and they will lose some money. But if she doesn't do anything, they will both enjoy the status quo.Therefore, the best strategy is for Brennen to wait for Bethany—he shouldn't reduce his prices—and always match what she does. They stand to win the most by always cooperating, but if she defects and reduces her prices, Brennen should do the same.This strategy is called ["tit for tat"](https://en.wikipedia.org/wiki/Tit_for_tat) and is the most straightforward and successful in playing prisoner's dilemma. Here is an excerpt from [Wikipedia](https://en.wikipedia.org/wiki/Tit_for_tat):> An agent using this strategy will first cooperate, then subsequently replicate an opponent's previous action. If the opponent previously was cooperative, the agent is cooperative. If not, the agent is not.In summary, the third choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Since this problem illustrates an economic scenario where Prisoner's Dilemma applies, ["The Prisoner's Dilemma in Business and the Economy"](https://www.investopedia.com/articles/investing/110513/utilizing-prisoners-dilemma-business-and-economy.asp) is a perfect introduction with a few interesting day-to-day examples.* The ["Prisoner's Dilemma"](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma) Wikipedia page is a good reference. You can also find here the strategy for the iterated prisoner's dilemma, which is precisely what this problem is about.* The ["Tit for tat"](https://en.wikipedia.org/wiki/Tit_for_tat) Wikipedia page comes with a full explanation of why this is the best strategy to maximize your payoff.</p></details>

-----------------------

## Date - 2023-04-06


## Title - Type II


### **Question** :

In statistics, the notion of a statistical error is integral to hypothesis testing. When testing the null hypothesis, there are two types of errors: **type I** and **type II**.Look at this confusion matrix of a hypothetical machine learning model classifying spam emails. The "P" stands for "Positive" samples (spam), and the "N" stands for "Negative" samples (not spam.) Our default assumption is that emails are not spam.![Confusion Matrix](https://user-images.githubusercontent.com/1126730/188163753-6424565d-3c8c-421e-95fb-7d89aba1453a.jpg)**How many type II errors does this model have?**


### **Choices** :

- There are 75 type II errors.
- There are 22 type II errors.
- There are 12 type II errors.
- There are 121 type II errors.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Type I errors are the same as **false positives**. For example, if we mark a valid email as spam, we are in the presence of a false positive. Type I errors are the rejection of a true [null hypothesis](https://www.investopedia.com/terms/n/null_hypothesis.asp) by mistake.On the other hand, Type II errors are the same as **false negatives**. For example, if we let a spam message pass as a valid email, we are in the presence of a false negative. This is a type II error because we accept the conclusion of the email being good, even though it is incorrect. Type II errors are the acceptance of a false null hypothesis by mistake.Therefore, there are a total of 22 type II errors in this hypothetical model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.* Check out ["Type I and type II errors"](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors) for the definition and examples of each type of error. * ["Understanding Null Hypothesis Testing"](https://opentextbc.ca/researchmethods/chapter/understanding-null-hypothesis-testing/) is an excellent article about hypothesis testing.</p></details>

-----------------------

## Date - 2023-04-07


## Title - Local professor


### **Question** :

Zuri is a local university professor teaching a Machine Learning course. She is covering Decision Trees in her course and wants to ensure that her students understand entropy and its role in the classification problem.**In the context of Decision Trees and entropy, what does low entropy indicate about the partitions in a classification problem?**


### **Choices** :

- Low entropy means the partitions are pure.
- Low entropy means the partitions are not pure.
- Low entropy means the partitions are useful.
- Low entropy means the partitions are not useful.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Entropy measures how random the information being processed is. The higher the entropy, the harder it is to draw conclusions from that information.Decision Trees use a "purity" metric to split at each node. Low entropy leads to pure nodes, where 100% of the data belongs to a single partition, while high entropy leads to impure nodes, where the data is split evenly between partitions.Low entropy means that the different classes are distinct and easier to predict, while high entropy means that the different classes are mixed and not well separated. If a partition has low entropy, the class labels are pure.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Understanding Entropy: the Golden Measurement of Machine Learning"](https://towardsdatascience.com/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3) for an introduction to entropy.</p></details>

-----------------------

## Date - 2023-04-08


## Title - What the book says


### **Question** :

Mariana wants to set up the correct loss function for her model.She read that she should focus on the average positive difference between her model's predictions and the target values.But that's what the book says. Now it's up to Mariana to translate that into technical terms.**Which of the following is the correct error measure that Mariana needs?**


### **Choices** :

- Mean Squared Error (MSE)
- Mean Absolute Error (MAE)
- Mean Positive Error (MPE)
- Root Mean Squared Error (RMSE)


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Mariana needs to use the Mean Absolute Error (MAE.) The book she read recommended the "average positive difference between predictions and target values." MAE will give Mariana the average magnitude of the errors without considering their direction, which is what "positive difference" means.Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) also ignore the direction of the error, but they square the values, so they don't simply return the average difference.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Root-mean-square deviation"](https://en.wikipedia.org/wiki/Root-mean-square_deviation) is the Wikipedia page covering RMSE.* ["Mean absolute error"](https://en.wikipedia.org/wiki/Mean_absolute_error) is the Wikipedia page covering MAE.* ["RMSE vs MAE, which should I use?"](https://stephenallwright.com/rmse-vs-mae/) is a great summary by Stephen Allwright about the properties of these two functions and how you should think about them.</p></details>

-----------------------

## Date - 2023-04-09


## Title - National security


### **Question** :

Selena works for the government as a data scientist, focusing on national security. She uses neural networks to classify military vehicles as part of their work.To help bring new team members up to speed, Selena organized a class around Perceptrons. In one of her examples, Selena gave the team the following context:* The Perceptron's weights were initialized as `[0.0, 0.0]`* A training sample with value `[1.0, 2.0]` was ran through the Perceptron* The true label associated with the sample was `0`* The Perceptron returned `1` as its prediction**What does the weight vector look like after the Perceptron updates the weights?**


### **Choices** :

- The weight vector will be `[0.0, 0.0]`.
- The weight vector will be `[-1.0, -2.0]`.
- The weight vector will be `[1.0, 2.0]`.
- The weight vector will be `[2.0, 4.0]`.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Since the true label is `0` but the Perceptron predicted `1`, the error will be `0 - 1 = -1`, therefore, the Perceptron will update the weight vector by the negative feature vector.As a reminder, the following snippet of code illustrates how to update the weights of a Perceptron:```import numpy as npx = np.array([1.0, 2.0])w = np.array([0.0, 0.0])error = -1w = w + error * xprint(w)```The feature vector is `[1.0, 2.0]`, and the weights were initialized with `[0.0, 0.0]`. The update will set them to `[-1.0, -2.0].`</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The ["Perceptron Algorithm for Classification in Python"](https://machinelearningmastery.com/perceptron-algorithm-for-classification-in-python/) is a great introduction to the Perceptron.</p></details>

-----------------------

## Date - 2023-04-10


## Title - The right size


### **Question** :

Charlee, a computer vision specialist working for an auto manufacturer, is frustrated because she can't seem to get the output size of her convolutional layers right.She is designing a convolutional neural network, and the first layer operates on grayscale images of size 32×32. Charlee is using a convolutional layer with a kernel of size 5, no padding, and a stride of 2.**What is the size of the output of the convolutional layer?**


### **Choices** :

- The output size is 32×32
- The output size is 28×28
- The output size is 14×14
- The output size is 5×5


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To compute the output size, we can use the following formula:```output = 1 + (input + 2 * padding - kernel) / stride```We can also analyze the problem step by step.Convolutions have many parameters that influence the output size, so let's go through the properties of the convolutional layer one by one.First, we know the input size to be 32×32.Next, we know that the kernel size is 5. This means we will move a sliding window of size 5×5 over the entire image. The window must fit completely in the image, so we cannot compute the convolution for the two rows and columns on the border. Therefore, the resulting output so far would be 28×28.Adding padding to the image is a way to avoid the reduction of the size, but Charlee isn't using any padding here.Finally, we have a stride of 2. This means the sliding window will move by 2 rows/columns every time, reducing each output dimension's size by half.Therefore, the final output size of the convolutional layer will be 14×14.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* It may be difficult to imagine why some of these things happen just by reading the text, so I encourage you to go to this ["Convolution Visualizer"](https://ezyang.github.io/convolution-visualizer/) and enter the parameters of the convolutional layer from the question. It should be much clearer now why this happens.* ["Convolution arithmetic"](https://github.com/vdumoulin/conv_arithmetic) is a technical report on convolution arithmetic in the context of deep learning.</p></details>

-----------------------

## Date - 2023-04-11


## Title - Educational platform


### **Question** :

Kali works at a company that runs an educational platform. She is working on a project to classify student performance data. Her team needs to handle a few categorical features in the dataset. One team member suggested using Label encoding, while another advocated for One-Hot encoding.Kali must decide, so she asked both colleagues to write a summary of both approaches.**Here are some of the highlights, but they are contradictory. Please, select those you think are correct.**


### **Choices** :

- One-Hot encoding replaces each label from the categorical feature with a unique integer based on alphabetical ordering.
- One-Hot encoding creates additional features based on the number of unique values in the categorical feature.
- Label encoding replaces each label from the categorical feature with a unique integer based on alphabetical ordering.
- Label encoding creates additional features based on the number of unique values in the categorical feature.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Before analyzing this question, we must understand what "categorical data" means.Categorical data are variables that contain label values rather than numeric values. For example, a variable representing the weather with values "sunny," "cloudy," and "rainy" is a categorical variable.Although some algorithms can use categorical data directly, most can't: they require the data to be numeric. We can use One-Hot or Label encoding to do this.[One-Hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f) creates a new feature for each unique value of the original categorical variable.For example, assume we have a dataset with a single feature called "weather" that could have the values "sunny," "cloudy," and "rainy." Applying One-Hot Encoding will get us a new dataset with three features, one for each value of the original "weather" column. A sample that had the value "cloudy" in the previous column will now have the value 0 for both "sunny" and "rainy" and the value 1 under the "cloudy" feature.On the other hand, [Label encoding](https://www.mygreatlearning.com/blog/label-encoding-in-python/) replaces each categorical value with a consecutive number starting from 0. For example, Label Encoding would replace our weather feature with a new one containing the values 0 instead of "cloudy," 1 instead of "rainy," and 2 instead of "sunny."</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What is One Hot Encoding? Why and When Do You Have to Use it?"](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f) is an excellent introduction to One-Hot encoding.* ["Label Encoding in Python Explained"](https://www.mygreatlearning.com/blog/label-encoding-in-python/) is an introduction to Label encoding.* ["One-Hot Encoding vs. Label Encoding using Scikit-Learn"](https://www.analyticsvidhya.com/blog/2020/03/one-hot-encoding-vs-label-encoding-using-scikit-learn/) covers both techniques and when to use each one.</p></details>

-----------------------

## Date - 2023-04-12


## Title - Feature matrix


### **Question** :

Taylor is a software engineer who has recently joined a machine learning startup. Her team is working on a project that involves building a model to classify different types of products based on customer feedback.While reviewing the code, Taylor came across the line `X[y == 0, 1]`. She knows that `X` is the feature matrix and `y` is the target vector that holds the class labels for each sample. However, she is unsure what the line `X[y == 0, 1]` does.**Assuming the code uses the Numpy library, which of the following is the correct interpretation of `X[y == 0, 1]`?**


### **Choices** :

- The code returns every sample that belongs to class 0.
- The code returns the first feature of every sample that belongs to class 1.
- The code returns the second feature of every sample that belongs to class 0.
- The code returns the label for the first sample in the dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's break down `X[y == 0, 1]`:We have `y == 0`, which will return `True` for every sample that belongs to class 0, and `False` otherwise. We use this as a selection mask on the set `X`, which will return every row that belongs to class 0.Notice, however, that we don't return every feature from `X`. Instead, we only return the second feature (index = 1.)Therefore, the code returns the second feature of every sample that belongs to class 0.We can write a simple snippet of code to illustrate this:```import numpy as npX = np.array([[2, 1], [4, 3], [6, 5], [8, 7]])y = np.array([0, 1, 1, 0])print(X[y == 0, 1])```If we run the above code snippet, we will get:```[1 7]```Notice how the result is the second feature of the first and fourth rows, which correspond to the samples with class 0.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check the ["NumPy quickstart"](https://numpy.org/doc/stable/user/quickstart.html) tutorial for an introduction to Numpy.</p></details>

-----------------------

## Date - 2023-04-13


## Title - OR logic gate


### **Question** :

We want to use a Perceptron to represent the OR logic gate. As a reminder, here is how the OR gate works:* 0 or 0 = 0* 0 or 1 = 1* 1 or 0 = 1* 1 or 1 = 1Our Perceptron will have two inputs, two weights, and a bias parameter. **Which of the following parameters will make our Perceptron act as an OR gate?**


### **Choices** :

- `w1 = 0.6`, `w2 = 0.6`, `b = 0.0`
- `w1 = 0.5`, `w2 = 0.5`, `b = -0.1`
- `w1 = 1.0`, `w2 = 1.0`, `b = -1.5`
- You need more than one Perceptron to represent the OR gate.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We can represent an OR gate using a single Perceptron. Here is a simple implementation with two input values, `x1` and `x2`:```def perceptron(x1, x2, w1, w2, b):    return int((x1*w1 + x2*w2 + b) > 0)```Using this function, we can try the different configurations suggested in this question. Here is an example of running the Perceptron for the OR gate using a set of parameters:```w1 = 0.6w2 = 0.6b = 0.0assert perceptron(0, 0, w1, w2, b) == 0assert perceptron(0, 1, w1, w2, b) == 1assert perceptron(1, 0, w1, w2, b) == 1assert perceptron(1, 1, w1, w2, b) == 1```Notice how each `assert` validates a specific pair of inputs. If there are no errors, then we can conclude the parameters work.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Neural Representation of AND, OR, NOT, XOR and XNOR Logic Gates (Perceptron Algorithm)"](https://medium.com/@stanleydukor/neural-representation-of-and-or-not-xor-and-xnor-logic-gates-perceptron-algorithm-b0275375fea1) for a deep dive on how to set up a Perceptron to represent multiple logic gates.</p></details>

-----------------------

## Date - 2023-04-14


## Title - Electric substations


### **Question** :

Gia is part of a team building a Machine Learning model to control electric substations. The team recently attended a training session to learn how to prepare the data for model training.Although the session covered the importance of splitting the data into train, validation, and test sets, it didn't explain why this is necessary.Gia wants to understand the reasoning behind splitting the data.**What are the reasons for splitting a dataset before training a Machine Learning model?**


### **Choices** :

- To prevent underfitting of the model
- To prevent overfitting of the model
- To make the training process faster
- To accurately evaluate the model's performance


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's talk about school for a minute.Imagine teaching a math class, and it's time to evaluate your students. You decide to leave them 100 exercises as their homework. These problems cover the content they need to master to ace the exam.How can you design an exam that effectively identifies those who learned the material?Let's assume you pick 20 of the same homework exercises and use them in your test. This strategy might result in false positives: students who memorize the solutions to their homework may get a high score, although they don't necessarily know how to reason. In machine learning, we call this "overfitting."To ensure students don't overfit to their training exercises, you don't want to use the same homework to test their knowledge. Instead, you want to find new problems that evaluate the same material but are different enough to force the students to show their skills.We want to do the same when training machine learning models. If we only evaluate our work in the same data we use to train the model, we might overfit and have a model that isn't capable of generalizing to different data. In other words, the model may "memorize" the training data and learn to return excellent predictions when tested.If we split the dataset and leave a portion to evaluate how much the model learned, we will ensure that overfit won't happen. Therefore, the second and fourth choices are correct: we can accurately assess the model's performance and avoid overfitting.Getting back to the previous analogy, those students that can't solve the homework in the first place are underfitting. Underfitting happens when the model cannot learn the training data, so we don't need separate splits to detect underfitting. The training data is enough, so the first choice is not correct.Finally, we don't split the data to improve the training process speed; we do it to evaluate its performance accurately.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.* ["Train, Validation, Test Split for Machine Learning"](https://blog.roboflow.com/train-test-split/) goes into detail about the importance of each split.</p></details>

-----------------------

## Date - 2023-04-15


## Title - No Free Lunch


### **Question** :

The No Free Lunch Theorem argues that, without having substantive information about the modeling problem, there is no single model that will always do better than any other model. Because of this, a strong case can be made to try various techniques, then determine which model to focus on.**What do you think about the statement above?**


### **Choices** :

- The statement is true because no single algorithm is universally the best for all possible tasks.
- The statement is true because there's always an algorithm that will do better than every other algorithm on any possible task.
- The statement is false because the most straightforward algorithm for a given task is usually the best option.
- The statement is false because the most flexible algorithm for a given task is usually the best option.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The [No Free Lunch Theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem) states that no single machine learning algorithm is universally the best for all possible tasks. This means that the performance of a machine learning algorithm depends on the specific characteristics of the data and job at hand, and there is no one-size-fits-all solution.This means that the statement presented on this problem is true.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["No Free Lunch Theorem for Machine Learning"](https://machinelearningmastery.com/no-free-lunch-theorem-for-machine-learning/) for a complete explanation of this theorem.</p></details>

-----------------------

## Date - 2023-04-16


## Title - Future sales


### **Question** :

Julia is working with a retail company to classify the stores in the city. They are interested in organizing each location by its sales performance.

She wants to start by organizing the data and preparing the features to help build a machine-learning model to predict future stores' sales performance.

**Which of the following columns on Julia's dataset are meaningful numerical features for the model?**


### **Choices** :

- The store's square footage.
- The store's street address.
- The number of employees working at the store.
- The store's phone number.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The store's square footage and the number of employees working at the store are meaningful numerical features for the model.

Street addresses and phone numbers are not meaningful numerical features for this problem because they do not represent useful information about the store's sales performance. 

The store's square footage and the number of employees, on the other hand, may be useful for predicting the store's sales performance because they can potentially indicate the size and resources available at the store. 

It's important to carefully select and engineer the features used in a machine learning model to ensure that they are relevant and can help the model make accurate predictions.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Feature Engineering for Machine Learning_](https://amzn.to/3SsnLAc) is an excellent book covering feature engineering.
* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.</p></details>

-----------------------

## Date - 2023-04-17


## Title - Defective cell phones


### **Question** :

Camille works at a company that produces cell phones, and her team wants to classify defects into three categories: visual scratches, software bugs, and hardware defects.Every cell phone could have one or more of these defects.Camille's team is using a convolutional neural network to solve this problem, and she is trying to determine the best way to design it.**What would be the best approach for Camille's team to design the network to classify the defects?**


### **Choices** :

- The output layer of the network should have a softmax activation function, and the loss function should be categorical cross-entropy.
- The output layer of the network should have a sigmoid activation function, and the loss function should be binary cross-entropy.
- The output layer of the network should have a softmax activation function, and the loss function should be binary cross-entropy.
- The output layer of the network should have a sigmoid activation function, and the loss function should be categorical cross-entropy.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The team is trying to build a [multi-label classification](https://en.wikipedia.org/wiki/Multi-label_classification) model. In multi-label classification, every cell phone might show multiple defects. This differs from [multi-class classification](https://en.wikipedia.org/wiki/Multiclass_classification), where a photo shows only one kind of defect.When building multi-label classification models, we need an output layer where every class is independent. Remember that we can have more than one active class for each input. The softmax activation function doesn't work because it uses every score to output the probabilities of each class. Softmax is the correct output for multi-class classification. Sigmoid is the correct output for multi-label classification problems. The sigmoid function converts output scores to a value between 0 and 1, independently of all the other scores.Multi-label classification problems borrow the same principles from binary classification problems. The difference is that we end up with multiple sigmoid outputs instead of one. In our example problem, we combine three different binary classifiers. This is why we should use a binary cross-entropy as the loss function.In summary, multi-class classification models should use a softmax output with the categorical cross-entropy loss function. Multi-label classification models should use a sigmoid output and the binary cross-entropy loss function.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The Wikipedia explanation of [Multi-label classification](https://en.wikipedia.org/wiki/Multi-label_classification) should give you most of what you need to understand for this task.* ["Demystifying the Difference Between Multi-Class and Multi-Label Classification Problem Statements in Deep Learning"](https://www.analyticsvidhya.com/blog/2021/07/demystifying-the-difference-between-multi-class-and-multi-label-classification-problem-statements-in-deep-learning/) is an excellent article comparing these two types of problems.* ["How to choose cross-entropy loss function in Keras?"](https://androidkt.com/choose-cross-entropy-loss-function-in-keras/) explains the differences between the loss functions that we discussed in this question.</p></details>

-----------------------

## Date - 2023-04-18


## Title - College applications


### **Question** :

Noa is a newly graduated data scientist who works for a school. She is tasked with developing a machine-learning model to predict what college their students will want to apply to at the end of the year.Noa has access to every grade from every previous student, including labels indicating the college they went to.She has several options for building a classification model. **Which of the following should be the best approach to build that model?**


### **Choices** :

- Noa should use a Decision Tree, a Supervised Learning technique.
- Noa should use Linear Regression, a Supervised Learning technique.
- Noa should use Reinforcement Learning.
- Noa should use Unsupervised Learning.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Supervised learning is a type of machine learning where the model is trained on labeled data. In the case of Palmer's model, she could use supervised learning to train a model on the data for past races, where the data is labeled with the outcome (i.e., whether each car crashed or not). This would allow the model to learn from the data and predict future races.A Decision Tree should work for Palmer. Decision trees use a tree-like structure to make predictions. They are simple to understand and can be effective for classification tasks. Linear regression is not a good option for classification tasks. Linear regression is a model for predicting a continuous numerical output based on input data, while classification involves predicting a discrete class or category based on input data. Reinforcement learning is not a good choice for Noa's model. Reinforcement learning is a type of machine learning where the model learns by interacting with its environment and receiving feedback as rewards or penalties. This differs from classification, which involves making predictions based on input data.Unsupervised learning is also not a good choice for Noa's model either because is a type of machine learning where the model is trained on unlabeled data. This means that the model must find patterns and relationships in the data without guidance from labels or outputs. In this example, Noa has access to plenty of data and knows the target she wants to predict.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check out ["Binary classification"](https://en.wikipedia.org/wiki/Binary_classification) in Wikipedia for more information.* ["4 Types of Classification Tasks in Machine Learning"](https://machinelearningmastery.com/types-of-classification-in-machine-learning/) is a great article going over different ways to implement a classification model.</p></details>

-----------------------

## Date - 2023-04-19


## Title - Model coefficients


### **Question** :

Kailani is a data scientist at a hospital and has developed a model that takes in patient inputs and predicts if a patient has a fatal disease. She wants to present her model's results to her boss. Unfortunately, he is not technical, so Kailani must communicate clearly.**Which of the following options represents the best way for Kailani to explain how the coefficients of her model work?**


### **Choices** :

- Positive coefficient indicates the variable is more likely to influence the outcome positively.
- Positive coefficient indicates the variable is more likely to influence the outcome negatively.
- Positive coefficient indicates the variable does not influence the outcome.
- Negative coefficient indicates the variable does not influence the outcome.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The coefficients in the context of a model are the parameters learned by the model during training. They determine the relationship between the input features and the output predictions.In the case of Kailani's model, the coefficients are used to predict whether a patient has a fatal disease. The coefficients weigh the input features and determine the final prediction.When Kailani explains the coefficients to her boss, she wants to ensure he understands how they influence the final prediction.The best way to explain this is by saying that a positive coefficient indicates that the variable is more likely to influence the outcome positively. This means that the variable has a positive relationship with the target variable and increases the probability of the target being positive.On the other hand, a negative coefficient indicates that the variable has a higher likelihood of negatively influencing the outcome. This means that the variable has a negative relationship with the target variable and decreases the probability of the target being positive.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems_](https://amzn.to/3SHGqsu) is one of the best books for machine learning fundamentals.</p></details>

-----------------------

## Date - 2023-04-20


## Title - Financial performance


### **Question** :

Charlee is a data scientist in the finance department of a startup. She is working on a project to build a machine-learning model that predicts stock prices for the company. Charlee knows her model's success will significantly impact the company's financial performance.Before she trains her model, Charlee must divide her dataset into three sets: a training set, a validation set, and a test set. She knows that using each set for specific purposes is essential to ensure accurate results.**Which of the following statements are true about the training and validation datasets during the development process?**


### **Choices** :

- The training dataset should be used only once before testing the model with the test set.
- The training dataset can be used multiple times throughout the model development process.
- The validation set should be used only once before testing the model with the test set.
- The validation set can be used multiple times throughout the model development process.


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The training and validation sets can be used multiple times throughout the model development process.We use a training set to train the model. This training set is a subset of the entire dataset. We use the training set multiple times throughout the model development process as the model is refined and improved.We use a validation set to tune the model's parameters and assess its performance to prevent overfitting. We evaluate the model's performance after each iteration of training.After we finish training and tuning the model, we can evaluate it on a test set, which hasn't been used during the training or validation process. The model's performance on the test set provides an estimate of how well the model will perform in real-world scenarios.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Train, Validation, Test Split for Machine Learning"](https://blog.roboflow.com/train-test-split/) goes into detail about the importance of each split.</p></details>

-----------------------

## Date - 2023-04-21


## Title - 200 customers


### **Question** :

Jordan is a machine learning engineer who works for a company specializing in personalized product recommendations. She wants to train a model that predicts whether a customer will buy a product based on their browsing history.Jordan gathered a dataset of 200 customer browsing sessions. Of these, 100 sessions resulted in a purchase, while 100 did not. Jordan wants to convert this data into a format that can be used to train a machine-learning model.As Jordan preprocesses the data, she separates the features into a tensor "X" and the corresponding target values into a tensor "y".**What is the shape of the tensor "y"?**


### **Choices** :

- (200,)
- (100, 100)
- (100, 2)
- (200, 2)


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Jordan wants to solve a binary classification problem with two classes: one representing every customer who made a purchase and the other representing the customers who did not. The `y` tensor contains the target labels. The shape of this tensor depends on the number of samples in the dataset.In this scenario, 100 customers purchased a product, and 100 didn't, so the total number of samples is 200. The shape of the tensor y should be (200,) with one dimension to represent the 200 samples.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Deep Learning with Python, Second Edition](https://amzn.to/3K3VZoy) covers the topic of tensors really well.* Check ["A Gentle Introduction to Tensors for Machine Learning with NumPy"](https://machinelearningmastery.com/introduction-to-tensors-for-machine-learning/) for a quick introduction to tensors.</p></details>

-----------------------

## Date - 2023-04-22


## Title - Purchase likelihood


### **Question** :

Haven is a data scientist at an agency specializing in developing machine learning models. She has been working on a project to build a model that predicts the likelihood of a customer purchasing one of their products.Before she trains her model, Haven needs to divide her dataset into a training set and a test set. She knows it is important to shuffle the dataset before dividing it to ensure the most accurate results.**What is the main reason for shuffling the dataset before dividing it into a training and test set?**


### **Choices** :

- To ensure the class labels are evenly distributed between the training and test sets.
- To ensure the training set contains more data than the test set.
- To ensure that the training and test sets contain the same features.
- To ensure that the training and test sets have the same number of observations


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The main reason for shuffling the dataset before dividing it into a training and test set is to ensure that the class labels are evenly distributed between the two sets. This is important because if the class labels are not evenly distributed, one of the sets might not contain data that belongs to some of the labels. For example, assuming our dataset is sorted by the class label, we might assign every sample from one class to the training set and every sample from another to a test set.Shuffling the dataset helps to mitigate this issue by randomly reordering the data, which helps to ensure that the training and test sets are representative samples of the entire data population.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Machine Learning Crash Course"](https://developers.google.com/machine-learning/crash-course) is a great introduction to Machine Learning.</p></details>

-----------------------

## Date - 2023-04-23


## Title - Weather forecasting


### **Question** :

Clara is a data scientist working for a weather forecasting company. The company wants to improve its temperature forecasting capabilities, so Clara is tasked with building a model that can accurately predict the temperature of a city based on meteorological data such as humidity, pressure, wind speed, and previous temperature records. Fortunately, she has access to a labeled dataset of meteorological data and temperatures, so she shouldn't have any problems building a model.**How can we classify the type of machine learning problem Clara wants to solve?**


### **Choices** :

- Supervised Learning problem.
- Unsupervised Learning problem.
- Classification problem.
- Regression problem.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>This is both a Supervised Learning and a Regression problem.A problem is considered Supervised Learning when the model is trained on a labeled dataset, where the input and output are known. This is the case of Clara's problem. She has access to a dataset that contains features (meteorological data) and labels (the temperature.)Clara needs to predict a continuous value: the temperature of a city. Anytime we need to predict continuous values instead of a discrete class, we are looking at a regression model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Supervised and Unsupervised Machine Learning Algorithms"](https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/) is an excellent introduction to the differences between supervised and unsupervised learning.</p></details>

-----------------------

## Date - 2023-04-24


## Title - Five columns


### **Question** :

Jordyn is studying for her final exam in her Machine Learning course. She is reviewing the different problems she might encounter.In one of the practice problems, Jordyn is given a dataset with five different categorical columns, one of which is the target value she needs to predict. The target class has two possible values. The four features each have 4, 5, 3, and 2 possible values, respectively.Jordyn has to create a synthetic dataset containing all possible samples without duplicates.**What will be the length of this dataset?**


### **Choices** :

- The length of the dataset will be 14.
- The length of the dataset will be 16.
- The length of the dataset will be 120.
- The length of the dataset will be 240.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The length of a synthetic dataset will depend on the number of possible combinations of values that can be generated based on the number of possible values for each attribute.To determine the number of possible combinations, we can calculate the product of the number of possible values for each attribute. In Jordyn's case, there are 4 possible values for the first attribute, 5 for the second attribute, 3 for the third attribute, 2 for the fourth attribute, and 2 for the target class.The product of these values is 4 x 5 x 3 x 2 x 2 = 240. This means that Jordyn's synthetic dataset will have 240 different examples, each with a different combination of values for the five columns.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Machine Learning Crash Course"](https://developers.google.com/machine-learning/crash-course) is a great introduction to Machine Learning.</p></details>

-----------------------

## Date - 2023-04-25


## Title - Younger brother


### **Question** :

Ana enjoys helping her younger brother with his computer science homework in her free time. He is taking an introductory Machine Learning class, and his homework is about Perceptrons.He needed to build a Perceptron for a binary classification problem but is his first time, and a Perceptron is not a simple thing to remember when you don't have a ton of experience.Ana decided to design a few questions to help her brother. They have a dataset with 4 features. They build the Perceptron and run the forward pass using a vector containing the values of a single sample. **What is the size of the output if this vector looks like this `[1.0, 2.0, 1.0, 2.0]`?**


### **Choices** :

- The output will contain a single value.
- The output will contain two values.
- The output will contain three values.
- The output will contain four values.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The size of the output of a Perceptron will be a single value. A Perceptron will always output the predicted class of the sample. Regardless of how many features are in the sample, the output will always be a single value.This output comes from multiplying the input vector by the weights and passing the result through the activation function, a step function in the case of a Perceptron.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The ["Perceptron Algorithm for Classification in Python"](https://machinelearningmastery.com/perceptron-algorithm-for-classification-in-python/) is a great introduction to the Perceptron.</p></details>

-----------------------

## Date - 2023-04-26


## Title - Learning conference


### **Question** :

Thea was attending a machine learning conference, eager to expand her knowledge and network with professionals in the field.During a panel discussion, one of the speakers brought up the topic of high-variance models and their sensitivity to training data.Thea paid close attention, as she knew this would be an essential concept to understand for her future work. **Which of the following algorithms can be considered high-variance models?**


### **Choices** :

- Linear Regression
- Logistic Regression
- Decision Trees
- k-Nearest Neighbors (KNN)


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Every machine learning algorithm deals with three types of errors: bias, variance, and irreducible error. We need to focus specifically on the variance error to answer this question.Here is what [Jason Brownlee](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) has to say about variance: "Variance is the amount that the estimate of the target function will change if different training data was used."In other words, variance refers to how much the answers given by the model will change if we use different training data. The model has high variance if the answers differ significantly when using different portions of our training dataset.Generally, non-linear machine learning algorithms with a lot of flexibility are high variance. For example, Decision Trees and k-Nearest Neighbors are high-variance models. Linear models, on the other hand, are usually low-variance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) is Jason Brownlee's article covering bias, variance, and their tradeoff.* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).</p></details>

-----------------------

## Date - 2023-04-27


## Title - Underlying chain principles


### **Question** :

Amy is a data scientist who works at a large tech company. She is passionate about understanding the underlying principles behind machine learning and is always looking for ways to improve her skills.Recently, Amy has been delving deeper into the subject of neural networks and has come across the concept of the chain rule from Calculus. Despite spending some time trying to understand how it works, Amy is not clear about its purpose.**Which of the following statements is true about the chain rule in neural networks?**


### **Choices** :

- The chain rule is utilized to compute the gradient of the loss function in relation to the network's inputs.
- The chain rule is utilized to compute the gradient of the loss function in relation to the network's weights.
- Optimize the performance of the network.
- Improve the accuracy of the network.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The chain rule in Calculus allows us to find the derivative of a composite function. In the context of neural networks, we use the chain rule as part of the backpropagation algorithm to calculate the gradient of the loss function with respect to the network's weights. This gradient tells us the direction and magnitude of the change we need to make to the weights to reduce the loss.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["The Chain Rule of Calculus for Univariate and Multivariate Functions"](https://machinelearningmastery.com/the-chain-rule-of-calculus-for-univariate-and-multivariate-functions) is a short introduction to the chain rule in machine learning.</p></details>

-----------------------

## Date - 2023-04-28


## Title - Transportation company


### **Question** :

Stevie is a data scientist working for a transportation company. She is tasked with building a machine-learning model to improve road traffic. The first step in the process is to perform Exploratory Data Analysis (EDA) on the data. Stevie has been invited to present to the company's board of directors about everything they will complete during this phase.**Which steps will Stevie take during the Exploratory Data Analysis process?**


### **Choices** :

- Stevie will examine the distribution of the target variable.
- Stevie will evaluate the performance of her model.
- Stevie will monitor the performance of her model.
- Stevie will examine the distribution of the features in the dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Exploratory Data Analysis is an essential step in any machine-learning project. It allows us to explore and understand our data before we build a model. During this process, we investigate the dataset to discover valuable patterns, spot any potential anomalies, and use statistics and plots to test different hypotheses and check assumptions. We also focus on understanding the distribution of the target variable and assessing the data quality, including checking for missing or corrupt values.Building a model is not part of the Exploratory Data Analysis process, so we don't evaluate the model's performance during this phase. We also don't monitor the model during this phase.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["What is Exploratory Data Analysis"](https://www.ibm.com/cloud/learn/exploratory-data-analysis) for an explanation of the EDA process.* The [_Feature Engineering for Machine Learning_](https://amzn.to/3JWZxZl) is a great book covering the entire Feature Engineering process.</p></details>

-----------------------

## Date - 2023-04-29


## Title - NAND logic gate


### **Question** :

We want to use a Perceptron to represent the NAND logic gate. As a reminder, here is how the NAND gate works:* 0 nand 0 = 1* 0 nand 1 = 1* 1 nand 0 = 1* 1 nand 1 = 0Our Perceptron will have two inputs, two weights, and a bias parameter. **Which of the following parameters will make our Perceptron act as a NAND gate?**


### **Choices** :

- `w1 = -1.0`, `w2 = -1.0`, `b = 2.0`
- `w1 = -1.0`, `w2 = -1.0`, `b = 1.0`
- `w1 = -1.0`, `w2 = -5.0`, `b = 2.0`
- You need more than one Perceptron to represent the NAND gate.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We can represent a NAND gate using a single Perceptron. Here is a simple implementation with two input values, `x1` and `x2`:```def perceptron(x1, x2, w1, w2, b):    return int((x1*w1 + x2*w2 + b) > 0)```Using this function, we can try the different configurations suggested in this question. Here is an example of running the Perceptron for the NAND gate using a set of parameters:```w1 = -1.0w2 = -1.0b = 2.0assert perceptron(0, 0, w1, w2, b) == 1assert perceptron(0, 1, w1, w2, b) == 1assert perceptron(1, 0, w1, w2, b) == 1assert perceptron(1, 1, w1, w2, b) == 0```Notice how each `assert` validates a specific pair of inputs. If there are no errors, then we can conclude the parameters work.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Neural Representation of AND, OR, NOT, XOR and XNOR Logic Gates (Perceptron Algorithm)"](https://medium.com/@stanleydukor/neural-representation-of-and-or-not-xor-and-xnor-logic-gates-perceptron-algorithm-b0275375fea1) for a deep dive on how to set up a Perceptron to represent multiple logic gates.</p></details>

-----------------------

## Date - 2023-04-30


## Title - Problem framing


### **Question** :

Understanding how to approach a new problem is a fundamental skill.Here is a list of four different problems. You can assume you have access to as much data as you need.**Which of the following applications could you frame as a Supervised Learning problem?**


### **Choices** :

- An Online Advertising application that, given one specific ad and a user's personal information, determines whether the user will click on the ad.
- A Self-Driving system that, given a picture of the front camera and the information of the radar, determines the position of other cars.
- A Visual Inspection application that, given the picture of a circuit board, determines whether there are any defects.
- An Email application that, given a new message, determines whether it's spam.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>There are multiple ways to approach these problems, but we can solve all of them using [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning).Supervised learning involves using labeled training data to make predictions or classify data. The question tells us that we can access all the data we need. We can then assume we have labeled data for each one of these problems.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Supervised and Unsupervised Machine Learning Algorithms"](https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/) is an excellent introduction to the differences between supervised and unsupervised learning.</p></details>

-----------------------

## Date - 2023-05-01


## Title - Meaningful patterns


### **Question** :

Brooklynn was exploring clustering algorithms for her latest project, as she wanted to find meaningful patterns in her dataset. After trying out K-Means clustering, she was impressed by its simplicity and efficiency. However, she was unsure about her data's optimal number of clusters.While researching online, Brooklynn encountered the term "elbow method" multiple times. Realizing its importance, she decided to delve deeper into the concept.**Which of the following statements is true about the elbow method?**


### **Choices** :

- In cluster analysis, the elbow method is used to determine the existing biases in a dataset.
- In cluster analysis, the elbow method is used to select the outliers in a dataset.
- In cluster analysis, the elbow method determines the optimal number of clusters.
- In cluster analysis, the elbow method is used to determine the features that better explain the patterns in a dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The [elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) is a way to determine the optimal number of clusters. When running K-means, you can run the algorithm with a range of values of `k`, and for each value, calculate the sum of squared errors. Then, you can plot a line chart of these errors for each value of `k`. If the line chart [looks like an arm](https://en.wikipedia.org/wiki/Elbow_method_(clustering)#/media/File:DataClustering_ElbowCriterion.JPG), then the "elbow" on the arm is the best number of clusters (`k`) that you should use.The elbow method is a way to choose the point where diminishing returns are no longer worth the cost. It's a very popular technique when using clustering algorithms.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Using the elbow method to determine the optimal number of clusters for k-means clustering"](https://bl.ocks.org/rpgove/0060ff3b656618e9136b) for an explanation of how to use the elbow method.* ["Elbow method (clustering)"](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) is the Wikipedia page covering the elbow method.* Check ["Knee of a curve"](https://en.wikipedia.org/wiki/Knee_of_a_curve) for a precise explanation of looking at the "elbow" of a curve.</p></details>

-----------------------

## Date - 2023-05-02


## Title - The network's loss


### **Question** :

The following code trains a multilayer perceptron with a single layer to learn how to perform an OR operation. 

```
import torch

X = torch.tensor([
  [0.0, 0.0],
  [0.0, 1.0],
  [1.0, 0.0],
  [1.0, 1.0],
])
y = torch.tensor([0.0, 1.0, 1.0, 1.0])
learning_rate = 0.01
W = torch.randn(2)

for epoch in range(100):
    y_hat = (X @ W).tanh()
    
    # Right here, we should compute the loss of the model
    # L = ...

    print(f"Epoch {epoch} Loss: {L.data:.4}")
    delta = 2 * L * (1 - torch.square(y_hat.tanh()))   
    W += learning_rate * (X.t() @ delta)
```

Notice one missing line: the one that computes the loss `L`.

**Which of the following is the correct way to update the loss?**


### **Choices** :

- `L = torch.square(y_hat - y).sum()`
- `L = y_hat - y`
- `L = torch.square(y_hat - y)`
- `L = (torch.square(y_hat) - y).sum()`


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Looking at the code, you'll realize the model uses the sum of the squared errors to compute the loss.

We can compute this using the following code:

```
L = torch.square(y_hat - y).sum()
```</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Cutting Your Losses: Loss Functions & the Sum of Squared Errors Loss"](https://dustinstansbury.github.io/theclevermachine/cutting-your-losses) for an explanation of the Sum of Squared Errors.</p></details>

-----------------------

## Date - 2023-05-03


## Title - Perceptron weights


### **Question** :

Arabella is a computer science student who has always been fascinated by artificial intelligence. As part of her coursework, she is building a simple perceptron from scratch. She has to decide on the number of weights her perceptron will use.Arabella has a matrix `X` that holds the features of her data samples and a vector `y` that holds the corresponding target values. She knows her perceptron will take `X` as input and produce a predicted value for each sample.**Which of the following options best describes the number of weights Arabella should use for her perceptron?**


### **Choices** :

- Arabella should use the same number of weights as the number of features in her data, i.e., weights = X.shape[1].
- Arabella should use the same number of weights as the number of samples in her data, i.e., weights = X.shape[0].
- Arabella should use the same number of weights as the number of numeric samples in her data.
- Arabella should use a random number of weights.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A perceptron is a machine learning algorithm that takes a set of inputs, known as features, and uses a set of weights to predict the class of a data point. The number of weights a perceptron uses equals the number of features.Each weight in the perceptron represents a feature in the data. The weights are used to compute a weighted sum of the inputs, which is then passed through an activation function to produce a prediction. The activation function is typically a step function that returns 0 or 1, indicating whether the sample belongs to one class or the other.Using a different weight for each feature, the perceptron captures the different relationships between the features and the target. We can learn the optimal weights that allow the model to make accurate predictions by training a perceptron.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The ["Perceptron Algorithm for Classification in Python"](https://machinelearningmastery.com/perceptron-algorithm-for-classification-in-python/) is a great introduction to the Perceptron.</p></details>

-----------------------

## Date - 2023-05-04


## Title - Damaged chip


### **Question** :

Ava works in a chip factory where 1,000,000 chips are manufactured each month. The factory estimates that 0.1% of the chips produced are defective. 

Ava developed a model that tests a chip and determines whether it's defective. During her tests, the model had a 98% accuracy rate. Ava wants to know whether the model is helpful.

She grabs the first chip manufactured that month, runs it through the model, and gets a positive result: The model predicts this is a defective chip.

**What can you tell about this result?**


### **Choices** :

- The chip is likely to be damaged with a 99% probability.
- The chip is likely to be damaged with a 95% probability.
- The chip is likely to be working with a 68% probability.
- The chip is likely to be working with a 95% probability.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's break down the problem step by step.

0.1% of the chips produced in the factory are defective, which means that 1,000 chips are defective and 999,000 work as expected. 

Ada's model is 98% accurate. If all 1,000 defective chips pass the test, `1,000 * 0.98 = 980` will be correctly identified as defective, but 20 will be incorrectly diagnosed as correct. If all 999,000 working chips go through the model, `999,000 * 0.98 = 979,020` will be correctly identified as working, but 19,980 will be incorrectly diagnosed as defective.

The chip that Ada ran through the model could be one of those 980 chips that are defective and had a positive result, but it could also be one of the 19,980 working chips with an incorrect positive result. To determine the probability this chip is damaged, we can compute how likely it is to be in each group.

Out of `980 + 19,980 = 20,960` positive results, Ada's chip is `980 / 20,960 = 5%` likely to be damaged and `19,980 / 20,960 = 95%` likely to be working.

Despite what Ada's model suggests, the chip is likely to be working correctly.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Cartoon Guide to Statistics_](https://amzn.to/3eg9iIo) is a fun and instructive introduction to probabilities and statistics.</p></details>

-----------------------

## Date - 2023-05-05


## Title - Game-playing skills


### **Question** :

Mariah loves playing games and has been studying game theory to improve her game-playing skills. She has learned about Nash equilibrium and wants to test her understanding by designing a simple game and playing it with her friends.**Which of the following are characteristics of Nash equilibrium in Mariah's game?**


### **Choices** :

- Mariah and her friends must choose their strategies at the same time.
- No player can improve their reward by changing their strategy, given the other players' strategies.
- The strategies chosen by the players must be completely random.
- Mariah and her friends must communicate with each other before choosing their strategies.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In a Nash equilibrium, each player has chosen a strategy that maximizes their reward given the other players' strategies. As a result, no player can improve their reward by changing their strategy since doing so would decrease their reward.The strategies chosen by the players do not need to be at the same time or at random, and Mariah and her friends do not need to communicate with each other before choosing their strategies.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["An Introduction to the Nash Equilibrium"](https://builtin.com/data-science/nash-equilibrium) for a great explanation of one of the cornerstones of Game Theory.</p></details>

-----------------------

## Date - 2023-05-06


## Title - Beginner tensor


### **Question** :

Catherine is a beginner in machine learning and is trying to understand the basics of tensors, which are essential data structures used in the field.

She comes across a few attributes related to tensors and wants to know which correctly represents a tensor.

**Which of the following are valid attributes that represent a tensor?**


### **Choices** :

- Dimensions: This attribute is also called the "rank" of the tensor.
- Connectivity: This attribute represents the relationship between the tensor's axes.
- Shape: This attribute represents the number of elements along each axis.
- Data type: This attribute represents the type of values contained in the tensor.


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Three primary attributes define a tensor:

1. Its rank, or the number of axes.
2. Its shape or the number of dimensions.
3. Its data type or the type of data contained in it.

The rank of a tensor refers to the tensor's number of axes.

Examples:
* Rank of a matrix is 2.
* Rank of a vector is 1.
* Rank of a scalar is 0.

The shape of a tensor describes the number of elements along each dimension.

Examples:
* `()` — scalar
* `(2,)` — vector 
* `(3, 2)` — matrix
* `(3, 2, 5)` — 3D tensor

The data type of a tensor refers to the data type it stores:

Examples:
* `float32`
* `float64`
* `uint8`
* `int64`</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Deep Learning with Python, Second Edition](https://amzn.to/3K3VZoy) covers the topic of tensors really well.* Check ["A Gentle Introduction to Tensors for Machine Learning with NumPy"](https://machinelearningmastery.com/introduction-to-tensors-for-machine-learning/) for a quick introduction to tensors.</p></details>

-----------------------

## Date - 2023-05-07


## Title - Unsupervised description


### **Question** :

Unsupervised learning, also known as unsupervised machine learning, is a subcategory of machine learning and artificial intelligence.**Which of the following is the correct description of Unsupervised learning?**


### **Choices** :

- Unsupervised learning is a type of machine learning where the model is not given any labeled training data. Instead, the model is only given a set of unlabeled data and is expected to discover the relationships and patterns within the data on its own.
- Unsupervised learning is a type of machine learning where the model is trained on labeled data, which consists of input data and corresponding correct output labels. The model can then make predictions on new, unseen data by using the patterns it learned from the training data.
- Unsupervised learning is a type of machine learning that involves training an agent to take action in an environment to maximize a reward. This is done through a process of trial and error, where the agent receives feedback in the form of rewards or punishments based on its actions and uses this feedback to adjust its behavior and improve its performance over time.
- Unsupervised learning is a type of machine learning where the training data consists of a mixture of labeled and unlabeled examples. It leverages the availability of labeled examples to improve the model's performance while still making use of the vast amount of unlabeled data that is often available.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Unsupervised learning is a type of machine learning where the model is not given any labeled training data. Instead, the model is only given a set of unlabeled data and is expected to discover the relationships and patterns within the data on its own.Supervised learning is a type of machine learning where the model is trained on labeled data, which consists of input data and corresponding correct output labels. The model can then make predictions on new, unseen data by using the patterns it learned from the training data.Reinforcement learning is a type of machine learning that involves training an agent to take action in an environment to maximize a reward. This is done through a process of trial and error, where the agent receives feedback in the form of rewards or punishments based on its actions and uses this feedback to adjust its behavior and improve its performance over time.Finally, Semi-supervised learning is a type of machine learning where the training data consists of a mixture of labeled and unlabeled examples. It leverages the availability of labeled examples to improve the model's performance while still making use of the vast amount of unlabeled data that is often available.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Supervised and Unsupervised Machine Learning Algorithms"](https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/) is an excellent introduction to the differences between supervised and unsupervised learning.* ["What is reinforcement learning? The complete guide"](https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide/) covers an introduction to Reinforcement learning.* ["What Is Semi-Supervised Learning"](https://machinelearningmastery.com/what-is-semi-supervised-learning/) covers Semi-supervised learning.</p></details>

-----------------------

## Date - 2023-05-08


## Title - Genre classification


### **Question** :

Aubrey felt exhausted after working on her music genre classification project. Her neural network performed exceptionally well during training but fell short regarding the test data.

Taking a break to clear her head, Aubrey went for a walk to reflect on the situation. Returning, she quickly jotted down a few strategies to tackle the problem.

**Which approaches will most likely help Aubrey address the issue?**


### **Choices** :

- Reduce the model's complexity by decreasing the number of layers and neurons.
- Apply regularization techniques to the model.
- Expand the volume of training data.
- Opt for a different optimization algorithm.


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Aubrey's model is experiencing overfitting.

The model performs well with the training data but has difficulty with the test set, indicating that it is not generalizing effectively to unseen samples.

By decreasing the model's complexity, Aubrey could prevent the neural network from "memorizing" the training samples. Complex models tend to overfit due to their excess capacity, making it simpler for them to memorize training data rather than generalize.

Switching the optimization algorithm is unlikely to resolve this issue. The current description does not provide any information about Aubrey's algorithm, nor is there any reason to believe it is inadequate. Furthermore, her model is learning from the training data, which would not be possible if the optimization algorithm was inappropriate.

Aubrey could also implement regularization techniques to combat overfitting. Regularization discourages the model from becoming overly complex or flexible, helping to prevent it from "memorizing" the training data.

Lastly, it is possible that Aubrey's training data does not encompass the full range of valid samples for her problem. In other words, her training dataset may not be sufficient for teaching the model to predict the test data accurately. By increasing the training data, Aubrey can create a model that generalizes better.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.</p></details>

-----------------------

## Date - 2023-05-09


## Title - Fascinated by intelligence


### **Question** :

Hallie has always been fascinated by artificial intelligence and is eager to embark on her career in the field. However, she's uncertain whether to focus on research or dive into a more industry-oriented role. 

To help her make an informed decision, we will outline some key differences between Machine Learning in a research setting and Machine Learning in a production setting.

**Select every statement that correctly highlights the differences between Machine Learning in a research environment versus production Machine Learning.**


### **Choices** :

- The design priority in a research environment usually leans towards higher performance—either the highest accuracy or other relevant metrics. Production environments place more emphasis on costs, scalability, and explainability.
- In a research environment, most work is centered around the initial training and validation of the model. In production, there is a significant focus on monitoring and maintaining models.
- The data used in a research environment is typically static, while the data used in a production setting is dynamic and constantly evolving.
- Both research and production environments are concerned with fairness, but the implications of fairness in a production environment are often more critical due to real-world consequences.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Researchers like to test their work on popular datasets. This gives them a fair comparison with other existing methods and makes reproducing their results easily. On the other hand, production data is constantly shifting. The datasets you use to train and test your models can quickly become obsolete.

A lot of emphasis on academia centers around better algorithms and techniques. Can we solve this particular problem and do it more accurately? Can we do it faster? Research jobs are about inventing new methods and squeezing as much as possible from what we already know.

Focusing on better techniques and algorithms leads to the main priority in many research positions: achieving better "performance." It could be about higher accuracy, a faster method, or fewer constraints. These, however, aren't usually the same concerns in the industry.

Production machine learning is generally more focused on the interpretability of results and the cost of the solution. "Higher accuracy" is not the most critical metric in many cases—it's still important but usually not at the expense of other factors. This is one of the main differences between research and industry positions. 

In a research environment, most work is centered around the initial training and validation of the model. Researchers explore techniques, hyperparameters, and training methodologies to improve model performance. In production environments, a significant focus is monitoring and maintaining models to ensure their effectiveness. As new data becomes available, models might require retraining, fine-tuning, or adaptation to maintain performance. Additionally, production environments often involve integrating models into larger systems, handling data pipelines, and ensuring the reliability and stability of the deployed models.

Fairness is essential to machine learning models in research and production environments. However, the implications of fairness are often more critical in a production setting due to real-world consequences. Unfair models may negatively impact users or perpetuate existing biases, leading to ethical concerns and potential legal repercussions. Fairness is studied and addressed in research, but the direct impact on real-world scenarios may not be immediate.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The ["Machine Learning Data Lifecycle in Production"](https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production) course in Coursera, part of the [Machine Learning Engineering for Production (MLOps) Specialization](https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops).* Check ["Why You Should Care About Data and Concept Drift"](https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift) to understand the importance of monitoring machine learning models.* ["A Gentle Introduction to Concept Drift in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-concept-drift-machine-learning/) is an excellent introduction to concept drift.</p></details>

-----------------------

## Date - 2023-05-10


## Title - Foreign films


### **Question** :

Ophelia is building a natural language processing model to classify movie reviews using an RNN.

She has a dataset of 20,000 reviews, split 50% into training and testing. The dataset is well-balanced, containing an equal amount of positive and negative reviews.

After some work, Ophelia's model achieves around 90% accuracy. She notices that the model struggles with reviews of foreign films, so Ophelia asks her team to collect more data.

After several days, the data collection and labeling team provides 1,000 well-balanced new labeled reviews, which Ophelia splits in half to extend the existing training and testing datasets.

The accuracy of the new model dropped to 88%.

**Which of the following is the approach that Ophelia should follow to fix the problem?**


### **Choices** :

- The architecture of the RNN model is no longer valid for the new batch of data. Ophelia should change the network to ensure she gets a better result.
- There must be a problem with the labels on the last batch of 1,000 foreign film reviews. Ophelia should work with the labeling team to correct the labels.
- Better data always leads to a better model, so Ophelia should review her code because the accuracy must improve or stay where it was.
- The accuracy here might not tell the story of what's happening. Ophelia should evaluate the model on the original 10,000 test reviews for a valid comparison.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Ophelia compared the 90% accuracy computed on the 10,000 reviews to the 88% accuracy on the augmented dataset containing 10,500 reviews. There is a problem with this.

The new reviews may be much more difficult for the model. Even if the model improved with the new data, Ophelia would not realize it because the additional reviews are causing the overall accuracy to drop.

Here is a hypothetical scenario of two models illustrating how the new model could improve even when returning a lower accuracy:

* Old model's accuracy on 10,000 reviews: 90% (9,000 reviews correctly classified.)
* Old model's accuracy on the new 500 reviews: 10% (50 reviews correctly classified.)
* New model's accuracy on the 10,000 reviews: 91% (9,100 reviews correctly classified.)
* New model's accuracy on the new 500 reviews: 28% (140 reviews correctly classified.)
* New model's accuracy on the 10,500 reviews: 88% (9,240 reviews correctly classified.)

In this hypothetical example, the new model is better on the original 10,000 reviews (from 90% to 91%) and the 500 new reviews (from 10% to 28%.) However, the approach used by Ophelia to evaluate the results made us believe the model got worse (going down from 90% to 88% accuracy.)

This is called the [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox). When comparing two experiments, we need to make sure that we evaluate the same data or at least data having the same underlying distribution. If we change the distribution, we can't compare the results and may make the wrong conclusion.

The wrong labels or architecture could be reasons for the model performing worse. However, we need to check if the model is performing worse in the first place. Also, adding more data doesn't automatically mean better results.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Simpson's Paradox"](https://www.britannica.com/topic/Simpsons-paradox) for a complete explanation about the paradox.* Here is a video about the [Simpson's Paradox](https://www.youtube.com/watch?v=ebEkn-BiW5k).</p></details>

-----------------------

## Date - 2023-05-11


## Title - Dog pictures


### **Question** :

Lauren is intrigued by the inner workings of deep neural networks and wants to understand how they process images to make accurate predictions.

She is particularly interested in how these networks handle images, so she grabbed a dataset of dog pictures and started running experiments.

**Which of the following statements reasonably simplifies how the network operates?**


### **Choices** :

- The earlier layers of a neural network compute more complex features than deeper layers.
- The deeper layers of a neural network compute more complex features than earlier layers.
- The complexity of the features computed by the neural network layers is proportionally distributed among all layers.
- The complexity of the features computed by the neural network layers has no relationship with the layer's position.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Think of a dog picture. You'll see the eyes, nose, mouth, ears, fur, and other characteristics. Notice how groups of pixels form edges, shapes, and the rest of the dog's features. 

A reasonable explanation for how a neural network works are to assume that earlier layers focus on detecting more basic features, like edges and shapes of the image. Later layers could use these earlier pieces to form more recognizable shapes, like the eyes and nose of the dog.

While the network deals with pixels early on, the deeper we go into it, the more it will work with complete patterns until it reaches the output layer.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["But what is a neural network?"](https://www.youtube.com/watch?v=aircAruvnKk) is Grant Sanderson's introduction to neural networks on YouTube. Highly recommended!* [_Neural Networks and Deep Learning_](http://neuralnetworksanddeeplearning.com/index.html) is a free online book written by [Michael Nielsen](https://twitter.com/michael_nielsen).</p></details>

-----------------------

## Date - 2023-05-12


## Title - Avoiding high bias


### **Question** :

Kiara is facing a challenging project with a large dataset and many relevant features. 

She understands the role of bias and variance in machine learning algorithms. High-bias models make more assumptions about the target function, while low-bias models make fewer assumptions.

Kiara wants to steer clear of high-bias models for her project.

**Which of the following algorithms should Kiara avoid?**


### **Choices** :

- Linear Regression
- Decision Trees
- Logistic Regression
- k-Nearest Neighbors


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Every machine learning algorithm deals with three types of errors: bias, variance, and irreducible error. We need to focus specifically on the bias error to answer this question.

Here is what [Jason Brownlee](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) has to say about bias: "Bias are the simplifying assumptions made by a model to make the target function easier to learn."

In other words, bias refers to the model's assumptions to simplify finding answers. The more assumptions it makes, the more biased the model is.

Often, linear models are high-bias. They are easier to understand but make too many assumptions about the target function, preventing them from performing well on complex problems. Linear and logistic regression are two examples of high-bias models.

Nonlinear models are usually low-bias. Decision Trees and k-Nearest Neighbors are two examples.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Here is Jason Brownlee's article I mentioned before ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/).* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).</p></details>

-----------------------

## Date - 2023-05-13


## Title - Convincing reason


### **Question** :

Nina is discussing the best approach for their new project with her coworker.

Their task is to develop a model to identify objects in drone images.

Nina believes a Convolutional Neural Network is the ideal choice, but her coworker thinks they should stick with a simple fully-connected neural network.

Nina must provide convincing reasons for choosing a Convolutional Neural Network over a fully-connected network.

**Which of the following arguments could Nina use to support her case?**


### **Choices** :

- Using a Convolutional Neural Network requires fewer parameters than a fully-connected network.
- Convolutional Neural Networks can learn a hierarchy of visual features similar to the human brain, which results in better performance.
- Convolutional Neural Networks are usually shallower than fully-connected networks, making the training process easier and faster.
- The local structure of the image can be used more efficiently by a Convolutional Neural Network, resulting in much better features and performance.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A [Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN) is a much better choice when dealing with image classification problems than a fully-connected network.

CNNs require fewer parameters than fully-connected networks and reuse the same parameters over the whole image. The number of weights in a convolutional layer depends on the kernel size and the number of channels, not the image resolution. For example, in the [AlexNet architecture](https://en.wikipedia.org/wiki/AlexNet), the five convolutional layers are responsible for only 4% of the parameters of the network. In comparison, the final three fully-connected layers contain the remaining parameters. 

A CNN can learn visual features of increasing complexity. The initial layers typically learn to recognize low-level details, like edges and colors, while deeper layers can handle more complex structures, like corners and patterns. The final layers of a CNN can learn complex representations like faces or any complex objects.

A CNN is more time and memory efficient than a fully-connected network, so we can use deeper networks with many layers, which is impossible in a fully-connected network. You should always expect CNNs to be deeper than fully-connected networks.

Finally, one reason CNNs are very effective in dealing with pictures is that they exploit the local structure of images. Since pixels located next to each other tend to be related, convolutional kernels can capture this information.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["A Beginner’s Guide to Convolutional Neural Networks (CNNs)"](https://towardsdatascience.com/a-beginners-guide-to-convolutional-neural-networks-cnns-14649dbddce8) is a great introduction to CNNs.* If you want to get deeper into how convolutions work, check out ["A guide to convolution arithmetic for deep learning"](https://arxiv.org/pdf/1603.07285.pdf).</p></details>

-----------------------

## Date - 2023-05-14


## Title - Unsatisfactory test


### **Question** :

Mckenna has been struggling with the Decision Tree model for her dataset. 

Although her model performs exceptionally well on the training data, its test performance is far from satisfactory.

Upon consulting her mentor, she was advised to prune the tree. However, Mckenna is unclear about how this would help resolve the issue.

**Which statements explain why pruning the tree will address Mckenna's problem?**


### **Choices** :

- Pruning the Decision Tree will lead to an increase in the model's bias.
- Pruning the Decision Tree will lead to a decrease in the model's variance.
- Pruning the Decision Tree will lead to an increase in the model's variance.
- Pruning the Decision Tree will lead to a decrease in the model's bias.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A Decision Tree is an algorithm with low bias and high variance. A Decision Tree makes almost no assumptions about the target function.

Because of its high variance, Decision Trees overfit easily to the training dataset. Mckenna experienced this when she tested the model on her test data.

To avoid overfitting, Mckenna should prune the tree. By doing so, she forces the tree to generalize better and make assumptions by reducing the number of nodes. In other words, by pruning the tree, she increases its bias and decreases its variance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) is an excellent introduction to the bias and variance tradeoff.* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).</p></details>

-----------------------

## Date - 2023-05-15


## Title - Course notes


### **Question** :

Alivia, a student taking a deep learning class on YouTube, is writing notes about the course and wants to capture the correct output size of a convolutional layer.Her layer receives an input image of size 64x64. She is using a kernel size of 13, padding of 3, and a stride of 2.**What is the output size of the convolutional layer?**


### **Choices** :

- The output size is 29x29
- The output size is 58×58
- The output size is 28×28
- The output size is 26×26


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Convolutions have many parameters that influence the output size, so let's go through the properties of the convolutional layer one by one.First, we know the input size to be 64×64.Next, we know that the kernel size is 13. This means we will move a sliding window of size 13×13 over the entire image. The window must fit entirely in the picture, so we cannot compute the convolution for the eight rows/columns on the border. Therefore, the resulting output so far would be 52×52.Adding padding to the image is a way to avoid the reduction of the size. In this case, we added a padding of 3, giving us an extra 6 pixels in each direction for an output of 58x58.Finally, we have a stride of 2. This means the sliding window will move by two rows/columns every time, reducing each output dimension's size by half.Therefore, the final output size of the convolutional layer will be 29×29.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* It may be difficult to imagine why some of these things happen just by reading the text, so I encourage you to go to this ["Convolution Visualizer"](https://ezyang.github.io/convolution-visualizer/) and enter the parameters of the convolutional layer from the question. It should be much more evident now why this happens.* ["Convolution arithmetic"](https://github.com/vdumoulin/conv_arithmetic) is a technical report on convolution arithmetic in the context of deep learning.</p></details>

-----------------------

## Date - 2023-05-16


## Title - Familiar loss


### **Question** :

Adriana is a Machine Learning enthusiast.

She recently learned about "binary cross-entropy" during a colleague's discussion. Although familiar with various Machine Learning techniques, binary cross-entropy was new to her. 

She opened a book and started reading about this loss function. Unfortunately, there was a question she couldn't answer.

**Which neural network problems should use binary cross-entropy as their loss function?**


### **Choices** :

- Binary cross-entropy is the loss function used for multi-label classification problems.
- Binary cross-entropy is the loss function used for binary classification problems.
- Binary cross-entropy is the loss function used for multi-class classification problems.
- Binary cross-entropy is the loss function used for regression problems.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Binary cross-entropy is the loss function we use when training binary classifiers. When working on a binary classification task, we categorize every sample into two classes. For example, given images of dogs or cats, a binary classifier will decide whether a picture shows a dog or a cat.

But binary classifiers aren't the only time we use binary cross-entropy.

Binary cross-entropy is also the loss function we use in multi-label classification problems. These are problems where we categorize every sample into one or more classes. For example, organizing movies based on the type of content, for instance, violence, adult language, smoking, or sex, where each film could belong to one or more categories.

In multi-label classification models, the output layer returns values independently. It's helpful to think of a model that outputs ten possible classes as a combination of ten different binary classifiers, and thus binary cross-entropy helps.

Neither multi-class classification nor regression problems use binary-cross entropy.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Understanding binary cross-entropy / log loss: A visual explanation"](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) is an excellent introduction to binary cross-entropy.- Another article that helps understand binary cross-entropy is ["Binary Cross Entropy/Log Loss for Binary Classification"](https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/).</p></details>

-----------------------

## Date - 2023-05-17


## Title - Sample arrays


### **Question** :

Margot is a data analyst working at a financial technology company. Her team is developing a model to predict stock prices based on various features, such as company performance and market trends.

Margot is using the NumPy library to manipulate the data, and she wants to filter her dataset to return every sample corresponding to class 0. Every feature of her data is stored in a NumPy array `X`, and the target values are in a NumPy array `y`.

**Which of the following is the correct way to filter the data?**


### **Choices** :

- Margot should use `y[0]`.
- Margot should use `X == 0`.
- Margot should use `X[y == 0, :]`.
- Margot should use `X[y == 0]`.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Margot can use `X[y == 0, :]` or `X[y == 0]` to filter the data and return every sample that belongs to class 0. To understand how this expression works, we can break it down:

We have `y == 0`, which will return `True` for every sample that belongs to class 0, and `False` otherwise. We use this as a selection mask on the set `X`, which will return every row that belongs to class 0.

To return every feature from `X`, we can use `:`. When slicing arrays, we can use `:` to specify a range of values along a particular axis. In this case, using `:` alone will return every column of the array. In this case, we can accomplish the same by omitting `:`.

The following code snippet illustrates how these two options work:

```
import numpy as np

X = np.array([[2, 1], [4, 3], [6, 5], [8, 7]])
y = np.array([0, 1, 1, 0])
print(X[y == 0, :])
print(X[y == 0])
```

You should get the following result:

```
[[2 1] [8 7]]
[[2 1] [8 7]]
```</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check the ["NumPy quickstart"](https://numpy.org/doc/stable/user/quickstart.html) tutorial for an introduction to Numpy.</p></details>

-----------------------

## Date - 2023-05-18


## Title - Organizing properties


### **Question** :

Ben is working with a real estate company to classify properties in the city. They are interested in organizing each property by its value.

He wants to start by organizing the data and preparing the features to help build a machine-learning model to predict the value of prospective properties.

**Which of the following columns on Ben's dataset are meaningful numerical features for the model?**


### **Choices** :

- The property's phone number.
- The property's square footage.
- The number of bedrooms.
- The number of bathrooms.


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The property's square footage, the number of bedrooms, and the number of bathrooms are meaningful numerical features for the model.

The phone number is not a meaningful numerical feature for this problem because it doesn't represent helpful information about the property's value. 

The property's square footage, number of bedrooms, and number of bathrooms may help predict the property's value because they can indicate the size and amenities of the property. 

It's important to carefully select and engineer the features used in a machine learning model to ensure that they are relevant and can help the model make accurate predictions.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Feature Engineering for Machine Learning_](https://amzn.to/3SsnLAc) is an excellent book covering feature engineering.
* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.</p></details>

-----------------------

## Date - 2023-05-19


## Title - XOR logic gate


### **Question** :

We want to use a Perceptron to represent the XOR logic gate. As a reminder, here is how the XOR gate works:* 0 xor 0 = 0* 0 xor 1 = 1* 1 xor 0 = 1* 1 xor 1 = 0Our Perceptron will have two inputs, two weights, and a bias parameter. **Which of the following parameters will make our Perceptron act as an XOR gate?**


### **Choices** :

- `w1 = 0.1`, `w2 = 0.6`, `b = 1.0`
- `w1 = 0.6`, `w2 = 1.6`, `b = -2.3`
- `w1 = 1.0`, `w2 = 1.0`, `b = -0.5`
- You need more than one Perceptron to represent the XOR gate.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We cannot represent an XOR gate using a single Perceptron. Here is a plot showing the four possible results of the XOR operation:![image](https://user-images.githubusercontent.com/1126730/214069328-c724f9fd-a4bf-4437-8028-6a4b1768518d.png)Notice how we can't separate `(0, 1)` and `(1, 0)` from `(0, 0)` and `(1, 1)` using one line. As a reminder, the Perceptron can only produce a linear decision boundary, so we won't be able to use it to represent the XOR gate.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Neural Representation of AND, OR, NOT, XOR and XNOR Logic Gates (Perceptron Algorithm)"](https://medium.com/@stanleydukor/neural-representation-of-and-or-not-xor-and-xnor-logic-gates-perceptron-algorithm-b0275375fea1) for a deep dive on how to set up a Perceptron to represent multiple logic gates.</p></details>

-----------------------

## Date - 2023-05-20


## Title - Blog encoding techniques


### **Question** :

Stevie is writing a blog post about encoding techniques for handling categorical features in a dataset. She wants to add a fun question at the end of the post to test her readers' understanding of the topic.She writes many different ideas, and now it's your turn to decide how Stevie should mark each statement.**Which statements regarding Label encoding and One-Hot encoding techniques are correct?**


### **Choices** :

- One-Hot encoding generates new columns for each categorical feature.
- One-Hot encoding creates a unique numerical representation for each categorical feature.
- Label encoding generates two new labels for each categorical feature.
- Label encoding generates new columns for each categorical feature.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Before analyzing this question, we must understand what "categorical data" means.Categorical data are variables that contain label values rather than numeric values. For example, a variable representing the weather with values "sunny," "cloudy," and "rainy" is a categorical variable.Although some algorithms can use categorical data directly, most can't: they require the data to be numeric. We can use One-Hot or Label encoding to do this.[One-Hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f) creates a new feature for each unique value of the original categorical variable.For example, assume we have a dataset with a single feature called "weather" that could have the values "sunny," "cloudy," and "rainy." Applying One-Hot Encoding will get us a new dataset with three features, one for each value of the original "weather" column. A sample that had the value "cloudy" in the previous column will now have the value 0 for both "sunny" and "rainy" and the value 1 under the "cloudy" feature.On the other hand, [Label encoding](https://www.mygreatlearning.com/blog/label-encoding-in-python/) replaces each categorical value with a consecutive number starting from 0. For example, Label Encoding would replace our weather feature with a new one containing the values 0 instead of "cloudy," 1 instead of "rainy," and 2 instead of "sunny."</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What is One Hot Encoding? Why and When Do You Have to Use it?"](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f) is an excellent introduction to One-Hot encoding.* ["Label Encoding in Python Explained"](https://www.mygreatlearning.com/blog/label-encoding-in-python/) is an introduction to Label encoding.* ["One-Hot Encoding vs. Label Encoding using Scikit-Learn"](https://www.analyticsvidhya.com/blog/2020/03/one-hot-encoding-vs-label-encoding-using-scikit-learn/) covers both techniques and when to use each one.</p></details>

-----------------------

## Date - 2023-05-21


## Title - Chat application


### **Question** :

Raegan is a software engineer at a company that builds a chat application. Recently, she started taking an online course on machine learning and came across the concept of splitting datasets before training a model.Although the course discussed the importance of splitting the data into train, validation, and test sets, it didn't mention why we do this.**Which of the following statements explain why we split a dataset before training a model?**


### **Choices** :

- We split the dataset to prevent the model from overfitting.
- We split the dataset to reduce the memory needed to train the model.
- We split the dataset to make the training process faster.
- We split the dataset to evaluate the model's performance accurately.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Imagine teaching a math class, and it's time to evaluate your students. You decide to leave them 100 exercises as their homework. These problems cover the content they need to master to ace the exam.How can you design an exam that effectively identifies those who learned the material?Let's assume you pick 20 of the same homework exercises and use them in your test. This strategy might result in false positives: students who memorize the solutions to their homework may get a high score, although they don't necessarily know how to reason. In machine learning, we call this "overfitting."To ensure students don't overfit to their training exercises, you don't want to use the same homework to test their knowledge. Instead, you want to find new problems that evaluate the same material but are different enough to force the students to show their skills.We want to do the same when training machine learning models. If we only evaluate our work in the same data we use to train the model, we might overfit and have a model that isn't capable of generalizing to different data. In other words, the model may "memorize" the training data and learn to return excellent predictions when tested.If we split the dataset and leave a portion to evaluate how much the model learned, we will ensure that overfit won't happen. Therefore, we can accurately assess the model's performance and avoid overfitting.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.* ["Train, Validation, Test Split for Machine Learning"](https://blog.roboflow.com/train-test-split/) goes into detail about the importance of each split.</p></details>

-----------------------

## Date - 2023-05-22


## Title - Animal characteristics


### **Question** :

Kaylani is the tech lead of a project that aims to classify pictures of animals based on 10 different characteristics. Each animal can have one or more of these features, so the system should recognize and tag the animal photos appropriately.The team used a convolutional neural network to build this system. The only question that remains is the best approach for designing the network.**Which would be the best approach for Kaylani and her team to design the network for classifying the photos?**


### **Choices** :

- Kaylani should use a sigmoid activation function in the output layer. The loss function should be binary cross-entropy.
- Kaylani should use a sigmoid activation function in the output layer. The loss function should be categorical cross-entropy.
- Kaylani should use a softmax activation function in the output layer. The loss function should be binary cross-entropy.
- Kaylani should use a softmax activation function in the output layer. The loss function should be categorical cross-entropy.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The team is trying to build a [multi-label classification](https://en.wikipedia.org/wiki/Multi-label_classification) model. Every animal will be classified with one or more of the 10 characteristics in multi-label classification. This differs from [multi-class classification](https://en.wikipedia.org/wiki/Multiclass_classification), where every animal will only be classified into one category.When building multi-label classification models, we need an output layer where every class is independent. Remember that we can have more than one active class for each input. The softmax activation function doesn't work because it uses every score to output the probabilities of each class. Softmax is the correct output for multi-class classification. Sigmoid is the correct output for multi-label classification problems. The sigmoid function converts output scores to a value between 0 and 1, independently of all the other scores.Multi-label classification problems borrow the same principles from binary classification problems. The difference is that we end up with multiple sigmoid outputs instead of one. In our example problem, we combine three different binary classifiers. This is why we should use a binary cross-entropy as the loss function.In summary, multi-class classification models should use a softmax output with the categorical cross-entropy loss function. Multi-label classification models should use a sigmoid output and the binary cross-entropy loss function.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The Wikipedia explanation of [Multi-label classification](https://en.wikipedia.org/wiki/Multi-label_classification) should give you most of what you need to understand for this task.* ["Demystifying the Difference Between Multi-Class and Multi-Label Classification Problem Statements in Deep Learning"](https://www.analyticsvidhya.com/blog/2021/07/demystifying-the-difference-between-multi-class-and-multi-label-classification-problem-statements-in-deep-learning/) is an excellent article comparing these two types of problems.* ["How to choose cross-entropy loss function in Keras?"](https://androidkt.com/choose-cross-entropy-loss-function-in-keras/) explains the differences between the loss functions that we discussed in this question.</p></details>

-----------------------

## Date - 2023-05-23


## Title - Marine biologist


### **Question** :

Aniyah is a marine biologist using machine learning to study the migratory patterns of various sea creatures. She needs to determine how close different species are to one another during their migrations.

Aniyah is considering using clustering algorithms like [K-Means](https://en.wikipedia.org/wiki/K-means_clustering). These algorithms require measuring the similarity between observations in her dataset.

Euclidean distance is one of the most popular distance metrics for this task.

**From the following list, select every correct statement about the Euclidean distance.**


### **Choices** :

- The Euclidean distance between two points does not depend on which of the two points is the start and which is the destination. In other words, the distance between _p_ and _q_ is the same as between _q_ and _p_.
- The Euclidean distance is a way to compute the distance between two points in two-dimensional spaces. The Euclidean distance doesn't work in multidimensional spaces.
- The Euclidean distance between two distinct points is always positive.
- Traveling from a point _p_ to a point _q_ via a point _r_ cannot be any shorter than traveling directly from _p_ to _q_.


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In one or more dimensions, we can use the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance). For example, in a line, the distance between two points is the numerical difference between their coordinates. In a plane, the distance is the [Pythagorean distance](https://en.wikipedia.org/wiki/Pythagorean_theorem). 

But can we use it in multidimensional spaces?

The answer is yes; the Euclidean distance works in [multidimensional spaces](https://hlab.stanford.edu/brian/euclidean_distance_in.html). Intuitively, this should make sense because we can use it as the metric to compute the distance between multi-feature observations in our dataset.

The distance from a point _p_ to another point _q_ is the same regardless of whether we start from _p_ or _q_. This distance is always a positive value as long as _p_ and _q_ are different points. If _p_ and _q_ are the same point, the distance is 0.

In the [Euclidean plane](https://en.wikipedia.org/wiki/Euclidean_space), the distance between any two distant points is the length of the line segment joining them. So this segment joining points _p_ and _q_ can't be any shorter, regardless of whether we get from _p_ to _q_ via a third point _r_.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Euclidean Distance In 'n'-Dimensional Space"](https://hlab.stanford.edu/brian/euclidean_distance_in.html) for a summary and visualization of how the Euclidean distance works in multi-dimensional spaces.* ["Euclidean distance"](https://en.wikipedia.org/wiki/Euclidean_distance) and ["Euclidean space"](https://en.wikipedia.org/wiki/Euclidean_space) are Wikipedia articles that will help with this topic.</p></details>

-----------------------

## Date - 2023-05-24


## Title - Piece replacement


### **Question** :

Lucille is a data scientist writing an article about a model developed to predict if a piece of equipment has to be replaced. The model takes in the piece's characteristics and outputs a prediction based on the relationships between the input features and the target variable.Lucille wants to explain how the coefficients, the parameters learned by the model during training, influence the final prediction.**Which of the following options represents the best way for Lucille to describe the role of the coefficients in the model?**


### **Choices** :

- A positive coefficient suggests that the feature does not affect the prediction.
- A negative coefficient suggests that the feature does not affect the prediction.
- A positive coefficient suggests that the feature has a higher probability of positively affecting the prediction.
- A positive coefficient suggests that the feature has a higher probability of negatively affecting the prediction.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The coefficients in the context of a model are the parameters learned by the model during training. They determine the relationship between the input features and the output predictions.In this example, the coefficients are used to predict whether the piece has to be replaced. The coefficients weigh the input features and determine the final prediction.The best way to describe the role of the coefficients is by saying that a positive coefficient indicates that the variable is more likely to influence the outcome positively. This means that the variable has a positive relationship with the target variable and increases the probability of the target being positive.On the other hand, a negative coefficient indicates that the variable has a higher likelihood of negatively influencing the outcome. This means that the variable has a negative relationship with the target variable and decreases the probability of the target being positive.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems_](https://amzn.to/3SHGqsu) is one of the best books for machine learning fundamentals.</p></details>

-----------------------

## Date - 2023-05-25


## Title - Grading exams


### **Question** :

Mya is a college professor grading exams for aspiring data scientists. She is currently evaluating the final project, which involves building a machine-learning model to predict the success of a new product launch for a tech company.The student must explain how they used the training, validation, and test sets during the development of their models.**Which of the following show appropriate practices for using these sets?**


### **Choices** :

- Use the training set multiple times to train the model.
- Use the validation set multiple times to fine-tune the model's parameters.
- Use the test set multiple times to fine-tune the model's parameters.
- Use the test set only once to fine-tune the model's parameters.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The training and validation sets can be used multiple times throughout the model development process.We use a training set to train the model. This training set is a subset of the entire dataset. We use the training set multiple times throughout the model development process as the model is refined and improved.We use a validation set to tune the model's parameters and assess its performance to prevent overfitting. We evaluate the model's performance after each iteration of training.After we finish training and tuning the model, we can evaluate it on a test set, which hasn't been used during the training or validation process. The model's performance on the test set provides an estimate of how well the model will perform in real-world scenarios.We should never use the test set to fine-tune the model's parameters.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Train, Validation, Test Split for Machine Learning"](https://blog.roboflow.com/train-test-split/) goes into detail about the importance of each split.</p></details>

-----------------------

## Date - 2023-05-26


## Title - Types of flowers


### **Question** :

Sarah is a computer science student who wants to build a machine-learning model for classifying types of flowers. She collected a dataset containing 150 flower samples with 3 features for each flower: the sepal length, the sepal width, and the petal length.As Sarah starts to build the model, she separates the features into a tensor `X` and the corresponding target labels into a tensor `y`.**What is the shape of the tensors `X` and `y`?**


### **Choices** :

- The shape of `X` is (150, 3), and the shape of `y` is (150,)
- The shape of `X` is (3, 150), and the shape of `y` is (150,)
- The shape of `X` is (150, 3), and the shape of `y` is (150, 1)
- The shape of `X` is (150, 1), and the shape of `y` is (150, 3)


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Sarah separated her data into two tensors. The `X` tensor contains the features, and the `y` tensor contains the target labels. The shape of these tensors depends on the number of samples and features in the dataset.In this scenario, there are 150 flower samples, and each sample has 3 features. Therefore, the shape of the tensor `X` should be (150, 3), and the tensor `y` should be (150,) because it's a vector containing the target values.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Deep Learning with Python, Second Edition](https://amzn.to/3K3VZoy) covers the topic of tensors really well.* Check ["A Gentle Introduction to Tensors for Machine Learning with NumPy"](https://machinelearningmastery.com/introduction-to-tensors-for-machine-learning/) for a quick introduction to tensors.</p></details>

-----------------------

## Date - 2023-05-27


## Title - Specific outcomes


### **Question** :

Tessa is a data scientist working in the field of Computer Vision. She has been assigned a project that involves building a model to predict specific outcomes based on visual data.Before training her model, Tessa divides her dataset into a training set and a test set. She understands that shuffling the dataset before dividing it is crucial in ensuring accurate results.**What is the primary reason for shuffling the dataset before dividing it into a training and test set?**


### **Choices** :

- To make sure the training set contains more data than the test set.
- To make sure the features in the training and test sets are the same.
- To make sure the class labels are evenly split between the training and test sets.
- To make sure the training set contains the same number of samples as the test set.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The main reason for shuffling the dataset before dividing it into a training and test set is to ensure that the class labels are evenly distributed between the two sets. This is important because if the class labels are not evenly distributed, one of the sets might not contain data that belongs to some of the labels. For example, assuming our dataset is sorted by the class label, we might assign every sample from one class to the training set and every sample from another to a test set.Shuffling the dataset helps to mitigate this issue by randomly reordering the data, which helps to ensure that the training and test sets are representative samples of the entire data population.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Machine Learning Crash Course"](https://developers.google.com/machine-learning/crash-course) is a great introduction to Machine Learning.</p></details>

-----------------------

## Date - 2023-05-28


## Title - Main technique


### **Question** :

Clara is a data scientist working for a weather forecasting company. The company wants to improve its temperature forecasting capabilities, so Clara is tasked with building a model that can accurately predict the temperature of a city based on meteorological data such as humidity, pressure, wind speed, and previous temperature records. Fortunately, she has access to labeled meteorological data and temperatures dataset, so she should be OK with building a model.**Which of the following is the main technique Clara should use to solve this task?**


### **Choices** :

- Clustering
- Regression
- Classification
- Dimensionality Reduction


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The primary technique that Clara should use to solve this problem is regression.Regression is a supervised learning technique that predicts a continuous value based on input features. In this example, Clara wants to predict the temperature of a city based on meteorological data such as humidity, pressure, wind speed, and previous temperature records. The temperature is a continuous value.To do this, she will use a dataset of temperature records, with meteorological data and their corresponding temperature values. She can train a model to make predictions on new, unseen data.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Linear Regression for Machine Learning"](https://machinelearningmastery.com/linear-regression-for-machine-learning/) is an introduction to Linear Regression.* ["Supervised and Unsupervised Machine Learning Algorithms"](https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/) is an excellent introduction to the differences between supervised and unsupervised learning.</p></details>

-----------------------

## Date - 2023-05-29


## Title - Soccer department


### **Question** :

Joanna works in the analytics department of a soccer team. She analyzes player performance data to determine potential strategies for upcoming games. Joanna is given a dataset with four categorical columns, one of which is the target value she needs to predict. The target class has two possible values. The three features each have 3, 3, and 2 possible values, respectively.Joanna must generate a synthetic dataset with all unique samples, excluding duplicates.**What will be the length of this dataset?**


### **Choices** :

- The length of the dataset will be 8.
- The length of the dataset will be 18.
- The length of the dataset will be 36.
- The length of the dataset will be 64.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The length of a synthetic dataset will depend on the number of possible combinations of values that can be generated based on the number of possible values for each attribute.To determine the number of possible combinations, we can calculate the product of the number of possible values for each attribute. In Joanna's case, there are 3 possible values for the first attribute, 3 for the second attribute, 2 for the third attribute,  and 2 for the target class.The product of these values is 3 x 3 x 2 x 2 = 36. This means Joanna's synthetic dataset will have 36 different examples, each with a different combination of values for the four columns.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Machine Learning Crash Course"](https://developers.google.com/machine-learning/crash-course) is a great introduction to Machine Learning.</p></details>

-----------------------

## Date - 2023-05-30


## Title - Perplexing problem


### **Question** :

Despite their best efforts, Nicole's team was facing a perplexing problem: their machine learning model was not generalizing well to the test data.

Exhausted and ready to reconsider their approach, the team was about to regroup when Nicole had a sudden idea.

She took the training dataset, removed the target variable, and combined it with the test data. Then, she created a new binary target, assigning a 1 to every test sample and a 0 to every training sample.

"Train a classifier on this new dataset, and let's see how accurate the predictions are," she instructed her team.

**Her team appeared puzzled. What is Nicole attempting to accomplish?**


### **Choices** :

- This classifier will estimate how different the training data is from the test data, potentially explaining why the team is struggling.
- This classifier will turn the initial problem into a more straightforward approach that will serve as a baseline for the team to continue their work and find a better solution.
- This classifier will estimate the performance of the team's model on unseen test data. It will help the team understand whether they are overfitting or underfitting the training data.
- Combining the training and test data is never a good idea, so this classifier's results will not be valid.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Nicole's approach should be familiar if you have experience participating in Kaggle competitions. 

Popular validation techniques, like cross-validation, allow you to test your models on unseen data if that data comes from the same distribution as your training dataset. Unfortunately, that's not always the case, and even slight differences between the training and test data will considerably affect the result of your model.

[Adversarial validation](https://articles.bnomial.com/adversarial-validation) is a technique to estimate the difference between your training and test data. [_The Kaggle Book_](https://amzn.to/3kbanRb) introduces it as follows:

> [adversarial validation] was long rumored among Kaggle participants and transmitted from team to team until it emerged publicly, thanks to a post by Zygmunt Zając on his FastML blog.

Nicole created a new dataset by joining the training and test data. The target of that new dataset is a binary variable differentiating the training and test samples. She can determine how easy it's to separate both datasets by running a classifier on that new data.

Adversarial validation relies on computing the [ROC-AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc), a graph showing the True Positive Rate and the False Positive Rate at different classification thresholds. The area under this curve (AUC) measures the model's performance. A perfect model will have an area of `1.0`, while a model that only makes mistakes will have an area of `0.0`.

If they run the classifier and the ROC-AUC is around `0.5`, Nicole will know that the training and test data are not easily distinguishable, which is good because it means the data comes from the same distribution. If the ROC-AUC is too high—closer to `1.0`—the classifier can tell training and test data apart, which means they come from a different distribution.

Adversarial validation is a very clever technique. The result could help explain the team's struggle and guide it to continue.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Adversarial validation"](https://articles.bnomial.com/adversarial-validation) is a great introduction to adversarial validation.* Check ["What is Adversarial Validation?"](https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation/notebook) for a discussion about this technique in Kaggle.* [_The Kaggle Book_](https://amzn.to/3kbanRb) is an amazing reference for those looking to participate in Kaggle.</p></details>

-----------------------

## Date - 2023-05-31


## Title - Learning hackathon


### **Question** :

Jayla was participating in a machine learning hackathon with her teammates, working to develop a solution for a complex problem.As the team brainstormed ideas, they started discussing the pros and cons of various algorithms. Jayla knew that some models were more sensitive to changes in training data than others.**Which of the following algorithms can be considered high-variance models?**


### **Choices** :

- Decision Trees
- Linear Regression
- Logistic Regression
- k-Nearest Neighbors (KNN)


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Every machine learning algorithm deals with three types of errors: bias, variance, and irreducible error. We need to focus specifically on the variance error to answer this question.Here is what [Jason Brownlee](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) has to say about variance: "Variance is the amount that the estimate of the target function will change if different training data was used."In other words, variance refers to how much the answers given by the model will change if we use different training data. The model has high variance if the answers differ significantly when using different portions of our training dataset.Generally, non-linear machine learning algorithms with a lot of flexibility are high variance. For example, Decision Trees and k-Nearest Neighbors are high-variance models. Linear models, on the other hand, are usually low-variance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) is Jason Brownlee's article covering bias, variance, and their tradeoff.* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).</p></details>

-----------------------

## Date - 2023-06-01


## Title - Chain rule


### **Question** :

Lia is a computer science teacher passionate about helping her students learn the latest developments in the field. She has been teaching a course on neural networks, and one of the topics that have come up is the chain rule from Calculus.For days, her students struggled to fully understand the purpose of the chain rule. Lia has spent time reviewing the materials with them, but they have yet to make much progress.**Which of the following are characteristics of the chain rule?**


### **Choices** :

- The chain rule allows the computation of derivatives of composite functions.
- The chain rule is based on the idea that the derivative of a composite function is the product of the derivative of the outer function and the derivative of the inner function.
- The chain rule is utilized to compute the gradient of the loss function in relation to the network's weights.
- The chain rule is used as part of the backpropagation algorithm.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The chain rule in Calculus allows us to find the derivative of a composite function.In the context of neural networks, we use the chain rule as part of the backpropagation algorithm to calculate the gradient of the loss function in relation to the network's weights. This gradient tells us the direction and magnitude of the change we need to make to the weights to reduce the loss.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Chain Rule"](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review) for a review of how it works.* ["The Chain Rule of Calculus for Univariate and Multivariate Functions"](https://machinelearningmastery.com/the-chain-rule-of-calculus-for-univariate-and-multivariate-functions) is a short introduction to the chain rule in machine learning.</p></details>

-----------------------

## Date - 2023-06-02


## Title - Researching presentation


### **Question** :

While exploring deep learning techniques, Gabriela stumbled upon an intriguing challenge: building a Machine Learning model to predict movie ratings based on user preferences.

Before diving into the problem, she realized she needed to understand the concept of "Tensors" to utilize a popular Deep Learning library effectively.

Determined to master the topic, Gabriela spent hours researching tensors and their properties. As she gained confidence in her understanding, she presented her findings during a local machine learning meetup.

**Which of the following assertions from Gabriela's presentation are correct?**


### **Choices** :

- A vector that contains only one number is called a "scalar" or a rank-0 tensor. An example of a scalar is any numeric value like a person's age or today's temperature.
- An array of numbers is called a "vector" or a rank-1 tensor. An example of a vector is an array containing the age of every person in a dataset.
- An array with two dimensions is called a matrix or a rank-2 tensor. An example is an array containing the age and sex of every person in a dataset.
- An array with three dimensions is a rank-3 tensor. An example is a 3D array containing the information of an image: its width, height, and the number of channels.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's get something out of the way: This question is about tensors as we use them in deep learning, not the mathematical definition of a tensor.

The rank of a tensor refers to its number of axes—or dimensions. For example:

* The rank of a scalar is 0.
* The rank of a vector is 1.
* The rank of a matrix is 2.
* The rank of a 3D tensor is 3.

A scalar, or 0D tensor, has a rank of 0 and contains a single number. These are also called "0-dimensional tensors." 

A vector, or 1D tensor, has a rank of 1 and represents an array of numbers.

A matrix, or 2D tensor, has a rank of 2 and represents an array of vectors. We refer to the two axes of a matrix as "rows" and "columns."

You can obtain higher-dimensional tensors (3D, 4D, etc.) by packing lower-dimensional tensors in an array. For example, packing a 2D tensor in an array gives you a 3D tensor. 

For example, to store a color image, we need three dimensions: one representing the image's width, another representing the height, and a final dimension for the color channels. Assuming we have three channels (red, blue, and green), you can think of having three different matrices, each containing the pixels for each one of the channels.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Deep Learning with Python_, Second Edition](https://amzn.to/3K3VZoy) covers the topic of tensors really well.* Check ["A Gentle Introduction to Tensors for Machine Learning with NumPy"](https://machinelearningmastery.com/introduction-to-tensors-for-machine-learning/) for a quick introduction to tensors and practical code.</p></details>

-----------------------

## Date - 2023-06-03


## Title - NOR logic gate


### **Question** :

We want to use a Perceptron to represent the NOR logic gate. As a reminder, here is how the NOR gate works:* 0 nor 0 = 1* 0 nor 1 = 0* 1 nor 0 = 0* 1 nor 1 = 0Our Perceptron will have two inputs, two weights, and a bias parameter. **Which of the following parameters will make our Perceptron act as a NOR gate?**


### **Choices** :

- `w1 = -1.0`, `w2 = -1.0`, `b = 1.0`
- `w1 = 0.5`, `w2 = 0.5`, `b = -0.1`
- `w1 = -2.0`, `w2 = -1.0`, `b = 0.8`
- You need more than one Perceptron to represent the NOR gate.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We can represent a NOR gate using a single Perceptron. Here is a simple implementation with two input values, `x1` and `x2`:```def perceptron(x1, x2, w1, w2, b):    return int((x1*w1 + x2*w2 + b) > 0)```Using this function, we can try the different configurations suggested in this question. Here is an example of running the Perceptron for the NOR gate using a set of parameters:```w1 = -1.0w2 = -1.0b = 1.0assert perceptron(0, 0, w1, w2, b) == 1assert perceptron(0, 1, w1, w2, b) == 0assert perceptron(1, 0, w1, w2, b) == 0assert perceptron(1, 1, w1, w2, b) == 0```Notice how each `assert` validates a specific pair of inputs. If there are no errors, then we can conclude the parameters work.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Neural Representation of AND, OR, NOT, XOR and XNOR Logic Gates (Perceptron Algorithm)"](https://medium.com/@stanleydukor/neural-representation-of-and-or-not-xor-and-xnor-logic-gates-perceptron-algorithm-b0275375fea1) for a deep dive on how to set up a Perceptron to represent multiple logic gates.</p></details>

-----------------------

## Date - 2023-06-04


## Title - Smart home system


### **Question** :

Phoenix is working on a Machine Learning model to optimize the energy consumption of a smart home system. She wants to implement the Gradient Descent optimization algorithm to train her model.

To balance the trade-off between computation speed and update accuracy, Phoenix is considering using Mini-Batch Gradient Descent. She wants to understand its benefits and characteristics.

**Which of the following statements is true about Mini-Batch Gradient Descent?**


### **Choices** :

- Mini-Batch Gradient Descent requires loading the entire dataset into memory for each update iteration.
- Mini-Batch Gradient Descent updates the model weights only after processing all the available training data.
- Mini-Batch Gradient Descent uses a batch of data (more than one sample but fewer than the entire dataset) during every iteration.
- Mini-Batch Gradient Descent is not a variant of Gradient Descent.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Gradient Descent is a popular optimization algorithm in machine learning. It minimizes an objective function by iteratively updating the model weights based on the function's gradient. The number of samples used to compute the gradient in each iteration can vary.

Using a single sample is called Stochastic Gradient Descent (SGD). Using all the data at once is called Batch Gradient Descent. Using a batch of data—more than one sample but fewer than the entire dataset—is called Mini-Batch Gradient Descent.

Mini-Batch Gradient Descent strikes a balance between computation speed and update accuracy. It leverages the benefits of both Stochastic Gradient Descent and Batch Gradient Descent by processing multiple samples in each iteration. This approach allows for more accurate updates than SGD while being computationally more efficient than Batch Gradient Descent, which requires loading the entire dataset into memory.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["The wrong batch size is all it takes" ](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) explains how different batch sizes influence the training process of neural networks using gradient descent.* ["A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size"](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) covers Batch and Mini-Batch Gradient Descent.* Check ["An overview of gradient descent optimization algorithms"](https://www.ruder.io/optimizing-gradient-descent/) for a deep dive into gradient descent and every one of its variants.</p></details>

-----------------------

## Date - 2023-06-06


## Title - Platform recommendations


### **Question** :

Kendall is a data scientist at a tech company building a recommendation system for its e-commerce platform.

Before looking into more complex models, she wants to establish a good baseline. Kendall is considering trying two different models: a simple neural network and a linear regression model.

**Which of the following statements are true for both Kendall's neural network and linear regression model?**


### **Choices** :

- Both models require numeric inputs between 0 and 1, so Kendall must standardize the values.
- Both models need numeric input features, so Kendall must convert non-numeric features.
- The result from both models is the linear sum of weighted inputs.
- The result from both models is a probability vector.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Although it is generally true that both models would perform better with scaled or standardized input features, neither model explicitly requires features to be within the range of 0 to 1.

Both neural networks and linear regression models require input features to be numeric. They cannot handle categorical features directly, so Kendall must transform non-numeric features before using them in either model.

The output of a linear regression model is a single numerical value, not a vector of probabilities. Neural networks do not necessarily have to produce such output either.

Lastly, while linear regression and neural networks involve a linear sum of weighted inputs, neural networks introduce non-linearities through activation functions. This key difference makes neural networks more powerful than linear regression.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Linear Regression v.s. Neural Networks"](https://towardsdatascience.com/linear-regression-v-s-neural-networks-cd03b29386d4) for comparing these two techniques.* ["3 Reasons Why You Should Use Linear Regression Models Instead of Neural Networks"](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html) is another great article talking about their differences.* ["Linear Regression for Machine Learning"](https://machinelearningmastery.com/linear-regression-for-machine-learning/) is a great introduction to linear regression.* ["Intro to Neural Networks"](https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/video-lecture) is a good summary of neural networks.</p></details>

-----------------------

## Date - 2023-06-07


## Title - Fascinating trees


### **Question** :

Amina, a software engineer passionate about problem-solving, recently became interested in machine learning. As she ventured into this new domain, she began experimenting with various models.

Soon, Amina discovered Decision Trees and found them fascinating. The model provided a unique balance of power and complexity.

Eager to share her enthusiasm, Amina started recommending Decision Trees to her colleagues who were also new to machine learning.

**Which of the following statements would Amina likely mention as an advantage of using a Decision Tree?**


### **Choices** :

- The predictions generated by a Decision Tree are easy to interpret and explain.
- A Decision Tree can solve regression and classification tasks.
- A Decision Tree is very resistant to overfitting.
- A Decision Tree doesn't require tuning to get good predictions.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>As a developer, [Decision Trees](https://en.wikipedia.org/wiki/Decision_tree_learning) have always made sense. 

I'm not talking about building a good Decision Tree but how they work to find predictions. In a short sentence, a Decision Tree is just a bunch of nested conditions that get us to the final solution.

And this property is precisely what makes them very popular: we can look at a prediction and fully understand why the model arrived at that conclusion. In other words, the predictions generated by Decision Trees are easy to interpret, an essential advantage of using Decision Trees over more obscure techniques, like neural networks or Support Vector Machines.

Decision Trees, unfortunately, are prone to overfitting if we don't take careful care of their depth. In other words, unless we ensure our tree doesn't go too deep, it will tend to fit noisy samples and output the wrong prediction. We can control overfitting in a Decision Tree by "pruning."

Remember that while a single Decision Tree is prone to overfitting, using an ensemble of trees is more resistant. Here is a quote from "[To Boost or not to Boost: On the Limits of Boosted Neural Networks](https://arxiv.org/pdf/2107.13600.pdf)":

> [these experiments] confirm that training single large decision trees is prone to overfitting while boosted ensembles of decision trees are resistant to overfitting.

Many people relate Decision Trees with classification tasks, but they are also valuable for solving regression tasks. A classification task is when the predicted outcome is a discrete class, while the result of a regression task is a Real number. This flexibility makes Decision Trees very useful.

Finally, we discussed above how Decision Trees are prone to overfit if we aren't careful with their depth. This is just one of the hyperparameters that we can tune. Like with most techniques, Decision Trees require experimentation and tuning to improve the quality of their results.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["To Boost or not to Boost: On the Limits of Boosted Neural Networks"](https://arxiv.org/pdf/2107.13600.pdf), the paper cited above comparing the overfitting tendency of a single Decision Tree versus an ensemble of trees.* ["Decision tree pruning"](https://en.wikipedia.org/wiki/Decision_tree_pruning) covers the process of pruning a tree to reduce overfitting.* Check ["Random Forest"](https://en.wikipedia.org/wiki/Random_forest) for more information about a widely used class of Decision Tree ensembles.</p></details>

-----------------------

## Date - 2023-06-08


## Title - Building a Perceptron


### **Question** :

Sara is a professor of computer science, and she is preparing an Introduction to Machine Learning class. She will introduce the concept of Perceptrons to her students, and she wants to emphasize the importance of choosing the correct number of weights when building a Perceptron.**Which of the following options is the correct way to choose the number of weights when building a Perceptron?**


### **Choices** :

- The number of weights should be randomly chosen.
- The number of weights should equal the number of samples in the dataset.
- The number of weights should equal the number of features in the dataset.
- The number of weights should equal the number of biases in the model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A perceptron is a machine learning algorithm that takes a set of inputs, known as features, and uses a set of weights to predict the class of a data point. The number of weights a perceptron uses equals the number of features.Each weight in the perceptron represents a feature in the data. The weights are used to compute a weighted sum of the inputs, which is then passed through an activation function to produce a prediction. The activation function is typically a step function that returns 0 or 1, indicating whether the sample belongs to one class or the other.Using a different weight for each feature, the perceptron captures the different relationships between the features and the target. We can learn the optimal weights that allow the model to make accurate predictions by training a perceptron.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The ["Perceptron Algorithm for Classification in Python"](https://machinelearningmastery.com/perceptron-algorithm-for-classification-in-python/) is a great introduction to the Perceptron.</p></details>

-----------------------

## Date - 2023-06-09


## Title - Improving the test


### **Question** :

Charlotte took a test from the pharmacy to understand whether she had the flu. She lives in a town with 10,000 people, and 5% are sick.

The test is 90% accurate, and Charlotte got a positive result. Fortunately, she knows enough about probabilities to understand she is likely healthy.

**What should be the accuracy of the test for Charlotte to be likely sick?**


### **Choices** :

- The test should be at least 92.4% accurate for Charlotte to be likely sick.
- The test should be at least 94.7% accurate for Charlotte to be likely sick.
- The test should be at least 95.1% accurate for Charlotte to be likely sick.
- Charlotte is likely to be healthy no matter how accurate the test is.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>First, let's understand why Charlotte will likely be healthy even when she gets a positive result.

5% of the people that live in Charlotte's town are sick, which means that 500 individuals are sick and 9,500 are healthy.

The test is 90% accurate. If all 500 sick individuals take the test, `500 * 0.9 = 450` will be correctly diagnosed as sick, but 50 will be incorrectly diagnosed as healthy. If all 9,500 healthy individuals take the test, `9,500 * 0.9 = 8,550` will be correctly diagnosed as healthy, but 950 will be incorrectly diagnosed as sick.

Charlotte could be one of those 450 sick people who had a positive test, but she could also be one of the 950 healthy with an incorrect positive result. To determine the probability of Charlotte being sick, we can compute how likely she is to be in each group.

Out of `450 + 950 = 1,400` positive results, Charlotte is `450 / 1,400 = 32%` likely to be sick, and `950 / 1,400 = 68%` likely to be healthy.

Despite what Charlotte's test suggests, she is likely to be healthy. Knowing this, what should be the accuracy of the test to change this result?

We will need at least a 51% probability of Charlotte being healthy. That means that:

```
x = test accuracy
a = sick individuals diagnosed as sick 
b = healthy individuals diagnosed as sick

a / (a + b) = 0.51

a = 500 * x = 500x
b = 9500 * (1 - x) = 9500 - 9500x
```

We can now put it all together:

```
500x / (500x + 9500 - 9500x) = 0.51
500x / (9500 - 9000x) = 0.51
500x = 0.51 * (9500 - 9000x)
500x = 4845 - 4590x
500x + 4590x = 4845
5090x = 4845
x = 4845 / 5090
x = 0.951
```

The test should be at least 95.1% accurate for Charlotte to be likely sick.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [The Cartoon Guide to Statistics](https://amzn.to/3eg9iIo) is a fun and instructive introduction to probabilities and statistics.</p></details>

-----------------------

## Date - 2023-06-10


## Title - Studying tensors


### **Question** :

Collins is new to machine learning and has been studying tensors, an important data structure in the field.

As she dives deeper, she comes across various attributes related to tensors and wants to determine which ones accurately describe a tensor.

****Which of the following are valid attributes that represent a tensor?****


### **Choices** :

- * Rank: This attribute refers to the number of dimensions, or axes, in the tensor.
- * Interaction: This attribute describes the connections between the tensor's axes.
- * Shape: This attribute represents the number of elements in each dimension.
- * Cardinality: This attribute represents the numerical relationship between the tensor axes.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Three primary attributes define a tensor:

1. Its rank, or the number of axes.
2. Its shape or the number of dimensions.
3. Its data type or the type of data contained in it.

The rank of a tensor refers to the tensor's number of axes.

Examples:
* Rank of a matrix is 2.
* Rank of a vector is 1.
* Rank of a scalar is 0.

The shape of a tensor describes the number of elements along each dimension.

Examples:
* `()` — scalar
* `(2,)` — vector 
* `(3, 2)` — matrix
* `(3, 2, 5)` — 3D tensor

The data type of a tensor refers to the data type it stores:

Examples:
* `float32`
* `float64`
* `uint8`
* `int64`</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [Deep Learning with Python, Second Edition](https://amzn.to/3K3VZoy) covers the topic of tensors really well.* Check ["A Gentle Introduction to Tensors for Machine Learning with NumPy"](https://machinelearningmastery.com/introduction-to-tensors-for-machine-learning/) for a quick introduction to tensors.</p></details>

-----------------------

## Date - 2023-06-11


## Title - Reinforcement description


### **Question** :

Reinforcement learning, also known as reinforcement machine learning, is a subcategory of machine learning and artificial intelligence.**Which of the following is the correct description of Reinforcement learning?**


### **Choices** :

- Reinforcement learning is a type of machine learning where the model is not given any labeled training data. Instead, the model is only given a set of unlabeled data and is expected to discover the relationships and patterns within the data on its own.
- Reinforcement learning is a type of machine learning where the model is trained on labeled data, which consists of input data and corresponding correct output labels. The model can then make predictions on new, unseen data by using the patterns it learned from the training data.
- Reinforcement learning is a type of machine learning that involves training an agent to take action in an environment to maximize a reward. This is done through a process of trial and error, where the agent receives feedback in the form of rewards or punishments based on its actions and uses this feedback to adjust its behavior and improve its performance over time.
- Reinforcement learning is a type of machine learning where the training data consists of a mixture of labeled and unlabeled examples. It leverages the availability of labeled examples to improve the model's performance while still making use of the vast amount of unlabeled data that is often available.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Reinforcement learning is a type of machine learning that involves training an agent to take action in an environment to maximize a reward. This is done through a process of trial and error, where the agent receives feedback in the form of rewards or punishments based on its actions and uses this feedback to adjust its behavior and improve its performance over time.Unsupervised learning is a type of machine learning where the model is not given any labeled training data. Instead, the model is only given a set of unlabeled data and is expected to discover the relationships and patterns within the data on its own.Supervised learning is a type of machine learning where the model is trained on labeled data, which consists of input data and corresponding correct output labels. The model can then make predictions on new, unseen data by using the patterns it learned from the training data.Finally, Semi-supervised learning is a type of machine learning where the training data consists of a mixture of labeled and unlabeled examples. It leverages the availability of labeled examples to improve the model's performance while still making use of the vast amount of unlabeled data that is often available.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Supervised and Unsupervised Machine Learning Algorithms"](https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/) is an excellent introduction to the differences between supervised and unsupervised learning.* ["What is reinforcement learning? The complete guide"](https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide/) covers an introduction to Reinforcement learning.* ["What Is Semi-Supervised Learning"](https://machinelearningmastery.com/what-is-semi-supervised-learning/) covers Semi-supervised learning.</p></details>

-----------------------

## Date - 2023-06-12


## Title - Social campaign


### **Question** :

Milani works at a marketing agency that specializes in social media campaigns.

The agency has collected vast text data from various social media platforms but lacks labels. To optimize future campaigns, Milani wants to classify the sentiment of each text sample. However, she knows she cannot proceed with supervised learning without labeled data.

Milani must decide on a technique for obtaining labels for the data.

**Which of the following techniques could Milani use to label the data?**


### **Choices** :

- Hire a team to review and label each text sample manually.
- Use user reactions or comments on the text samples to automatically generate labels.
- Apply a supervised learning method to deduce the labels directly from the existing data.
- Implement semi-supervised learning to spread labels across the entire dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To use a supervised learning method, Milani needs to produce labels for the data. There are various techniques she can use to achieve this.

The most common approach to labeling data is by using human labelers. Milani could hire a team to review each text sample and assign an appropriate sentiment label (e.g., positive, negative, or neutral).

Sometimes, feedback from existing interactions can be used to create labels, known as "direct labeling." For instance, Milani could use likes, dislikes, or other reactions to text samples on social media platforms to approximate the sentiment. Although direct labeling may not always capture the "true ground truth," it is a viable approach. 

The third choice suggests using a supervised learning method to infer the labels from the existing dataset. However, this is not feasible since supervised learning requires labels, which Milani lacks.

Lastly, [semi-supervised learning](https://machinelearningmastery.com/semi-supervised-learning-with-label-propagation/) could be used if Milani already had a small portion of labeled data. The method would then generate labels for the remaining data. However, there is no indication that Milani has any labeled data, making semi-supervised learning an unsuitable option. 

Other techniques to generate labels include [active learning](https://articles.bnomial.com/active-learning) and [weak supervision](https://snorkel.ai/weak-supervision/).</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The ["Machine Learning Data Lifecycle in Production"](https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production) course in Coursera, part of the [Machine Learning Engineering for Production (MLOps) Specialization](https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops).* Check out ["Semi-Supervised Learning With Label Propagation"](https://machinelearningmastery.com/semi-supervised-learning-with-label-propagation/) for an introduction to how to use a few labels with semi-supervised learning.* ["Active Learning"](https://articles.bnomial.com/active-learning) is an introduction to a learning technique to build better-performing machine learning models using fewer training labels.* ["Weak Supervision: A New Programming Paradigm for Machine Learning"](http://ai.stanford.edu/blog/weak-supervision/) is a good article from Stanford introducing Weak Supervision.</p></details>

-----------------------

## Date - 2023-06-13


## Title - Industrial laboratory


### **Question** :

Juliana is a machine learning specialist working in an industrial laboratory. 

Her current project involves developing a predictive model to detect anomalies in the manufacturing process. Before training her model, Juliana needs to divide her dataset into training and test sets.

Before splitting the data, Juliana writes code to shuffle the datasets.

**Why is shuffling a dataset crucial before dividing it into training and test sets?**


### **Choices** :

- To ensure that class labels are evenly distributed across the training and test sets.
- To ensure that the training and test sets are equally difficult for the model to learn and evaluate.
- To ensure that the training and test sets contain the same features.
- To prevent any unintentional order or sequence in the dataset from influencing the model's performance.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Shuffling a dataset before dividing it into training and test sets is critical.

First, we can ensure that the class labels are evenly distributed across the training and test sets. This helps avoid imbalances in the distribution of class labels, which could negatively impact the model's performance during training and evaluation. For example, if one class is over-represented in the training set but under-represented in the test set, the model might learn to predict the over-represented class well but perform poorly on the under-represented class.

Second, shuffling helps ensure that the training and test sets are equally difficult for the model to learn and evaluate. If the data has some inherent order, such as being sorted by time or some other variable, the model might be exposed to certain patterns only during training, which could lead to overfitting or underfitting. By shuffling the data, we can ensure that the model is exposed to a diverse and representative data sample during training and evaluation, improving its generalization capabilities.

Finally, shuffling helps us prevent the influence of any unintentional order or sequence in the data. For example, if the data was collected in batches and each batch had some unique characteristics, not shuffling the dataset could result in the model learning these batch-specific characteristics, which are irrelevant to the problem. Shuffling the dataset reduces the risk of the model learning irrelevant patterns and improving its ability to generalize to unseen data.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems_](https://amzn.to/3SHGqsu) is one of the best books for machine learning fundamentals.* ["Machine Learning Crash Course"](https://developers.google.com/machine-learning/crash-course) is a great introduction to Machine Learning.</p></details>

-----------------------

## Date - 2023-06-14


## Title - Implementing neural networks


### **Question** :

Maggie is learning to implement neural networks and has discovered the importance of using non-linearities. 

She learned that if she doesn't add non-linearities to the model, the network won't solve the problem.

**Which of the following options will add non-linearities to Maggie's neural network?**


### **Choices** :

- Using convolution operations as part of the network.
- Using Stochastic Gradient Descent to train the network.
- Implementing the backpropagation process.
- Using Rectifier Linear Unit (ReLU) as an activation function.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>For a neural network to learn complex patterns, we must ensure that the network can approximate any function, not only linear ones. This is why we call it "non-linearities."

The way we do this is by using activation functions. 

An interesting fact: the [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) states that, when using non-linear activation functions, we can turn a two-layer neural network into a universal function approximator. This is an excellent illustration of how powerful neural networks are.

Some of the most popular activation functions are [sigmoid](https://en.wikipedia.org/wiki/Logistic_function) and [ReLU](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks). 

A [convolution operation is a linear operation](https://en.wikipedia.org/wiki/Convolution#Properties). You can check [this answer](https://ai.stackexchange.com/questions/19879/arent-all-discrete-convolutions-not-just-2d-linear-transforms) in Stack Exchange for an excellent explanation.

Finally, neither Stochastic Gradient Descent nor backpropagation has anything to do with the linearity of the network operations.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Activation function"](https://en.wikipedia.org/wiki/Activation_function) from Wikipedia to understand more about this topic.* I find the ["Universal approximation theorem"](https://en.wikipedia.org/wiki/Universal_approximation_theorem) fascinating.</p></details>

-----------------------

## Date - 2023-06-15


## Title - Supervised work


### **Question** :

Makayla is an experienced data scientist who has worked with supervised and unsupervised learning models for years. 

Recently, she was introduced to the concept of self-supervised learning. 

This is new to her. This is different.

**Which of the following statements about self-supervised learning are true?**


### **Choices** :

- Self-supervised learning is just a fancy name for unsupervised learning.
- Similar to supervised learning, self-supervised methods can judge whether their prediction is correct during training.
- Self-supervised methods require only a small number of labeled samples.
- Like unsupervised learning, self-supervised methods don't require labeled training data.


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Self-supervised methods are considered a form of unsupervised learning. However, not all unsupervised methods are self-supervised. Self-supervised methods have similarities to both supervised and unsupervised learning. 

Self-supervised learning doesn't require labeled data, similar to unsupervised learning. On the other hand, in self-supervised methods, "supervision" can be derived directly from the data. Therefore, like supervised learning, we can judge whether a prediction is true or false.

An example would be a neural network that predicts the next word given a part of a sentence. We can take the text of a book and create random samples by picking parts of sentences. We don't need any labels, but for every instance, we still know what the correct answer is.

Semi-supervised learning requires only a small number of labeled samples, but that's not how self-supervised learning works.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Self-supervised learning"](https://project.inria.fr/paiss/files/2018/07/zisserman-self-supervised.pdf) is a presentation by Andrew Zisserman covering self-supervised learning for images.* ["Self-supervised learning: The dark matter of intelligence"](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/) is an excellent post from Meta's AI Research team.</p></details>

-----------------------

## Date - 2023-06-16


## Title - Item on the list


### **Question** :

Palmer is studying different Deep Learning architectures for her upcoming Computer Vision project. 

She collects a list of potential choices and emails her colleague for feedback. Minutes later, she receives the response noticing that one of the items on Palmer's list is not a Deep Learning architecture.

**Can you help Palmer identify the item in the list that is not a Deep Learning architecture?**


### **Choices** :

- ResNet
- ImageNet
- U-Net
- VGG16


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[ImageNet](https://www.image-net.org/) is not a Deep Learning architecture but a large-scale dataset commonly used for training and benchmarking computer vision models. It contains millions of images categorized into thousands of classes. On the other hand, ResNet, U-Net, and VGG16 are popular Deep Learning architectures used for various computer vision tasks.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check the [ImageNet](https://www.image-net.org/) website for more information about one of the most popular public datasets in the world.</p></details>

-----------------------

## Date - 2023-06-17


## Title - Sister dice


### **Question** :

Alayah has a box containing a 6-sided and a 12-sided die.

She asks her sister to randomly pick one out, roll it, and share the result. Her sister tells her that the result is 2.

**What is the probability that Alayah's sister picked the 6-sided die?**


### **Choices** :

- The probability is 1/2
- The probability is 2/3
- The probability is 3/4
- The probability is 1/3


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's assume that `A` represents the event of rolling the die and getting a `2`, `B1` represents pulling out the 6-sided die, and `B2` represents pulling out the 12-sided die.

We can compute the probability that Alayah's sister picked the 6-sided die using the [Bayes theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem):

```
P(B1|A) = (P(A|B1)*P(B1))/P(A)
P(B1|A) = (1/12)/P(A)

P(A) = P(A|B1)*P(B1)+P(A|B2)*P(B2)
P(A) = 1/12 + 1/24​
P(A) = 1/8

P(B1|A) = (1/12)/(1/8)
P(B1|A) = 2/3​
```

Thus, the answer is `2/3`.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["A Gentle Introduction to Bayes Theorem for Machine Learning"](https://machinelearningmastery.com/bayes-theorem-for-machine-learning/) is a great starting point to understand the Bayes theorem and how to use it.* You can also check the Wikipedia page of [the Bayes theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem).</p></details>

-----------------------

## Date - 2023-06-18


## Title - Team tech lead


### **Question** :

Logan recently became a Team Lead at a tech company focusing on natural language processing.

A few weeks into the project, the team is working on a sentiment analysis model to categorize customer reviews into positive, negative, and neutral sentiments. Logan wants to ensure that each sentiment category is properly represented in both the training and test sets, avoiding cases where the model encounters a sentiment during testing that it hasn't seen during training or vice versa.

**Which of the following terms describes the process Logan wants her team to follow?**


### **Choices** :

- Logan asked her team to cross-validate their dataset.
- Logan asked her team to bootstrap their dataset.
- Logan asked her team to validate their dataset.
- Logan asked her team to stratify their dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Stratified sampling is a method to split a dataset to produce subsets containing a balanced proportion of samples for each category.

In Logan's case, she is working on a classification problem involving sentiment analysis. Stratified sampling will ensure that her training and test sets have approximately the same percentage of sentiment samples as the complete dataset.

For example, suppose the dataset has 5,000 positive, 10,000 negatives, and 15,000 neutral reviews. Logan wants to maintain this ratio when splitting the data into training and test sets. If the team selects 24,000 total reviews for training, Logan expects to see 4,000 positive (16.67%), 8,000 negatives (33.33%), and 12,000 neutral (50%) reviews.

Therefore, Logan wants her team to stratify their dataset.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Stratified sampling in Machine Learning"](https://medium.com/analytics-vidhya/stratified-sampling-in-machine-learning-f5112b5b9cfe) is a quick introduction to stratified sampling.* Check ["What is Stratified Cross-Validation in Machine Learning?"](https://towardsdatascience.com/what-is-stratified-cross-validation-in-machine-learning-8844f3e7ae8e) for more information about stratified cross-validation.</p></details>

-----------------------

## Date - 2023-06-19


## Title - Convolutional paper


### **Question** :

Annabelle is writing a paper about convolutional layers.Her examples are small images of size 8x8. Annabelle wants to find different combinations of parameters for the convolutional layer that give her a similar-sized output.**Assuming Annabelle inputs an 8x8 picture to a convolutional layer, which of the following parameters will give her an output size of 2x2?**


### **Choices** :

- Kernel size = 7, Padding = 0, Stride = 1
- Kernel size = 5, Padding = 0, Stride = 2
- Kernel size = 7, Padding = 2, Stride = 4
- Kernel size = 5, Padding = 2, Stride = 4


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To compute the output size, we can use the following formula:```output = 1 + (input + 2 * padding - kernel) / stride```Running every choice through this formula will give you the output of every case is the same 2x2.Convolutions have many parameters that influence the output size. We can go through the properties of the convolutional layer one by one and complete one of the examples.First, we know the input size to be 8x8.Next, we know that the kernel size is 7. This means we will move a sliding window of size 7×7 over the entire image. Assuming a stride of 1, the output of this operation will be 2x2.Adding padding to the image is a way to avoid the reduction of the size. In the first example, we didn't add any padding.Finally, we have a stride of 1. This means the sliding window will move by one row and column every time. Since that's the stride we assumed at the beginning, the final output size of the convolutional layer will be 2×2.You can apply the same thinking to every example.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* It may be difficult to imagine why some of these things happen just by reading the text, so I encourage you to go to this ["Convolution Visualizer"](https://ezyang.github.io/convolution-visualizer/) and enter the parameters of the convolutional layer from the question. It should be much more evident now why this happens.* ["Convolution arithmetic"](https://github.com/vdumoulin/conv_arithmetic) is a technical report on convolution arithmetic in the context of deep learning.</p></details>

-----------------------

## Date - 2023-06-20


## Title - Significant impact


### **Question** :

Trinity just started reading a book about machine learning and came across the concept of model coefficients. She learned that coefficients are parameters learned by a machine learning model during training. Trinity wants to understand how these coefficients can be used to determine the impact of a feature on a prediction. Specifically, she wants to know how to determine if a feature significantly impacts the prediction based on the coefficients.**How can Trinity determine if a feature significantly impacts the prediction based on the coefficients?**


### **Choices** :

- A positive coefficient indicates that a feature significantly impacts the prediction.
- A negative coefficient indicates that a feature significantly impacts the prediction.
- The coefficient with a large magnitude indicates that a feature significantly impacts the prediction.
- The coefficients do not indicate the relationship between the feature and target variable.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The coefficients in the context of a model are the parameters learned by the model during training. They determine the relationship between the input features and the output predictions.Each feature in the model is assigned a coefficient, a numerical value representing that feature's importance in the prediction.If a feature has a large magnitude coefficient, it has a strong relationship with the target variable and, therefore, significantly impacts the prediction. On the other hand, if a feature has a small magnitude coefficient, it means that it has a weak relationship with the target variable and, therefore, has a less significant impact on the prediction.The sign of the coefficient only indicates the direction of the relationship between the feature and target variable but does not indicate the strength of the relationship.In summary, the magnitude of the coefficient is the key factor that indicates the significance of a feature in the prediction.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems_](https://amzn.to/3SHGqsu) is one of the best books for machine learning fundamentals.</p></details>

-----------------------

## Date - 2023-06-21


## Title - Climate researcher


### **Question** :

Joanna is a climate change researcher.

She wants to analyze satellite images of the Amazon rainforest to spot deforestation hotspots to help policymakers act preventively. Joanna built a shallow neural network to predict deforestation patterns, but it's not as accurate as she'd like. 

A deep neural network could be better.

**What should Joanna do to transform her shallow neural network into a deep one?**


### **Choices** :

- Joanna should change the activation function in the existing layers.
- Joanna should change the type of input data she's using.
- Joanna should add more layers to the network.
- Joanna should use a different optimization algorithm.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Joanna should add more layers to the network to transform a shallow neural network into a deep one. A deep neural network is an artificial neural network with multiple layers between the input and output layers. The more layers we add, the "deeper" the network becomes.

Changing the activation function in the existing layers might affect the model's performance, but it won't make the network deeper. Activation functions play a role in introducing non-linearities, but they don't affect the depth of the network.

Changing the type of input data would not make the network deeper. Instead, it would likely require a model modification to accommodate the new data type, which doesn't address the depth issue.

A different optimization algorithm could improve the training process and performance but won't deepen the neural network. Optimization algorithms help minimize the loss function but don't increase the network's depth.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Wikipedia's definition of ["Deep neural networks"](https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks) serves as a succinct summary of what deep neural networks are.* Check ["A Layman's Guide to Deep Neural Networks"](https://towardsdatascience.com/a-laymans-guide-to-deep-neural-networks-ddcea24847fb) for a non-mathematical introduction to deep neural networks.</p></details>

-----------------------

## Date - 2023-06-22


## Title - Genre songs


### **Question** :

Leilani had been passionately working on her music recommendation system for weeks. 

She built a neural network to classify songs into different genres, and while the training phase produced impressive results, the model's performance on the test data left her dissatisfied.

After a few experiments, Leilani realized her model was overfitting.

**Which of the following strategies would you say is likely to help Leilani solve her problem?**


### **Choices** :

- Increase the complexity of the model.
- Switch to a different optimization algorithm.
- Regularize the model.
- Increase the amount of training data.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Leilani's model is doing great with the training data but struggling on the test set, which indicates that her model is not generalizing well to unseen samples.

If Leilani increases the complexity of her model, it may lead to even more overfitting. A more complex model has more capacity, which means it can easily "memorize" the training data instead of learning to generalize from it. While the model's performance on the training data might improve, its performance on the test data will likely worsen. The increased complexity can cause the model to capture the noise in the training data rather than the underlying patterns, making it less effective at handling new, unseen data.

Switching the optimization algorithm is unlikely to solve this problem. Nothing in the statement gives us any information about which algorithm Leilani is currently using, and we don't have any reason to believe it's not a good one. We also know that her model is learning the training data, which wouldn't be possible if the optimization algorithm wasn't appropriate.

Leilani could use regularization techniques to help with overfitting. Regularization discourages the model from becoming too complex or flexible, which helps prevent the model from "memorizing" the training data.

Finally, there's a chance that the training data that Leilani is using doesn't fully capture the breadth of valid samples for her problem. In other words, her training dataset might not be enough to teach the model how to predict the test data correctly. Adding more data to the training set allows Leilani to build a model that generalizes better.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.</p></details>

-----------------------

## Date - 2023-06-23


## Title - Crossroads career


### **Question** :

Madilyn is at a crossroads in her Machine Learning career. 

She's been offered two distinct opportunities: one in a research-focused environment and another in the industry. She wants to make an informed decision and seeks advice on the key differences between these paths.

**Which of the following statements accurately reflect the differences between Research Machine Learning and Production Machine Learning?**


### **Choices** :

- The data used in research environments is typically dynamic and constantly changing, while the data in production settings is often static and unchanging.
- Research environments prioritize achieving the highest model performance, while production settings focus on costs, scalability, and explainability.
- Fairness is only a concern in research environments, while production systems don't need to worry about it.
- In a research environment, a substantial amount of work is dedicated to monitoring and maintaining models, while in production environments, the focus is primarily on the initial training and validation of the model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Researchers like to test their work on popular datasets. This gives them a fair comparison with other existing methods and makes reproducing their results easily. On the other hand, production data is constantly shifting. The datasets you use to train and test your models can quickly become obsolete.

A lot of emphasis on academia centers around better algorithms and techniques. Can we solve this particular problem and do it more accurately? Can we do it faster? Research jobs are about inventing new methods and squeezing as much as possible from what we already know.

Focusing on better techniques and algorithms leads to the main priority in many research positions: achieving better "performance." It could be about higher accuracy, a faster method, or fewer constraints. These, however, aren't usually the same concerns in the industry.

Production machine learning is generally more focused on the interpretability of results and the cost of the solution. "Higher accuracy" is not the most critical metric in many cases—it's still important but usually not at the expense of other factors. This is one of the main differences between research and industry positions. 

In a research environment, most work is centered around the initial training and validation of the model. Researchers explore techniques, hyperparameters, and training methodologies to improve model performance. In production environments, a significant focus is monitoring and maintaining models to ensure their effectiveness. As new data becomes available, models might require retraining, fine-tuning, or adaptation to maintain performance. Additionally, production environments often involve integrating models into larger systems, handling data pipelines, and ensuring the reliability and stability of the deployed models.

Fairness is essential to machine learning models in research and production environments. However, the implications of fairness are often more critical in a production setting due to real-world consequences. Unfair models may negatively impact users or perpetuate existing biases, leading to ethical concerns and potential legal repercussions. Fairness is studied and addressed in research, but the direct impact on real-world scenarios may not be immediate.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The ["Machine Learning Data Lifecycle in Production"](https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production) course in Coursera, part of the [Machine Learning Engineering for Production (MLOps) Specialization](https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops).* Check ["Why You Should Care About Data and Concept Drift"](https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift) to understand the importance of monitoring machine learning models.* ["A Gentle Introduction to Concept Drift in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-concept-drift-machine-learning/) is an excellent introduction to concept drift.</p></details>

-----------------------

## Date - 2023-06-24


## Title - Half the records


### **Question** :

Celeste evaluated her model in a dataset with 10,000 records and got an accuracy of 90%.

She collected 1,000 new records and added half to the training set and the other half to the test set. After training a new model, she evaluated it with 10,500 records, and the accuracy decreased to 88%.

**Which of the following is a valid conclusion about Celeste's model?**


### **Choices** :

- Celeste's model is worse than before.
- Celeste's model is better than before.
- Celeste's model is the same as before.
- We don't know exactly whether Celeste's model is better or worse.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When Celeste added 1,000 new records and split them between the training and test sets, the distribution of the test set changed. Since the new test set has a different distribution, we cannot directly compare the model's accuracy before and after adding the new records.

The decreased accuracy from 90% to 88% does not necessarily mean the model is worse. 

It could be that the new data is more challenging for the model, or the model has improved in certain aspects but is not apparent in the overall accuracy. To accurately assess whether the model is better or worse, Celeste must evaluate the models on the same data with the same underlying distribution.

Here is a hypothetical scenario of two models illustrating how Model 2 could improve even when returning a lower accuracy:

* Model 1 accuracy on 10,000 records: 90% (9,000 records correctly classified.)
* Model 1 accuracy on the new 500 records: 10% (50 records correctly classified.)
* Model 2 accuracy on the 10,000 records: 91% (9,100 records correctly classified.)
* Model 2  accuracy on the new 500 records: 28% (140 records correctly classified.)
* Model 2 accuracy on the 10,500 records: 88% (9,240 records correctly classified.)

In this hypothetical example, Model 2 is better on the original 10,000 records (91% versus 90%) and the 500 new records (28% versus 10%.) But its accuracy is worse with the 10,500 test set (88% versus 90%.)

This is called the [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox). When comparing two experiments, we need to make sure that we evaluate the same data or at least data having the same underlying distribution. If we change the distribution, we can't compare the results and may make the wrong conclusion.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Simpson's Paradox"](https://www.britannica.com/topic/Simpsons-paradox) for a complete explanation about the paradox.* Here is a video about the [Simpson's Paradox](https://www.youtube.com/watch?v=ebEkn-BiW5k).</p></details>

-----------------------

## Date - 2023-06-25


## Title - Exploring birds


### **Question** :

Cali is an avid birdwatcher and has decided to use her passion for birds to explore the capabilities of deep neural networks. 

She gathers a diverse dataset of bird images covering different species, habitats, and behaviors. With this dataset, Cali begins experimenting with deep neural networks to see how well they can classify the images and recognize the distinct characteristics of each species.

**Which of the following statements reasonably simplifies how the network operates?**


### **Choices** :

- The earlier layers of a neural network compute simpler features than deeper layers.
- The deeper layers of a neural network compute more complex features than earlier layers.
- The earlier layers of a neural network compute more complex features than deeper layers.
- The deeper layers of a neural network compute simpler features than earlier layers.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Think of a bird picture. You'll see the eyes, feathers, beak, and other characteristics. Notice how groups of pixels form edges, shapes, and the rest of the bird's features. 

A reasonable explanation for how a neural network works are to assume that earlier layers focus on detecting more basic features, like edges and shapes of the image. Later layers could use these earlier pieces to form more recognizable shapes, like the eyes and beak of the bird.

While the network deals with pixels early on, the deeper we go into it, the more it will work with complete patterns until it reaches the output layer.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["But what is a neural network?"](https://www.youtube.com/watch?v=aircAruvnKk) is Grant Sanderson's introduction to neural networks on YouTube. Highly recommended!* [_Neural Networks and Deep Learning_](http://neuralnetworksanddeeplearning.com/index.html) is a free online book written by [Michael Nielsen](https://twitter.com/michael_nielsen).</p></details>

-----------------------

## Date - 2023-06-26


## Title - Bias balance


### **Question** :

Amari is working on a complex project involving a dataset with multiple features and non-linear relationships between them. She understands that the success of her machine learning model depends on finding the right balance between bias and variance.

A high-bias model makes more assumptions about the target function, which can lead to underfitting. On the other hand, low-bias models make fewer assumptions, allowing them to capture complex patterns more effectively.

Given the complexity of her project, Amari wants to avoid high-bias algorithms.

**Which of the following algorithms should Amari stay away from?**


### **Choices** :

- Linear Regression
- Neural Networks
- Random Forest
- Naive Bayes


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Every machine learning algorithm deals with three types of errors: bias, variance, and irreducible error. We need to focus specifically on the bias error to answer this question.

Here is what [Jason Brownlee](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) has to say about bias: "Bias are the simplifying assumptions made by a model to make the target function easier to learn."

In other words, bias refers to the model's assumptions to simplify finding answers. The more assumptions it makes, the more biased the model is.

Often, linear models are high-bias. They are easier to understand but make too many assumptions about the target function, preventing them from performing well on complex problems. Linear and logistic regression are two examples of high-bias models.

Nonlinear models are usually low-bias. Decision Trees and k-Nearest Neighbors are two examples.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Here is Jason Brownlee's article I mentioned before ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/).* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).</p></details>

-----------------------

## Date - 2023-06-27


## Title - Photos from the app


### **Question** :

Briella is developing a machine learning model to classify photos from users for their company's new app.

She wants to use a Convolutional Neural Network, but a colleague suggested using a fully-connected neural network, arguing that it's a simpler approach.

**Which of the following are good reasons for Briella to use a Convolutional Neural Network?**


### **Choices** :

- Convolutional Neural Networks can learn a hierarchy of visual features similar to the human brain, which results in better performance.
- The number of parameters required for a Convolutional Neural Network is typically smaller than that of a fully-connected network.
- Convolutional Neural Networks are usually shallower than fully-connected networks, making the training process easier and faster.
- Convolutional Neural Networks can classify images even when the training data is highly imbalanced, without additional pre-processing steps.


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A [Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN) is a much better choice when dealing with image classification problems than a fully-connected network.

A CNN can learn visual features of increasing complexity. The initial layers typically learn to recognize low-level details, like edges and colors, while deeper layers can handle more complex structures, like corners and patterns. The final layers of a CNN can learn complex representations like faces or any complex objects.

CNNs require fewer parameters than fully-connected networks and reuse the same parameters over the whole image. The number of weights in a convolutional layer depends on the kernel size and the number of channels, not the image resolution. For example, in the [AlexNet architecture](https://en.wikipedia.org/wiki/AlexNet), the five convolutional layers are responsible for only 4% of the parameters of the network. In comparison, the final three fully-connected layers contain the remaining parameters. 

A CNN is more time and memory efficient than a fully-connected network, so we can use deeper networks with many layers, which is impossible in a fully-connected network. You should always expect CNNs to be deeper than fully-connected networks.

Finally, a CNN is not inherently immune to the issues caused by imbalanced datasets. When the training data is highly imbalanced, the model will likely be biased toward the majority class, which can lead to reduced performance in the minority class. Like a fully-connected network, a CNN requires appropriate handling of imbalanced datasets to ensure accurate and unbiased performance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["A Beginner’s Guide to Convolutional Neural Networks (CNNs)"](https://towardsdatascience.com/a-beginners-guide-to-convolutional-neural-networks-cnns-14649dbddce8) is a great introduction to CNNs.* If you want to get deeper into how convolutions work, check out ["A guide to convolution arithmetic for deep learning"](https://arxiv.org/pdf/1603.07285.pdf).</p></details>

-----------------------

## Date - 2023-06-28


## Title - Testing competition


### **Question** :

Evelynn recently joined a machine learning competition, where her task is to build a model to predict outcomes based on a given dataset. 

After implementing a Decision Tree model, she's happy with its performance on the training data. However, the results are disappointing when she evaluates the model on the competition's unseen test data.

After looking for advice online, Evelynn reads she should try pruning the tree to improve the model's performance. 

**Why would pruning the tree help address Evelynn's problem?**


### **Choices** :

- Pruning the Decision Tree will decrease the model's bias.
- Pruning the Decision Tree will increase the model's variance.
- Pruning the Decision Tree will decrease the model's variance.
- Pruning the Decision Tree will not help Evelynn.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A Decision Tree is an algorithm with low bias and high variance. A Decision Tree makes almost no assumptions about the target function.

Because of its high variance, Decision Trees overfit easily to the training dataset. Evelynn encountered this issue when she tested the model on the competition's unseen test data.

To avoid overfitting, Evelynn should prune the tree. By doing so, she forces the tree to generalize better and make assumptions by reducing the number of nodes. In other words, by pruning the tree, she increases its bias and decreases its variance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) is an excellent introduction to the bias and variance tradeoff.* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).</p></details>

-----------------------

## Date - 2023-06-29


## Title - Media company


### **Question** :

Dakota is a data scientist at a media company. She is working on a project to build a machine-learning model that predicts the popularity of different types of content.The model's success will significantly impact the company's ability to provide its audience with the most engaging content.Before she trains her model, Dakota must divide her dataset into three sets: a training set, a validation set, and a test set. She knows that using each set for specific purposes is essential to ensure accurate results.**Which of the following are correct practices for using these three sets?**


### **Choices** :

- Dakota should use the test set only once to fine-tune the model's parameters.
- Dakota should use the training set only once to train the model.
- Dakota should use the validation set multiple times to fine-tune the model's parameters.
- Dakota should use the training set multiple times to train the model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The training and validation sets can be used multiple times throughout the model development process.We use a training set to train the model. This training set is a subset of the entire dataset. We use the training set multiple times throughout the model development process as the model is refined and improved.We use a validation set to tune the model's parameters and assess its performance to prevent overfitting. We evaluate the model's performance after each iteration of training.After we finish training and tuning the model, we can evaluate it on a test set, which hasn't been used during the training or validation process. The model's performance on the test set provides an estimate of how well the model will perform in real-world scenarios.We should never use the test set to fine-tune the model's parameters.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Train, Validation, Test Split for Machine Learning"](https://blog.roboflow.com/train-test-split/) goes into detail about the importance of each split.</p></details>

-----------------------

## Date - 2023-06-30


## Title - Binary neural network


### **Question** :

Gracelynn just started working on a new project where she needs to create a neural network model for classifying customer feedback.

She has been exploring different loss functions and came across binary cross-entropy. Gracelynn needs to know if this loss function would suit her neural network model.

**In which type of neural network problems should Gracelynn use binary cross-entropy as the loss function?**


### **Choices** :

- Binary cross-entropy is the loss function used for multi-class classification problems.
- Binary cross-entropy is the loss function used for multi-label classification problems.
- Binary cross-entropy is the loss function used for binary classification problems.
- Binary cross-entropy is the loss function used for regression problems.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Binary cross-entropy is the loss function we use when training binary classifiers. When working on a binary classification task, we categorize every sample into two classes. For example, given images of dogs or cats, a binary classifier will decide whether a picture shows a dog or a cat.

But binary classifiers aren't the only time we use binary cross-entropy.

Binary cross-entropy is also the loss function we use in multi-label classification problems. These are problems where we categorize every sample into one or more classes. For example, organizing movies based on the type of content, for instance, violence, adult language, smoking, or sex, where each film could belong to one or more categories.

In multi-label classification models, the output layer returns values independently. It's helpful to think of a model that outputs ten possible classes as a combination of ten different binary classifiers, and thus binary cross-entropy helps.

Neither multi-class classification nor regression problems use binary-cross entropy.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Understanding binary cross-entropy / log loss: A visual explanation"](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) is an excellent introduction to binary cross-entropy.- Another article that helps understand binary cross-entropy is ["Binary Cross Entropy/Log Loss for Binary Classification"](https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/).</p></details>

-----------------------

## Date - 2023-07-01


## Title - Trading used cars


### **Question** :

Emma worked at an online platform for trading used cars. Given her background in data science, she was tasked with building an automatic price recommendation system.

A crucial step in this project was to analyze the Manufacturer's Suggested Retail Price (MSRP) of the cars, which was the target variable for the predictive model.

One afternoon, while going through the data, she decided to plot a histogram of the MSRP to understand its distribution better.

**Which of the following statements are correct regarding the distribution plot of the MSRP?**


### **Choices** :

- The tail of the distribution is the histogram section that gradually decreases on one side.
- The head of the distribution is the area where the values are densely concentrated.
- A long tail in the histogram implies many values are widely spread away from the head.
- The head of the distribution falls right at the center of the histogram.


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Visualizing a histogram](https://en.wikipedia.org/wiki/Histogram) can definitely provide a lot of insight into the distribution of a variable.

The head of the distribution is the area where a significant number of values are concentrated. The position of this area is not necessarily at the center of the histogram, especially if the histogram is skewed to one side.

The distribution's tails are the histogram's parts that start narrowing down. When many values are spread out far from the head of the distribution, this is known as a long tail. You can see an example of a long tail in this image.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["A Complete Guide to Histograms"](https://chartio.com/learn/charts/histogram-complete-guide/) for more information about histograms.</p></details>

-----------------------

## Date - 2023-07-02


## Title - Deep Learning's rise


### **Question** :

Jenna had always been fascinated by the rise and prominence of deep learning in the tech world.

One afternoon, while on a call with her mentor discussing various aspects of her career, she couldn't help but bring up the topic of deep learning's surge in popularity.

Her mentor, a veteran in the field, agreed with her observations and mentioned that many of the principles in deep learning today were already present in the '90s. However, the circumstances then did not allow for today's boom. 

Intrigued, Jenna wondered what these "circumstances" could be.

**Which of the following reasons contributed to the rise in popularity of deep learning?**


### **Choices** :

- The development and improvement of algorithms made training deep networks possible.
- The gaming industry's massive investment in developing fast, parallel chips.
- The beginning of the latest AI Winter drew significant positive press and encouraged investments in the field.
- The proliferation of the Internet allowed the collection and distribution of massive amounts of data.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In the book [Deep Learning with Python, Second Edition](https://amzn.to/3K3VZoy), François Chollet remarks:

> (...) typical deep learning models used in computer vision or speech recognition require significantly more computational power than a standard laptop can provide. Throughout the 2000s, companies like NVIDIA and AMD pumped in billions of dollars in developing fast, massively parallel chips (graphical processing units, or GPUs) to power the graphics of progressively photorealistic video games (...)

This huge investment provided a big push to deep learning: the required computing power suddenly became readily available.

Similarly, the Internet played a significant role in fueling the popularity of deep learning. Its wide usage facilitated the collection and distribution of enormous amounts of data, which is crucial for powering these algorithms.

Moreover, several key algorithmic improvements enabled the scientific community to train deep neural networks effectively. These included:

> * Better activation functions for neural layers
> * Improved weight-initialization schemes
> * More efficient optimization schemes

However, the "AI Winter" refers to a period of reduced funding and interest in artificial intelligence research. Contrary to the statement in the third choice, it was not associated with a positive press or increased investment in the field. Hence, it's not a correct reason for the rise in popularity of deep learning.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check [_Deep Learning with Python, Second Edition_](https://amzn.to/3K3VZoy) for a great introduction to Deep Learning.</p></details>

-----------------------

## Date - 2023-07-03


## Title - Pineapple pictures


### **Question** :

Claire is a software developer at a small startup working on a quirky project: an application that can identify whether a picture contains a pineapple. 

Despite its odd nature, the team is excited about the project. They gathered a large collection of images to train the model and decided to use deep learning to build a binary classifier. The primary question now is about choosing the right activation function for the final layer of the network.

**Which activation functions could be a good candidate for the output layer?**


### **Choices** :

- Sigmoid
- Rectifier Linear Unit (ReLU)
- Softmax
- Leaky ReLU


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Claire's team is constructing a binary classifier. The goal is to structure the output layer to produce an easily interpretable result.

ReLU returns its input if it's positive or zero otherwise, while Leaky ReLU works similarly but allows a small, non-zero output for negative inputs.

These functions don't seem suitable for the final layer in Claire's model. Suppose the penultimate layer has successfully captured all the necessary information for prediction. In that case, applying ReLU or Leaky ReLU to the output layer would leave any positive values untouched and turn negative values into zero (or a tiny positive result in the case of Leaky ReLU). This isn't helpful because we can't use the magnitude of these values to differentiate between inputs.

On the other hand, the [Sigmoid function](https://en.wikipedia.org/wiki/Logistic_function) can convert its input into a range from 0 to 1. This characteristic makes it a suitable choice for binary classification models. By setting a threshold, we can interpret the output of the model. For instance, anything equal to or under `0.5` could represent the absence of a pineapple, and anything above `0.5` could indicate its presence.

Finally, the [Softmax function](https://en.wikipedia.org/wiki/Softmax_function) can turn a vector of numbers into a vector of probabilities. In the context of the binary classifier that Claire is working on, softmax could output a vector with two values: the first representing the absence of a pineapple and the second representing its presence. The team could then take the larger of these two values to make the final prediction.

It's worth noting that softmax is essentially a generalization of the sigmoid function for multi-class cases. However, softmax reduces to sigmoid when used in a binary classification context, yielding the same result.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["A Gentle Introduction To Sigmoid Function"](https://machinelearningmastery.com/a-gentle-introduction-to-sigmoid-function/) for a quick introduction to Sigmoid.* ["The Differences between Sigmoid and Softmax Activation Functions"](https://medium.com/arteos-ai/the-differences-between-sigmoid-and-softmax-activation-function-12adee8cf322) is a short comparison between these two functions.</p></details>

-----------------------

## Date - 2023-07-04


## Title - Job project


### **Question** :

Kira has been assigned a new project at her job, where she needs to design an object detection model to identify different types of birds from various camera trap images.

She wants to build a versatile model that can be adapted and used across different tasks in the future.

However, to do so, Kira needs an effective way to evaluate her model's performance, enabling her to compare various versions of her model and choose the most efficient one.

**Which evaluation metrics should Kira use to evaluate her model?**


### **Choices** :

- ROC Curve
- Precision-Recall Curve
- Recall
- F1 score


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The recall is a beneficial metric for object detection, but it doesn't provide Kira with a comprehensive view of her model's performance unless combined with precision. A model might have high recall but poor precision, which would not serve Kira's purpose well. Hence, recall alone isn't the most suitable metric for her.

An issue with [ROC Curves](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) for object detection tasks is that they require a concept of True Negatives for the computation of False Positive Rate, which is one of the axes of the ROC curve. However, in object detection, the number of bounding boxes that don't contain an object of interest is too large to compute True Negatives accurately. Therefore, ROC curves are not typically used in such scenarios.

Kira should consider using a [Precision-Recall Curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html). Similar to the ROC curve, the Precision-Recall curve doesn't depend on True Negatives. It uses the model's precision, making it more suitable for her object detection task.

Lastly, calculating the [F1-score](https://en.wikipedia.org/wiki/F-score) is another excellent option for Kira. It considers both the precision and recall of the model, providing a balanced measure of its performance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Classification: ROC Curve and AUC"](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) for an explanation of how to create and interpret a ROC curve.* For more information about Precision-Recall curves, check [Scikit-Learn's documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html).</p></details>

-----------------------

## Date - 2023-07-05


## Title - Better scheduler


### **Question** :

Kira is working on a deep-learning project to predict stock prices. She has built her model, but she's facing a challenge in getting her model to learn effectively during training.

Kira read about a concept called a learning rate scheduler and thinks it might be what she needs to help her model learn better. However, she isn't entirely sure about its purpose and how it might help her.

She decided to dig deeper and understand what a learning rate scheduler does.

**Which of the following statements correctly defines the role of a learning rate scheduler?**


### **Choices** :

- The learning rate scheduler will help the optimization algorithm get past a flat region by continuing its previous movement.
- The learning rate scheduler will adjust the learning rate during training according to a pre-defined schedule.
- The learning rate scheduler will save a copy of the network weights according to the value of the learning rate.
- The learning rate scheduler will help the optimization algorithm accelerate in one direction based on past updates.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When we train a deep learning model, the learning rate is one of the most crucial factors in controlling how the model learns. The learning rate defines how much the model changes in response to the estimated error each time the weights are updated.

Using a learning rate scheduler can significantly improve the model's learning process. The scheduler adjusts the learning rate at different stages during training according to a pre-defined schedule.

Typically, the learning rate starts relatively high to allow quick learning, and then it is gradually reduced to allow the model to converge effectively. This approach is like taking big steps when you're far from your destination and smaller steps as you get closer.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Learning Rate Scheduling"](https://d2l.ai/chapter_optimization/lr-scheduler.html) is a great introduction to learning rate schedulers.- ["How to Choose a Learning Rate Scheduler for Neural Networks"](https://neptune.ai/blog/how-to-choose-a-learning-rate-scheduler) is an article from [Neptune AI](https://neptune.ai/), focusing on some practical ideas on how to use schedulers.</p></details>

-----------------------

## Date - 2023-07-06


## Title - Facing resistance


### **Question** :

Maria has been exploring the potential of improving her deep learning model by incorporating Rectified Linear Units (ReLU) as the activation function in some model layers.

Knowing that she may face resistance from her colleagues, Maria decides to list down some of the benefits of using ReLU to persuade her team.

She has a list but would like assistance reviewing it before presenting it to her team.

**Which of the following are the advantages of using the ReLU?**


### **Choices** :

- ReLU is computationally straightforward to implement.
- ReLU provides better representational sparsity than the sigmoid and tanh activation functions because it can produce an actual zero output.
- ReLU saturates around extreme values, allowing backpropagation to converge faster and helping the network learn.
- Neural networks are easier to optimize when their behavior is as linear as possible. ReLU acts mainly as a linear function, improving our ability to optimize neural networks.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>ReLU is a piecewise linear function that provides the input directly if it is positive and zero if it's otherwise:

```
f(x) = max(x, 0)
```

ReLU will convert negative input values to zero, causing the function to saturate. However, ReLU does not saturate for positive input values because it keeps them as they are. Contrarily, functions like sigmoid and tanh do saturate, around `0` and `1` for sigmoid, and `-1` and `1` for tanh, for both positive and negative inputs.

Moreover, the saturation of ReLU does not aid in faster convergence of backpropagation. Saturation is often a challenge in neural networks and leads to problems like "[dying ReLU](https://arxiv.org/abs/1903.06733)" or the "[vanishing gradient problem](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/)."

ReLU can turn negative inputs into zero, unlike sigmoid and tanh, which approximate a zero value. Therefore, when using ReLU, it's possible to have layers with one or more nodes containing zero values, a situation referred to as "sparse representation". This property simplifies the model and can conserve computational resources while effectively representing data in a lower-dimensional space.

The implementation of ReLU is remarkably simple, particularly in comparison to sigmoid and tanh, which require expensive computational processes like exponentiation.

Lastly, optimizing neural networks is easier when their behavior is as linear as possible. ReLU behaves primarily as a linear function, and this attribute simplifies the optimization process of neural networks. ReLU is sometimes called a "piecewise linear function" because it behaves linearly for half of its input domain and vice versa.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["A Gentle Introduction to the Rectified Linear Unit (ReLU)"](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) for an introduction to ReLU.* The [_Deep Learning_](https://amzn.to/3MqvoTQ) book is a great resource.</p></details>

-----------------------

## Date - 2023-07-07


## Title - Well-regarded distance


### **Question** :

Kalani is working on a machine learning project involving clustering data points based on similarity. She knows that selecting the right distance metric is crucial for her clustering algorithm to produce meaningful results.

While exploring various distance metrics, she comes across the Euclidean distance, which is widely used and well-regarded. To ensure she understands the concept correctly, she decides to research the properties of Euclidean distance.

**From the following list, select every correct statement about the Euclidean distance.**


### **Choices** :

- The Euclidean distance between two distinct points is always positive.
- The Euclidean distance is always zero for any pair of identical points.
- The Euclidean distance between two points is affected by the order of the dimensions, so swapping the order of dimensions would change the computed distance.
- The Euclidean distance can be computed as the dot product of the differences between the corresponding coordinates.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In one or more dimensions, we can use the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance). For example, in a line, the distance between two points is the numerical difference between their coordinates. In a plane, the distance is the [Pythagorean distance](https://en.wikipedia.org/wiki/Pythagorean_theorem). 

But can we use it in multidimensional spaces?

The answer is yes; the Euclidean distance works in [multidimensional spaces](https://hlab.stanford.edu/brian/euclidean_distance_in.html). Intuitively, this should make sense because we can use it as the metric to compute the distance between multi-feature observations in our dataset.

The distance from a point _p_ to another point _q_ is the same regardless of whether we start from _p_ or _q_. This distance is always a positive value as long as _p_ and _q_ are different points. If _p_ and _q_ are the same point, the distance is 0.

Finally, the Euclidean distance is the square root of the sum of the squared differences between corresponding coordinates, not the dot product of the differences.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Euclidean Distance In 'n'-Dimensional Space"](https://hlab.stanford.edu/brian/euclidean_distance_in.html) for a summary and visualization of how the Euclidean distance works in multi-dimensional spaces.* ["Euclidean distance"](https://en.wikipedia.org/wiki/Euclidean_distance) and ["Euclidean space"](https://en.wikipedia.org/wiki/Euclidean_space) are Wikipedia articles that will help with this topic.</p></details>

-----------------------

## Date - 2023-07-08


## Title - Her first day


### **Question** :

The Gradient Descent algorithm has three variations: Mini-Batch Gradient Descent, Stochastic Gradient Descent, and Batch Gradient Descent.

This was a lot for Fatima's first day.

Although she understood the overarching concept of Gradient Descent, she had difficulty understanding the precise workings of Stochastic Gradient Descent.

**Which of the following assertions accurately defines the Stochastic Gradient Descent algorithm?**


### **Choices** :

- Stochastic Gradient Descent uses a single sample of data during every iteration.
- Stochastic Gradient Descent determines the optimal amount of data required to compute the gradient of the cost function.
- Stochastic Gradient Descent uses a batch of data (more than one sample but fewer than the entire dataset) during every iteration.
- Stochastic Gradient Descent uses all available data once during every iteration.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Here's a simplified explanation of the Gradient Descent algorithm's working: We select samples from the training dataset, feed them into the model, and assess the disparity between our results and the expected outcomes. This "error" is then used to compute the necessary changes to the model weights to enhance the results.

A key decision in this process involves the number of samples used per iteration to feed the model. There are three choices we can make:

* Use a single sample of data.
* Use all available data.
* Use a portion of the data.

When we use a single data sample per iteration, we call it "Stochastic Gradient Descent." Essentially, the algorithm uses one sample to compute the updates.

"Batch Gradient Descent" refers to using the entire data set in a single iteration. After processing every sample, the algorithm takes the entire dataset to compute the updates.

Lastly, "Mini-Batch Gradient Descent" involves using a portion of the data—more than a single sample but less than the entire dataset. This algorithm operates like Batch Gradient Descent, the only difference being the number of samples used.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["The wrong batch size is all it takes" ](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) explains how different batch sizes influence the training process of neural networks using gradient descent.* ["A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size"](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) covers Batch and Mini-Batch Gradient Descent.* Check ["An overview of gradient descent optimization algorithms"](https://www.ruder.io/optimizing-gradient-descent/) for a deep dive into gradient descent and every one of its variants.</p></details>

-----------------------

## Date - 2023-07-09


## Title - Project presentation


### **Question** :

Lucy is preparing for her data science project presentation.

In her project, she has been working with a database related to car performance and specifications. Throughout her work, she had to deal with different types of features in the dataset.

She plans to explain the handling of various features during her presentation, with special emphasis on ordinal features.

However, Lucy has trouble remembering the exact definition of an "ordinal feature."

**Which of the following definitions correctly summarizes what an ordinal feature is?**


### **Choices** :

- An ordinal feature is a categorical variable with ten or more possible values.
- Any feature used in a machine learning model is an ordinal feature.
- An ordinal feature is a categorical variable with fewer than ten possible values.
- An ordinal feature is a categorical variable with a meaningful order.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Lucy can refer to the ["Ordinal and One-Hot Encodings for Categorical Data"](https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/) article by Jason Brownlee to recall the definition:

> [An ordinal variable is a] variable that comprises a finite set of discrete values with a ranked ordering between values.

The ordering of values is the distinguishing feature of an ordinal variable. For instance, a feature denoting safety ratings with values such as "low," "medium," and "high" is ordinal because there is a clear order in these values.

The number of possible values does not define an ordinal feature. Also, machine learning models can use features that aren't ordinal. For instance, machine learning models commonly use numerical and nominal features.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Ordinal and One-Hot Encodings for Categorical Data"](https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/) for an explanation about ordinal features.</p></details>

-----------------------

## Date - 2023-07-10


## Title - Rapid expansion


### **Question** :

With the rapid expansion of her digital health platform, Rachel observed an exponential increase in their core dataset size.

Their deployed neural network model began to exhibit serious issues. The problem was clear: the platform collected more features from a diverse user base. The existing model was not fully utilizing the data available, leading to underfitting.

Rachel knew they had to increase the capacity of the model.

**What measures can Rachel take to enhance the capacity of her neural network model?**


### **Choices** :

- Rachel should increase the number of hidden layers in her neural network.
- Rachel should increase the regularization applied to her model.
- Rachel should raise the learning rate for training her model.
- Rachel should increase the batch size for training her model.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>This segment from ["How to Control Neural Network Model Capacity With Nodes and Layers"](https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers/) can be useful:

> The capacity of a deep learning neural network model controls the scope of the types of mapping functions that it is able to learn. (...) The capacity of a neural network model is defined by configuring the number of nodes and the number of layers.

Rachel has three options to increase her network's capacity:
* She can add more hidden layers.
* She can increase the number of nodes on each layer.
* She can combine both of these strategies.

By increasing her network's capacity, Rachel will improve the model's generalization power, resulting in better performance as the refined model considers more data.

The learning rate, regularization methods, and batch size don't impact the network's capacity.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [How to Control Neural Network Model Capacity With Nodes and Layers](https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers/)* [The capacity of feedforward neural networks](https://www.math.uci.edu/~rvershyn/papers/bv-capacity-neural-networks.pdf)</p></details>

-----------------------

## Date - 2023-07-11


## Title - Advising a company


### **Question** :

Abigail was the first to join the video conference.

Her team has been advising a company working on a deep-learning model. They've been at a standstill for a while, and Abigail's task is to move things forward.

Just ten minutes into the call, Abigail figured out the problem.

During the training of their network and the backpropagation process, the gradients got smaller until they almost reached zero, leaving the weights in the lower layers unmodified.

Abigail identified this as the vanishing gradient problem. Now all she needs to do is hypothesize about why this is happening.

**Which of the following could be causing the model to suffer from the vanishing gradient problem?**


### **Choices** :

- The hidden layers of the model use the ReLU activation function.
- The model uses batch normalization.
- The hidden layers of the model use the sigmoid activation function.
- The hidden layers of the model use the tanh activation function.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>If the gradients of the loss function approach zero, the model will stop learning because the network will stop updating the weights. This phenomenon is known as the [Vanishing Gradient Problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and it's very common when using the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) and [tanh](https://www.sciencedirect.com/topics/mathematics/hyperbolic-tangent-function) activation functions in deep neural networks.

The sigmoid and tanh functions squeeze a large input space into a value between `[0..1]` and `[-1..1]`, respectively. Therefore, large changes in the input of these functions cause small changes in the output. On top of that, both functions saturate when their input grows extremely large or small. Sigmoid saturates at `0` and `1`, tanh saturates at `-1` and `1`. The derivatives at these extremes are very close to zero. 

This is not a problem if we are building a shallow network, but as we add more and more layers using these activation functions, the gradients will eventually become too small, and the network will stop learning.

[ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), on the other hand, is a way to solve the vanishing gradient problem. ReLU is much less likely to saturate, and its derivative is `1` for values larger than zero. This means that the second choice is an incorrect answer.

Finally, [batch normalization](https://en.wikipedia.org/wiki/Batch_normalization) is another way to mitigate the vanishing gradient problem. If we normalize the input to a layer using a sigmoid activation function, the values won't reach the edges and will stay around the area where the derivative isn't too small.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The Wikipedia page of ["Vanishing gradient problem"](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) has a great explanation of this issue.* Check ["How to Fix the Vanishing Gradients Problem Using the ReLU"](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/) for an explanation of how ReLU fixes this problem.</p></details>

-----------------------

## Date - 2023-07-12


## Title - Artificial game


### **Question** :

Nayeli is planning a game night with her friends and wants to introduce them to something new: a basic artificial intelligence game! 

She thinks it would be fun if they could play tic-tac-toe against a computer that can learn how to win. Nayeli discovered she could create a program using a neural network to help the computer learn.

She's planning to start simple, with a neural network with only one hidden layer. But she wants to explain this to her friends in the simplest way possible.

**What is the best way for Nayeli to talk about this type of neural network?**


### **Choices** :

- A hidden neural network
- A shallow neural network
- A recurrent neural network
- A deep neural network


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Nayeli is planning to use the kind of network for her tic-tac-toe game called a "shallow neural network." This is a special term that computer scientists use when a network only has one hidden layer.

A neural network with many hidden layers is known as a "deep neural network." But Nayeli's network only has one layer, so it's not deep - it's shallow!

The terms "hidden neural network" and "recurrent neural network" are not the right ones to use here. A "hidden neural network" isn't a term that's used in computer science. And while "recurrent neural network" is a real term, it describes a different kind of network that's not the one Nayeli is using for her game.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["But what is a neural network?"](https://www.youtube.com/watch?v=aircAruvnKk) is Grant Sanderson's introduction to neural networks on YouTube. Highly recommended!* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html) is a free online book written by [Michael Nielsen](https://twitter.com/michael_nielsen).</p></details>

-----------------------

## Date - 2023-07-13


## Title - Inherently different


### **Question** :

Raelyn, a deep learning enthusiast, is often tripped up by the differences between multi-class and multi-label classification problems.

In multi-class classification, every sample in the dataset belongs to one class. Conversely, in multi-label classification, every instance can belong to one or more categories. The similarity can lead to confusion, but they are inherently different.

A common point of confusion for Raelyn is remembering the appropriate loss function to train a neural network to tackle these two types of problems.

**Which two of the following statements accurately describe the situation?**


### **Choices** :

- Binary cross-entropy is typically used for multi-class classification problems.
- Binary cross-entropy is the go-to loss function for multi-label classification problems.
- Categorical cross-entropy is commonly employed for multi-class classification problems.
- Categorical cross-entropy is the standard loss function for multi-label classification problems.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In [multi-class classification](https://en.wikipedia.org/wiki/Multiclass_classification) tasks, the objective of the model's output layer is to predict the class that best fits the network's input. The softmax function is typically employed in this scenario because it converts the output scores into probabilities for each class.

The categorical cross-entropy loss function measures the dissimilarity between two probability distributions. Given this functionality, it pairs perfectly with a softmax output layer when handling multi-class classification tasks.

On the other hand, [multi-label classification](https://en.wikipedia.org/wiki/Multi-label_classification) models aim to return independent values as output. The sigmoid function, which translates output scores to a value between 0 and 1, suits this requirement.

The principles of binary classification apply to multi-label classification problems but with multiple sigmoid outputs instead of just one. Conceptually, it's helpful to consider a model predicting ten potential classes as a combination of ten binary classifiers. The binary cross-entropy loss function, frequently used to train binary classification models, is aptly suited to this context.

In conclusion, multi-class classification models generally use a softmax output layer combined with the categorical cross-entropy loss function. In contrast, multi-label classification models pair a sigmoid output layer with the binary cross-entropy loss function.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Binary crossentropy"](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/binary-crossentropy) for an explanation of how Binary Cross-Entropy works.* Check ["Categorical crossentropy"](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy) for an explanation of how Categorical Cross-Entropy works.</p></details>

-----------------------

## Date - 2023-07-14


## Title - Adversarial validation


### **Question** :

Zoe is working on a machine learning project where her model is underperforming on the test data. 

She suspects the training and test datasets might have different distributions, so she uses adversarial validation to explore this possibility.

**What's the correct way to set up adversarial validation?**


### **Choices** :

- Combine the training dataset (excluding the target variable) with the test dataset and assign a new binary target, with 1 for test samples and 0 for training samples.
- Combine the training dataset (excluding the target variable) with the test dataset and assign a new binary target, with 0 for test samples and 1 for training samples.
- Split the training dataset into two parts: one for training and one for validation. Assign a new binary target, with 1 for validation and 0 for training samples.
- Split the test dataset into two parts and assign a new binary target to each split, with 1 for the first split and 0 for the second split.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Popular validation techniques, like cross-validation, allow you to test your models on unseen data if that data comes from the same distribution as your training dataset. Unfortunately, that's not always the case, and even slight differences between the training and test data will considerably affect the result of your model.

[Adversarial validation](https://articles.bnomial.com/adversarial-validation) is a technique to estimate the difference between your training and test data. [_The Kaggle Book_](https://amzn.to/3kbanRb) introduces it as follows:

> [adversarial validation] was long rumored among Kaggle participants and transmitted from team to team until it emerged publicly, thanks to a post by Zygmunt Zając on his FastML blog.

To set it up, you will create a new dataset by joining the training and test data. The target of that new dataset is a binary variable differentiating the training and test samples. You can determine how easy it's to separate both datasets by running a classifier on that new data.

Adversarial validation relies on computing the [ROC-AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc), a graph showing the True Positive Rate and the False Positive Rate at different classification thresholds. The area under this curve (AUC) measures the model's performance. A perfect model will have an area of `1.0`, while a model that only makes mistakes will have an area of `0.0`.

If you run the classifier and the ROC-AUC is around `0.5`, you will know that the training and test data are not easily distinguishable, which is good because it means the data comes from the same distribution. If the ROC-AUC is too high—closer to `1.0`—the classifier can tell training and test data apart, which means they come from a different distribution.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Adversarial validation"](https://articles.bnomial.com/adversarial-validation) is a great introduction to adversarial validation.* Check ["What is Adversarial Validation?"](https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation/notebook) for a discussion about this technique in Kaggle.* [_The Kaggle Book_](https://amzn.to/3kbanRb) is an amazing reference for those looking to participate in Kaggle.</p></details>

-----------------------

## Date - 2023-07-15


## Title - Clean data


### **Question** :

Sophia is a data scientist at a digital marketing agency.

She recently acquired a new dataset from a client, which contains details about customers' online behavior. However, she noticed that many values were missing, several fields were incomplete, and some data was corrupted.

Sophia understands the importance of clean data for building a successful Machine Learning model, so she addressed these issues before moving forward.

During a meeting with her team, Sophia discusses potential strategies for dealing with the problematic dataset. 

**Which of the following are valid techniques Sophia and her team could use to handle the problems with their data?**


### **Choices** :

- Predict the missing values using a separate Machine Learning model.
- Use a machine learning algorithm that is robust to missing values.
- Drop any rows or columns that contain missing or corrupted data.
- Replace missing values with the mean, median, mode, or other imputation techniques.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Replacing missing values with the mean, median, mode, or other [imputation](https://en.wikipedia.org/wiki/Imputation_(statistics)) techniques is an excellent approach to handling this problem. Remember that, for imputation to work, we need to apply it to those features where most of the data is in good shape and only a few values are missing. If the dataset contains features with that characteristic, imputation will help. 

Sometimes, a row or column is in such poor shape that the best approach is to drop it altogether. Imagine a feature where only a small percentage of rows have values, or most of the data is corrupted. In those cases, getting rid of the data is the appropriate approach. 

A more advanced technique commonly used is to predict missing values using a separate model. For example, you could use a linear regression model to fill in the blanks on one specific column of data. This approach considers the correlation between other features and the column with missing values, potentially producing good results. 

Finally, we can use a particular implementation of a machine learning algorithm that is robust to missing values. For example, we could run the [k-Nearest Neighbors algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) and ignore a column with missing values when computing the distance. However, this depends on the algorithm's implementation, so you must watch out for that.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Handling Missing Values"](https://www.kaggle.com/dansbecker/handling-missing-values) is a practical tutorial on how to handle missing values.</p></details>

-----------------------

## Date - 2023-07-16


## Title - Semi-supervised description


### **Question** :

Semi-supervised learning, also known as semi-supervised machine learning, is a subcategory of machine learning and artificial intelligence.**Which of the following is the correct description of Semi-supervised learning?**


### **Choices** :

- Semi-supervised learning is a type of machine learning where the model is not given any labeled training data. Instead, the model is only given a set of unlabeled data and is expected to discover the relationships and patterns within the data on its own.
- Semi-supervised learning is a type of machine learning where the model is trained on labeled data, which consists of input data and corresponding correct output labels. The model can then make predictions on new, unseen data by using the patterns it learned from the training data.
- Semi-supervised learning is a type of machine learning that involves training an agent to take action in an environment to maximize a reward. This is done through a process of trial and error, where the agent receives feedback in the form of rewards or punishments based on its actions and uses this feedback to adjust its behavior and improve its performance over time.
- Semi-supervised learning is a type of machine learning where the training data consists of a mixture of labeled and unlabeled examples. It leverages the availability of labeled examples to improve the model's performance while still making use of the vast amount of unlabeled data that is often available.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Semi-supervised learning is a type of machine learning where the training data consists of a mixture of labeled and unlabeled examples. It leverages the availability of labeled examples to improve the model's performance while still making use of the vast amount of unlabeled data that is often available.Unsupervised learning is a type of machine learning where the model is not given any labeled training data. Instead, the model is only given a set of unlabeled data and is expected to discover the relationships and patterns within the data on its own.Supervised learning is a type of machine learning where the model is trained on labeled data, which consists of input data and corresponding correct output labels. The model can then make predictions on new, unseen data by using the patterns it learned from the training data.Finally, Reinforcement learning is a type of machine learning that involves training an agent to take action in an environment to maximize a reward. This is done through a process of trial and error, where the agent receives feedback in the form of rewards or punishments based on its actions and uses this feedback to adjust its behavior and improve its performance over time.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Supervised and Unsupervised Machine Learning Algorithms"](https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/) is an excellent introduction to the differences between supervised and unsupervised learning.* ["What is reinforcement learning? The complete guide"](https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide/) covers an introduction to Reinforcement learning.* ["What Is Semi-Supervised Learning"](https://machinelearningmastery.com/what-is-semi-supervised-learning/) covers Semi-supervised learning.</p></details>

-----------------------

## Date - 2023-07-17


## Title - Handwritten talent


### **Question** :

Wynter, a talented data scientist, decided to take on a new challenge: creating a Machine Learning model to analyze and classify handwritten digits.

In preparation for the project, she needed to familiarize herself with "Tensors," a key concept when working with deep learning libraries.

Wynter dedicated significant time to learning about tensors and their properties. Eager to share her newfound knowledge, she taught a workshop at a local coding event.

**During the workshop, Wynter explains the following points about tensors. Are they accurate?**


### **Choices** :

- A vector that contains only one number is called a "scalar" or a rank-1 tensor. An example of a scalar is any numeric value like a person's age or today's temperature.
- An array of numbers is called a "vector" or a rank-2 tensor. An example of a vector is an array containing the age of every person in a dataset.
- An array with two dimensions is called a matrix or a rank-2 tensor. An example is an array containing the age, sex, height, and country of birth of every person in a dataset.
- An array with three dimensions is a rank-3 tensor. An example is a 3D array containing the information of an image: for every `x` and `y` position, it contains a vector containing the values of each channel.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's get something out of the way: This question is about tensors as we use them in deep learning, not the mathematical definition of a tensor.

The rank of a tensor refers to its number of axes—or dimensions. For example:

* The rank of a scalar is 0.
* The rank of a vector is 1.
* The rank of a matrix is 2.
* The rank of a 3D tensor is 3.

A scalar, or 0D tensor, has a rank of 0 and contains a single number. These are also called "0-dimensional tensors." 

A vector, or 1D tensor, has a rank of 1 and represents an array of numbers.

A matrix, or 2D tensor, has a rank of 2 and represents an array of vectors. We refer to the two axes of a matrix as "rows" and "columns."

You can obtain higher-dimensional tensors (3D, 4D, etc.) by packing lower-dimensional tensors in an array. For example, packing a 2D tensor in an array gives you a 3D tensor. 

For example, to store a color image, we need three dimensions: one representing the image's width, another representing the height, and a final dimension for the color channels. Assuming we have three channels (red, blue, and green), you can think of having three different matrices, each containing the pixels for each one of the channels.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Deep Learning with Python_, Second Edition](https://amzn.to/3K3VZoy) covers the topic of tensors really well.* Check ["A Gentle Introduction to Tensors for Machine Learning with NumPy"](https://machinelearningmastery.com/introduction-to-tensors-for-machine-learning/) for a quick introduction to tensors and practical code.</p></details>

-----------------------

## Date - 2023-07-18


## Title - Sold monthly


### **Question** :

Anna has just landed her dream job.

She is working for a big fashion retailer. Thousands of products are sold monthly, and Anna plans to assist the company in understanding its customers better.

After a week of examining all the data they collected about each customer, Anna started pondering a suitable method to categorize them into clusters based on their attributes.

The challenge is that Anna is uncertain about the optimal approach. Should she classify customers based on their purchasing habits, or would it be better to do it by age? What about the type of apparel or spending capacity?

**How would you tackle this problem if you were in Anna's position?**


### **Choices** :

- Use a semi-supervised learning algorithm to process the data, thus leveraging supervised and unsupervised techniques.
- Use an unsupervised learning algorithm to find interesting ways to categorize customers.
- Define a few categories beforehand, and train a supervised learning algorithm to sort every customer into one.
- Apply a supervised learning algorithm to discover potentially interesting ways to categorize customers.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Customer segmentation](https://towardsdatascience.com/customer-segmentation-with-machine-learning-a0ac8c3d4d84) is a popular field where you try to find similar characteristics among your customers. It's the perfect opportunity to use unsupervised learning: a clustering algorithm.

For example, Anna could use [K-Means](https://en.wikipedia.org/wiki/K-means_clustering) to find interesting patterns and group together the customers that share them. A critical distinction is that she doesn't need to consider the segments preemptively; the clustering algorithm will find them for her.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Customer Segmentation with Machine Learning"](https://towardsdatascience.com/customer-segmentation-with-machine-learning-a0ac8c3d4d84) for a quick introduction to Customer Segmentation.* ["10 Clustering Algorithms With Python"](https://machinelearningmastery.com/clustering-algorithms-with-python/) will introduce you to 10 different clustering algorithms.</p></details>

-----------------------

## Date - 2023-07-19


## Title - Watch manufacturer


### **Question** :

Dahlia is a data scientist implementing a Machine Learning model for the logistic department of a watch manufacturer. She is using Gradient Descent to train her model.

Different approaches to Gradient Descent depend on the number of data samples used in each iteration to compute the error. Dahlia comes across the term "Mini-Batch Gradient Descent" and wants to understand how it works.

**Which of the following statements accurately describes Mini-Batch Gradient Descent?**


### **Choices** :

- Mini-Batch Gradient Descent calculates the error using a single data sample in each iteration.
- Mini-Batch Gradient Descent calculates the error using all available data samples in each iteration.
- Mini-Batch Gradient Descent calculates the error using an average of the entire training set in each iteration.
- Mini-Batch Gradient Descent calculates the error using a batch of samples (more than one but fewer than the entire dataset) in each iteration.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Gradient Descent is a popular optimization algorithm in machine learning. It minimizes an objective function by iteratively updating the model weights based on the function's gradient. The number of samples used to compute the gradient in each iteration can vary.

Using a single sample is called Stochastic Gradient Descent (SGD). Using all the data at once is called Batch Gradient Descent. Using a batch of data—more than one sample but fewer than the entire dataset—is called Mini-Batch Gradient Descent.

Mini-Batch Gradient Descent strikes a balance between computation speed and update accuracy. It leverages the benefits of both Stochastic Gradient Descent and Batch Gradient Descent by processing multiple samples in each iteration. This approach allows for more accurate updates than SGD while being computationally more efficient than Batch Gradient Descent, which requires loading the entire dataset into memory.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["The wrong batch size is all it takes" ](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) explains how different batch sizes influence the training process of neural networks using gradient descent.* ["A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size"](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) covers Batch and Mini-Batch Gradient Descent.* Check ["An overview of gradient descent optimization algorithms"](https://www.ruder.io/optimizing-gradient-descent/) for a deep dive into gradient descent and every one of its variants.</p></details>

-----------------------

## Date - 2023-07-20


## Title - Plant species


### **Question** :

Camilla has recently been promoted to a team leader position at a company focusing on image recognition.

Although she lacks extensive management experience, Camilla knows the importance of implementing a robust process for her new team.

A month into their project, the team is working on identifying various plant species based on user images. Camilla wants to ensure that each plant species is adequately represented in the training and test sets, preventing situations where the model encounters a species during testing that it hasn't seen during training or vice versa.

**Which of the following terms describes the process Camilla wants her team to follow?**


### **Choices** :

- Camilla asked her team to stratify their dataset.
- Camilla asked her team to cross-validate their dataset.
- Camilla asked her team to validate their dataset.
- Camilla asked her team to bootstrap their dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Stratified sampling is a method used to split a dataset to produce subsets containing a balanced proportion of samples for each category.

In Camilla's case, she is working on a classification problem involving plant species identification. Stratified sampling will ensure that her training and test sets have approximately the same percentage of plant species samples as the complete dataset.

For example, suppose the dataset has 3,000 images of roses, 7,000 images of tulips, and 10,000 of daffodils. Camilla wants to maintain this ratio when splitting the data into training and test sets. If the team selects 16,000 total images for training, Camilla expects to see 2,400 roses (15%), 5,600 tulips (35%), and 8,000 daffodils (50%).

Therefore, Camilla wants her team to stratify their dataset.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Stratified sampling in Machine Learning"](https://medium.com/analytics-vidhya/stratified-sampling-in-machine-learning-f5112b5b9cfe) is a quick introduction to stratified sampling.* Check ["What is Stratified Cross-Validation in Machine Learning?"](https://towardsdatascience.com/what-is-stratified-cross-validation-in-machine-learning-8844f3e7ae8e) for more information about stratified cross-validation.</p></details>

-----------------------

## Date - 2023-07-21


## Title - Working manufacturer


### **Question** :

Harley is a data scientist at a cellphone manufacturer working on a model to predict customer satisfaction based on various phone features.

Before exploring more complex models, she wants to compare a simple neural network and a linear regression model.

**Which of the following statements are true for both Harley's neural network and linear regression model?**


### **Choices** :

- Both models need numeric input features, so Harley must convert non-numeric features.
- Both models require numeric inputs between 0 and 1, so Harley must standardize the values.
- The result from both models is a probability vector.
- Both models' results involve a weighted sum of the input features, but neural networks introduce non-linearities through activation functions.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Although it is generally true that both models would perform better with scaled or standardized input features, neither model explicitly requires features to be within the range of 0 to 1.

Both neural networks and linear regression models require input features to be numeric. They cannot handle categorical features directly, so Harley must transform non-numeric features before using them in either model.

The output of a linear regression model is a single numerical value, not a vector of probabilities. Neural networks do not necessarily have to produce such output either.

While linear regression and neural networks involve a weighted sum of input features, neural networks introduce non-linearities through activation functions. This key difference makes neural networks more powerful than linear regression.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Linear Regression v.s. Neural Networks"](https://towardsdatascience.com/linear-regression-v-s-neural-networks-cd03b29386d4) for comparing these two techniques.* ["3 Reasons Why You Should Use Linear Regression Models Instead of Neural Networks"](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html) is another great article talking about their differences.* ["Linear Regression for Machine Learning"](https://machinelearningmastery.com/linear-regression-for-machine-learning/) is a great introduction to linear regression.* ["Intro to Neural Networks"](https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/video-lecture) is a good summary of neural networks.</p></details>

-----------------------

## Date - 2023-07-22


## Title - Up and down


### **Question** :

Sophie is a machine learning student working late into the night.

She's been trying to train a neural network using mini-batch gradient descent. The issue that keeps her up is the training loss which keeps going up and down instead of consistently decreasing.

Sophie is on a tight schedule and can only try a couple more things to rectify this before moving on.

**What do you suggest Sophie should try next to address the issue? Choose all that apply.**


### **Choices** :

- Increase the batch size to ensure a more diverse sample set in each batch.
- Return to the dataset and ensure it's balanced correctly, as this could be a data problem.
- Increase the learning rate to take larger steps toward the gradient.
- Decrease the learning rate to prevent skipping the local minimum.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The oscillating training loss that Sophie is experiencing is less likely to be caused by an imbalanced dataset. If that were the case, the model would struggle to learn, but the loss wouldn't oscillate in this manner.

Increasing the learning rate would allow more significant steps in the direction of the gradient. However, this might lead to overshooting the local minima, which can cause oscillations as the model "bounces" back and forth.

On the other hand, decreasing the learning rate can help address the issue. A lower learning rate would lead to smaller steps, making it less likely for the model to miss the local minimum.

Additionally, if Sophie uses a very small batch size, outliers in the data could cause significant shifts in the training loss from one batch to the next. This could also lead to the observed oscillations in loss. Therefore, increasing the batch size to get a more diverse set of samples in each batch could help stabilize the training loss.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["The wrong batch size is all it takes" ](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) explains how different batch sizes influence the training process of neural networks using gradient descent.* ["What could an oscillating training loss curve represent?"](https://ai.stackexchange.com/questions/14079/what-could-an-oscillating-training-loss-curve-represent) is a StackExchange question that will help you answer this question.* Check ["Why is my training loss fluctuating?"](https://www.researchgate.net/post/Why_is_my_training_loss_fluctuating) for another set of answers covering this problem.</p></details>

-----------------------

## Date - 2023-07-23


## Title - Electrocardiogram readings


### **Question** :

Emily is struggling with her machine learning project.

She is developing a model to detect early signs of a rare heart condition using electrocardiogram (ECG) readings. The issue is that Emily has a dataset with significantly more healthy samples than those with the disease, leading her model to misclassify ill patients as healthy frequently.

Acquiring more data for such a rare condition is extremely tough, and stringent privacy laws further complicate matters.

Emily is in dire need of alternative solutions.

**Which of the following options could Emily consider to enhance her model's performance?**


### **Choices** :

- Assign a higher weight to the samples indicating the rare heart condition.
- Augment the dataset with slightly modified copies of the images showing the disease.
- Reduce the learning rate to allow the model to learn underrepresented samples.
- Swap out her model with a Decision Tree as these are more resistant to imbalanced data.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Handling imbalanced datasets is a typical challenge in machine learning, and there are numerous strategies to tackle it. Often, a combination of multiple approaches leads to the best results.

One technique that's typically simple to implement is to give higher weight to underrepresented classes in the loss function. This strategy results in a higher penalty for the model if it misclassifies a sample from the rare class. In essence, increasing the weight of certain classes makes them "more important" from the loss function's perspective. Every significant machine learning framework offers a way to manipulate the weight assigned to each class.

Another effective approach to address this issue is to augment the underrepresented class. Data augmentation involves generating new training data through minor modifications to existing samples. For example, ECG readings can be augmented by introducing noise or slight time variations.

It's worth mentioning that these aren't the only ways to deal with imbalanced datasets. Other strategies could involve oversampling the underrepresented classes, downsampling the dominant class, or creating additional synthetic data.

However, Decision Trees are not inherently more robust to imbalanced data, and reducing the learning rate does not directly help the model to better learn from the underrepresented class.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check out ["A Gentle Introduction to Imbalanced Classification"](https://machinelearningmastery.com/what-is-imbalanced-classification/) for a description of how to tackle imbalanced problems.</p></details>

-----------------------

## Date - 2023-07-24


## Title - Nominal types


### **Question** :

Isabella is preparing for a project in her machine learning class.

She's new to the field, and they've been studying structured data and various feature types over the past weeks.

Her assignment is to eliminate all nominal features from a given dataset.

Isabella is facing a problem: she can't recall the definition of "nominal features." 

**Which of the following descriptions correctly describes a nominal feature?**


### **Choices** :

- A nominal feature is a categorical variable with more than ten possible values.
- A nominal feature is a categorical variable with fewer than ten potential values.
- A nominal feature is a categorical variable with an established order.
- A nominal feature is a categorical variable without a meaningful order.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>According to Jason Brownlee in his ["Ordinal and One-Hot Encodings for Categorical Data"](https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/) article:

> [A nominal variable is a] variable that comprises a finite set of discrete values with no relationship between values.

In other words, nominal features are categorical features where the order of values does not carry any significance. For instance, consider a feature "animal type" with possible values: "cat," "dog," and "bird." There is no meaningful order among these values. 

The number of possible values does not define a nominal feature.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Ordinal and One-Hot Encodings for Categorical Data"](https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/) for an explanation of nominal features.</p></details>

-----------------------

## Date - 2023-07-25


## Title - Abnormal cells


### **Question** :

Mabel works as a data analyst in a health tech company. Recently, she's been involved in a project that uses deep learning to identify abnormal cells in medical images. 

The company has collected a dataset of 2000 grayscale images of cells under a microscope, each of size 256 x 256. Half of those are images of normal cells, and the other half are images of abnormal cells.

In a discussion with her team, a question made Mabel consider the tensor structure required to store all this data.

**What's the correct shape of a tensor capable of storing all this data simultaneously?**


### **Choices** :

- We can store it in a tensor of shape `(2000, 256, 256)`
- We can store it in a tensor of shape `(256, 256, 1)`
- We can store it in a tensor of shape `(2000, 256, 256, 1)`
- We can store it in a tensor of shape `(1000, 256, 256, 1)`


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Working with images typically requires a tensor to store the height, the width, and the color depth. Grayscale images have a single color channel like the cell images.

Assuming our pictures are of size 256 x 256, we need to store the value of each pixel in the image. Since we're dealing with grayscale images, we would conventionally store each image in a tensor of shape `(256, 256)` or `(256, 256, 1)`.

Notice that, by convention, image data is often represented in a 4D array (also known as a 4D tensor) with dimensions corresponding to the number of samples (i.e., the number of images), the height of the image, the width of the image, and the number of color channels. For grayscale images, the number of color channels is 1.

Including the color channel in the tensor's shape can help ensure compatibility with various image processing and machine learning libraries, which expect input in this format. This means the preferred choice to store one image of size 256 x 256 is a tensor of shape `(256, 256, 1)`, but `(256, 256)` technically works.

So far, we know how to store a single grayscale picture of size 256 x 256. But how about storing 2000 of them? We need another dimension, which leads us to a tensor of shape `(2000, 256, 256, 1)` or `(2000, 256, 256)`.

The order of these dimensions is intentional. Conventionally, tensors that hold images are structured in the following way: (samples, height, width, and channels). There's another convention where channels go before the dimensions of the images, but it's more common to put them at the end.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Deep Learning with Python_, Second Edition](https://amzn.to/3K3VZoy) covers the topic of tensors really well.* Check ["A Gentle Introduction to Tensors for Machine Learning with NumPy"](https://machinelearningmastery.com/introduction-to-tensors-for-machine-learning/) for a quick introduction to tensors and practical code.</p></details>

-----------------------

## Date - 2023-07-26


## Title - Disappointing loss


### **Question** :

Sarah is a data scientist at a well-known tech company. 

She has been working diligently on building a neural network for predicting user behavior. After carefully preparing the dataset, she was ready to start training her model.

Excitedly, Sarah started the training process. However, after half an hour, she noticed that her training loss wasn't reducing as anticipated; it remained significantly high, much to her disappointment.

Puzzled, Sarah went back to scrutinizing her process, attempting to uncover the potential reasons for this anomaly.

**Select every potential reason causing the loss to behave this way.**


### **Choices** :

- Sarah is using a learning rate that's too high.
- The neural network is getting stuck at local minima.
- The regularization that Sarah is using is too aggressive.
- Sarah is using a learning rate that's too low.


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Many factors can cause problems during training. Let's discuss some of the more common ones.

[Regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)) is a helpful technique to avoid overfitting, but it may prevent the network from learning when it is too aggressive. This happens because regularization imposes a penalty on the network's weights, preventing them from becoming too large. If we aren't careful, the weights may stop changing, the network will stop learning, and the loss will stay high.

Using a learning rate that's too low might also keep the training loss high. The network weights will be updated very slowly with a low learning rate. Unless we run the training process for many iterations, the network will struggle to get to where the loss is sufficiently low.

Finally, the optimization may be stuck at a local minimum. This will cause the loss to stop decreasing altogether. Strategies to overcome this problem include better initializing the network's parameters or increasing the learning rate or momentum to overcome the local minimum.

It's improbable that the problem is caused by a learning rate that's too high. If this were the case, Sarah would see rather significant changes in the loss. With a high learning rate, it's also common to see the loss oscillating after some time.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What should I do when my neural network doesn't learn?"](https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn) is an excellent discussion about this topic.</p></details>

-----------------------

## Date - 2023-07-27


## Title - Visual search


### **Question** :

Talia is a data scientist at a company that develops an e-commerce platform with a visual search feature.

The company has collected many product images from various online retailers, but the images are not labeled by category. To enhance the visual search functionality, Talia wants to classify each product image into specific categories. However, she realizes that she cannot use supervised learning without labeled data.

Talia needs to determine a method for obtaining labels for the image data.

**Which of the following techniques could Talia use to label the data?**


### **Choices** :

- Assemble a team to examine and manually label the category of each product image.
- Apply a supervised learning method to extract the labels directly from the existing data.
- Use semi-supervised learning to propagate labels throughout the entire dataset.
- Use reinforcement learning to propagate labels throughout the entire dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To employ a supervised learning method, Talia needs to create labels for the data. She has several techniques available to achieve this.

The first choice suggests using human labelers, the most common approach to labeling data. Talia could assemble a team to examine each product image and assign an appropriate category label (e.g., clothing, electronics, or furniture).

Using a supervised learning method to infer the labels from the existing dataset is not feasible since supervised learning requires labels, which Talia lacks. 

Using semi-supervised learning to propagate labels throughout the entire dataset could be used if Talia already had a small portion of labeled data. The method would then generate labels for the remaining data. However, there is no indication that Talia has any labeled data, making semi-supervised learning an unsuitable option.

Finally, using reinforcement learning to propagate labels throughout the entire dataset is not ideal for labeling data in this context, as it is mainly designed for decision-making processes and learning through trial and error.

Other techniques to generate labels include [active learning](https://articles.bnomial.com/active-learning) and [weak supervision](https://snorkel.ai/weak-supervision/).</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The ["Machine Learning Data Lifecycle in Production"](https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production) course in Coursera, part of the [Machine Learning Engineering for Production (MLOps) Specialization](https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops).* Check out ["Semi-Supervised Learning With Label Propagation"](https://machinelearningmastery.com/semi-supervised-learning-with-label-propagation/) for an introduction to how to use a few labels with semi-supervised learning.* ["Active Learning"](https://articles.bnomial.com/active-learning) is an introduction to a learning technique to build better-performing machine learning models using fewer training labels.* ["Weak Supervision: A New Programming Paradigm for Machine Learning"](http://ai.stanford.edu/blog/weak-supervision/) is a good article from Stanford introducing Weak Supervision.</p></details>

-----------------------

## Date - 2023-07-28


## Title - E-commerce recommendations


### **Question** :

Finley is a data scientist working at a large e-commerce company.

Her current project involves developing a recommendation system to suggest personalized products for customers. Before training her model, Finley needs to divide her dataset into training and test sets.

Before splitting the data, Finley writes code to shuffle the datasets.

**Why is it essential to shuffle a dataset before partitioning it into training and test sets?**


### **Choices** :

- To ensure the training and test sets contain the same features.
- To prevent unintended patterns or sequences in the dataset from affecting the model's performance.
- To ensure the training and test sets contain the same number of samples.
- To guarantee that class labels are evenly distributed across the training and test sets.


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Shuffling a dataset before dividing it into training and test sets is critical.

First, we can ensure that the class labels are evenly distributed across the training and test sets. This helps avoid imbalances in the distribution of class labels, which could negatively impact the model's performance during training and evaluation. For example, if one class is over-represented in the training set but under-represented in the test set, the model might learn to predict the over-represented class well but perform poorly on the under-represented class.

Second, shuffling helps ensure that the training and test sets are equally difficult for the model to learn and evaluate. If the data has some inherent order, such as being sorted by time or some other variable, the model might be exposed to certain patterns only during training, which could lead to overfitting or underfitting. By shuffling the data, we can ensure that the model is exposed to a diverse and representative data sample during training and evaluation, improving its generalization capabilities.

Finally, shuffling helps us prevent the influence of any unintentional order or sequence in the data. For example, if the data was collected in batches and each batch had some unique characteristics, not shuffling the dataset could result in the model learning these batch-specific characteristics, which are irrelevant to the problem. Shuffling the dataset reduces the risk of the model learning irrelevant patterns and improving its ability to generalize to unseen data.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems_](https://amzn.to/3SHGqsu) is one of the best books for machine learning fundamentals.* ["Machine Learning Crash Course"](https://developers.google.com/machine-learning/crash-course) is a great introduction to Machine Learning.</p></details>

-----------------------

## Date - 2023-07-29


## Title - Separating classes


### **Question** :

Haven is trying to solve a simple problem using a Machine Learning model.

She decided to use a neural network because it's the area she needs to learn the most.

Unfortunately, Haven realized her network was not correctly separating the classes in her dataset. After reading more about the issue, she suspects she must introduce non-linearities to her neural network.

**How can Haven solve her problem?**


### **Choices** :

- Haven should use Rectifier Linear Unit (ReLU) as an activation function.
- Haven should use the Identity function as an activation function.
- Haven should use the Sigmoid function as an activation function.
- Haven should use Stochastic Gradient Descent to train the network.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>For a neural network to learn complex patterns, we must ensure that the network can approximate any function, not only linear ones. This is why we call it "non-linearities."

The way we do this is by using activation functions. 

An interesting fact: the [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) states that, when using non-linear activation functions, we can turn a two-layer neural network into a universal function approximator. This is an excellent illustration of how powerful neural networks are.

Some of the most popular activation functions are [sigmoid](https://en.wikipedia.org/wiki/Logistic_function) and [ReLU](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks). 

The identity function doesn't work for this purpose because it returns the same value used as its argument. Finally, Stochastic Gradient Descent doesn't have anything to do with the linearity of the network operations.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Activation function"](https://en.wikipedia.org/wiki/Activation_function) from Wikipedia to understand more about this topic.* I find the ["Universal approximation theorem"](https://en.wikipedia.org/wiki/Universal_approximation_theorem) fascinating.</p></details>

-----------------------

## Date - 2023-07-30


## Title - Self clothing


### **Question** :

Lilliana is the head of the IT department at a large clothing store. 

She has a strong background in data science and has built several models for forecasting sales and identifying customer trends using supervised and unsupervised learning techniques.

Recently, during a tech conference, she came across a presentation on self-supervised learning, which she found intriguing as it was a new concept.

**Which of the following statements about self-supervised learning are accurate?**


### **Choices** :

- Self-supervised learning is a hybrid of supervised and semi-supervised learning techniques.
- Like supervised learning, self-supervised learning methods can evaluate the accuracy of their predictions during training.
- Self-supervised methods require only a small number of labeled samples.
- Like unsupervised learning, self-supervised methods don't require labeled training data.


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Self-supervised methods are considered a form of unsupervised learning. However, not all unsupervised methods are self-supervised. Self-supervised methods have similarities to supervised, unsupervised, and semi-supervised learning, but they aren't a hybrid of supervised and semi-supervised techniques.

Self-supervised learning doesn't require labeled data, similar to unsupervised learning. On the other hand, in self-supervised methods, "supervision" can be derived directly from the data. Therefore, like supervised learning, we can judge whether a prediction is true or false.

An example would be a neural network that predicts the next word given a part of a sentence. We can take the text of a book and create random samples by picking parts of sentences. We don't need any labels, but for every instance, we still know what the correct answer is.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Self-supervised learning"](https://project.inria.fr/paiss/files/2018/07/zisserman-self-supervised.pdf) is a presentation by Andrew Zisserman covering self-supervised learning for images.* ["Self-supervised learning: The dark matter of intelligence"](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/) is an excellent post from Meta's AI Research team.</p></details>

-----------------------

## Date - 2023-07-31


## Title - Complex decision making


### **Question** :

Ariyah, a data analyst passionate about helping businesses make informed decisions, has been exploring Machine Learning techniques to improve her analysis skills. 

She's particularly drawn to Decision Trees because they provide a simple yet effective way to model complex decision-making processes.

**Which of the following points are advantages of using Decision Trees?**


### **Choices** :

- Decision Trees are versatile and can handle categorical and numerical data.
- Decision Trees are very resistant to overfitting.
- The logic behind Decision Trees is easy to interpret, making them a great tool for explaining results to non-experts.
- Decision Trees can solve regression and classification tasks.


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Decision Trees are a popular machine learning technique because they can handle categorical and numerical data, making them applicable to various problems.

Additionally, they are easily interpretable, as the tree structure can be visualized and follows a series of "if-then" rules, allowing even non-experts to understand the decision-making process. You can look at a prediction from a Decision Tree and fully understand why the model arrived at that conclusion. 

Decision Trees, unfortunately, are prone to overfitting if we don't take careful care of their depth. In other words, unless we ensure our tree doesn't go too deep, it will tend to fit noisy samples and output the wrong prediction. We can control overfitting in a Decision Tree by "pruning."

Remember that while a single Decision Tree is prone to overfitting, using an ensemble of trees is more resistant. Here is a quote from "[To Boost or not to Boost: On the Limits of Boosted Neural Networks](https://arxiv.org/pdf/2107.13600.pdf)":

> [these experiments] confirm that training single large decision trees is prone to overfitting while boosted ensembles of decision trees are resistant to overfitting.

Many people relate Decision Trees with classification tasks, but they are also valuable for solving regression tasks. A classification task is when the predicted outcome is a discrete class, while the result of a regression task is a Real number. This flexibility makes Decision Trees very useful.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["To Boost or not to Boost: On the Limits of Boosted Neural Networks"](https://arxiv.org/pdf/2107.13600.pdf), the paper cited above comparing the overfitting tendency of a single Decision Tree versus an ensemble of trees.* ["Decision tree pruning"](https://en.wikipedia.org/wiki/Decision_tree_pruning) covers the process of pruning a tree to reduce overfitting.* Check ["Random Forest"](https://en.wikipedia.org/wiki/Random_forest) for more information about a widely used class of Decision Tree ensembles.</p></details>

-----------------------

## Date - 2023-08-01


## Title - Architecture mismatch


### **Question** :

Meadow is discussing popular Deep Learning architectures with her colleagues during a meeting. They've listed some examples of such architectures, but one doesn't fit in the list. 

Meadow wants to point out the odd one but needs your help.

**Can you determine which of the following doesn't belong on this list?**


### **Choices** :

- ResNet
- InceptionV3
- Word2vec
- MobileNet


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Word2vec is the odd one on the list, as it is a natural language processing (NLP) technique for learning word embeddings rather than a Deep Learning architecture. The other items in the list (InceptionV3, ResNet, and MobileNet) are all Deep Learning architectures used primarily for image classification tasks.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check [Word2vec](https://en.wikipedia.org/wiki/Word2vec) for more information about the Word2vec technique.</p></details>

-----------------------

## Date - 2023-08-02


## Title - Game surprise


### **Question** :

Winter is planning a board game night with her friends and wants to surprise them with a new game. 

The game comes with two dice. One is a 6-sided die, and the other is a huge 24-sided die.

While getting ready, Winter's brother enters the room, curious about the dice. He selects one of the dice from the box and rolls a 4.

**What is the probability that her brother pulled out the 6-sided die?**


### **Choices** :

- The probability is 1/5
- The probability is 2/5
- The probability is 3/5
- The probability is 4/5


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's assume that `A` represents the event of rolling the die and getting a `4`, `B1` represents pulling out the 6-sided die, and `B2` represents pulling out the 24-sided die.

We can compute the probability that Winter's brother picked the 6-sided die using the [Bayes theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem):

```
P(B1|A) = (P(A|B1)*P(B1))/P(A)
P(B1|A) = (1/12)/P(A)

P(A) = P(A|B1)*P(B1)+P(A|B2)*P(B2)
P(A) = 1/12 + 1/48​
P(A) = 5/48

P(B1|A) = (1/12)/(5/48)
P(B1|A) = 4/5​
```

Thus, the answer is `4/5`.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["A Gentle Introduction to Bayes Theorem for Machine Learning"](https://machinelearningmastery.com/bayes-theorem-for-machine-learning/) is a great starting point to understand the Bayes theorem and how to use it.* You can also check the Wikipedia page of [the Bayes theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem).</p></details>

-----------------------

## Date - 2023-08-03


## Title - Healthcare company


### **Question** :

Jocelyn is a data scientist at a healthcare company.

Her job is to develop a model that predicts the likelihood of patients developing diabetes. She built a neural network using patients' medical records, but the predictions aren't as precise as she needs them to be. 

Jocelyn believes making the network deeper could be the solution.

**What should Jocelyn do to make her network deeper?**


### **Choices** :

- Jocelyn should add more layers to the network.
- Jocelyn should add more neurons to the existing layers.
- Jocelyn should change the activation function in the existing layers.
- Jocelyn should use a different optimization algorithm.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Jocelyn should add more layers to the network. A deep neural network is an artificial neural network with multiple layers between the input and output layers. The more layers we add, the "deeper" the network becomes.

Adding neurons alone doesn't change the depth of the network. It does change its capacity, but a network with the same number of layers but more neurons will still have the same depth. 

Changing the activation function in the existing layers might affect the model's performance, but it won't make the network deeper. Activation functions play a role in introducing non-linearities, but they don't affect the depth of the network.

A different optimization algorithm could improve the training process and performance but won't deepen the neural network. Optimization algorithms help minimize the loss function but don't increase the network's depth.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Wikipedia's definition of ["Deep neural networks"](https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks) serves as a succinct summary of what deep neural networks are.* Check ["A Layman’s Guide to Deep Neural Networks"](https://towardsdatascience.com/a-laymans-guide-to-deep-neural-networks-ddcea24847fb) for a non-mathematical introduction to deep neural networks.</p></details>

-----------------------

## Date - 2023-08-04


## Title - Object schedule


### **Question** :

Emma created a Deep Learning model to identify objects in pictures taken from a drone camera. During her exploration phase, she found a specific learning-rate schedule that performed well for her model.

She wants to train a new model using the same architecture with images from a different drone camera.

Everything will stay the same except the dataset. 

**Should Emma expect her learning-rate schedule to also work for the second model?**


### **Choices** :

- Only if the second dataset has the same number of samples. The learning-rate schedule depends on the number of samples in the dataset. Therefore the exact schedule won't work with a different size dataset.
- Yes, because both datasets come from the same type of drone camera. The learning-rate schedule should change whenever we have a dataset from a different target distribution. This won't be the case here.
- No, because a new dataset changes the tradeoff between optimization and regularization. The learning-rate schedule is dataset-specific.
- Yes, because learning-rate schedules are specific to the optimization mechanisms used by the model. A new dataset will require a new optimization process, invalidating the learning-rate schedule.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's take a look at [this tweet](https://twitter.com/fchollet/status/1508477486882979843) from [François Chollet](https://amzn.to/3K3VZoy):

> PSA: never attempt to use a training schedule from an old dataset on a new dataset. Even if the model, the number of samples, and the target distribution are the same, the intrinsic difficulty of the problem may have changed (tradeoff between optimization and regularization). 

> Learning rate schedules and regularization are fundamentally dataset-specific, far more than model architecture.

Notice that he mentions that even when the dataset size and the target distribution are the same, the learning-rate schedule might not be helpful anymore. Even when we don't change anything about the model architecture or the method we use to train it, a new dataset may change the intrinsic difficulty of the problem.

With a new dataset, we may need more regularization to avoid overfitting, or we may need less of it to ensure the model learns the specifics of the problem. A different dataset leads to a different set of tradeoffs. 

Therefore, Emma should not reuse the learning-rate schedule and instead find the best for the second model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* François' book, [_Deep Learning with Python, Second Edition_](https://amzn.to/3K3VZoy), is among the best Deep Learning references you'll find.</p></details>

-----------------------

## Date - 2023-08-05


## Title - Improving rate


### **Question** :

Annalise is a data science enthusiast trying to build her first deep-learning model. She has prepared her dataset and chosen the architecture for her neural network.

During her training phase, however, Annalise noticed that her model's performance was not improving. After researching, she came across a concept called "learning rate scheduler" and thought it might be what she needed.

Annalise wants to understand more about how a learning rate scheduler functions and how it might help improve her model's learning.

**Which of the following correctly describes what a learning rate scheduler does in deep learning?**


### **Choices** :

- The learning rate scheduler will save a copy of the network weights according to the value of the learning rate.
- The learning rate scheduler will help the optimization algorithm accelerate in one direction based on past updates.
- The learning rate scheduler will adjust the learning rate during training according to a pre-defined schedule.
- The learning rate scheduler will help the optimization algorithm get past a flat region by continuing its previous movement.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>One critical aspect of training neural networks in deep learning is setting the learning rate. The learning rate determines how much we adjust the model in response to the estimated error each time the weights are updated. The model may need too many updates to converge if the learning rate is too small. The model might skip the optimal solution or even diverge if it's too large.

A popular technique for finding a good balance is using a learning rate scheduler. This predefined schedule adjusts the learning rate between epochs or iterations as the training progresses.

The most common scenario is to start with a high learning rate and decrease it over time. Initially, we take significant steps towards the minimum but move more carefully as we hone in on it.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Learning Rate Scheduling"](https://d2l.ai/chapter_optimization/lr-scheduler.html) is a great introduction to learning rate schedulers.- ["How to Choose a Learning Rate Scheduler for Neural Networks"](https://neptune.ai/blog/how-to-choose-a-learning-rate-scheduler) is an article from [Neptune AI](https://neptune.ai/), focusing on some practical ideas on how to use schedulers.</p></details>

-----------------------

## Date - 2023-08-06


## Title - Logistic characteristics


### **Question** :

Emily is developing a logistic classifier for her latest research. 

She understands that the accuracy of her predictions depends on the error function she uses. She wants this function to have the following characteristics:

* The function should yield a small number if the sample is classified correctly.
* The function should yield a large number if the sample is misclassified.
* The error for a group of samples should be the sum or average for all the samples.

Emily has a few options but would like your advice.

**Which of the following would be the most suitable error function for a logistic classifier?**


### **Choices** :

- Log loss: a function that returns the negative logarithm of the product of probabilities.
- Absolute error: a function that returns the absolute value of the difference between the prediction and the label.
- Square error: a function that returns the square of the difference between the prediction and the label.
- Mean percentage: a function that returns the average error of the differences between predicted and actual values.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The absolute error, the square error, and the log loss are good choices that satisfy the characteristics that Emily needs. The "mean percentage" is a made-up error function that doesn't exist.

Out of these three functions, we need to analyze whether they can be used to build a binary classification model.

Neither the absolute nor square error functions are used in binary classification problems. They don't penalize mistakes as harshly as log loss does, so they aren't a good choice for this type of problem. 

Log loss, however, is one of the most popular error functions and a perfect fit for a binary classifier like the one Emily is trying to build.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check ["Logistic Regression for Machine Learning"](https://machinelearningmastery.com/logistic-regression-for-machine-learning/) for an introduction to Logistic Regression.- ["Logistic Regression: Loss and Regularization"](https://developers.google.com/machine-learning/crash-course/logistic-regression/model-training) is a quick summary about the Log-loss and regularization.</p></details>

-----------------------

## Date - 2023-08-07


## Title - Encoding features


### **Question** :

In developing a machine learning model, a significant portion of the work is done before the training begins.

Giuliana knows that she must prepare her data before it can be used. She has been considering encoding some of the features in her dataset. One-Hot Encoding appears to be a suitable option.

It's been a long time since Giuliana used One-Hot Encoding, and she could use some guidance.

**Which of the following statements accurately describes how One-Hot Encoding functions?**


### **Choices** :

- One-Hot Encoding transforms a numerical feature into its categorical equivalent.
- One-Hot Encoding converts a string-encoded feature into its numerical equivalent.
- One-Hot Encoding changes a string-encoded feature into its categorical equivalent.
- One-Hot Encoding generates additional features based on the number of unique values in a categorical feature.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Categorical data are variables that contain label values rather than numeric values. For instance, a variable representing the color with values "red," "blue," and "green" is a categorical variable.

While some algorithms can work with categorical data directly, most cannot: they need the data to be numeric. One-Hot Encoding is one of the methods we can use to convert categorical data into a numerical format.

For instance, suppose we have a dataset with a single feature called "color" that could have the values "red," "blue," and "green." Applying One-Hot Encoding will result in a new dataset with three features, one for each value of the original "color" column.

A sample that had the value "blue" in the previous column will now have the value `0` for both "red" and "green" and the value `1` under the "blue" feature.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Why One-Hot Encode Data in Machine Learning?"](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) for an explanation of how One-Hot Encoding works.</p></details>

-----------------------

## Date - 2023-08-08


## Title - Descent dynamics


### **Question** :

Leighton, a data science enthusiast, was intrigued by the various forms of the Gradient Descent algorithm: Mini-Batch Gradient Descent, Stochastic Gradient Descent, and Batch Gradient Descent.

Despite grasping the fundamental concept, Leighton found the precise dynamics of Stochastic Gradient Descent slightly elusive.

**Which of the following explanations accurately portrays the workings of the Stochastic Gradient Descent algorithm?**


### **Choices** :

- Stochastic Gradient Descent uses a batch of data (more than one sample but fewer than the entire dataset) during every iteration.
- Stochastic Gradient Descent determines the optimal amount of data required to compute the gradient of the cost function.
- Stochastic Gradient Descent uses a single sample of data during every iteration.
- Stochastic Gradient Descent uses all available data once during every iteration.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Here's a simpler way to understand the operation of Gradient Descent: We select some samples from our training dataset, feed them through the model, and measure the difference between our outcomes and the expected results. We use this "error" to compute the changes needed to improve the model weights.

One key decision in this process is the number of samples we use for every iteration of our model. We have three alternatives:

* Use a single sample of data.
* Use all of the available data.
* Use a subset of the data.

When we use a single data sample for each iteration, it's called "Stochastic Gradient Descent." In simple terms, the algorithm uses one sample to compute the updates.

Using all the data at once is called "Batch Gradient Descent." Once all samples have been processed, the algorithm uses the entire dataset to compute the updates.

Lastly, "Mini-Batch Gradient Descent" implies using a subset of the data—more than one sample but less than the total dataset. This algorithm functions similarly to Batch Gradient Descent, but the number of samples used is different.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["The wrong batch size is all it takes" ](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) explains how different batch sizes influence the training process of neural networks using gradient descent.* ["A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size"](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) covers Batch and Mini-Batch Gradient Descent.* Check ["An overview of gradient descent optimization algorithms"](https://www.ruder.io/optimizing-gradient-descent/) for a deep dive into gradient descent and every one of its variants.</p></details>

-----------------------

## Date - 2023-08-09


## Title - Looking properties


### **Question** :

Wren is a data scientist looking into the world of neural networks. She has been particularly intrigued by the Rectified Linear Unit (ReLU) Activation Function, often used in deep learning models. 

The ReLU function is a simple function that outputs the input directly if it is positive; otherwise, it outputs zero. 

Wren is interested in the mathematical properties of the ReLU function, especially when it comes to its continuity and differentiability.

**Which of the following is true about the Rectified Linear Unit (ReLU) Activation Function?**


### **Choices** :

- The function is neither continuous nor differentiable.
- The function is differentiable but not continuous.
- The function is both continuous and differentiable.
- The function is continuous but not differentiable.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>ReLU is continuous but not differentiable.

While the function is indeed differentiable for values of `x>0` and `x<0`, it is not differentiable for `x=0`. 

Think about it: what is the derivative of the ReLU function at `x=0`? Is it constant? Is it rising? The derivative is not defined at `x=0`, so the function is not differentiable.

This may be surprising because we know that Gradient Descent needs a differentiable function to work. How come it works with ReLU, then?

In a [really good post](https://sebastianraschka.com/faq/docs/relu-derivative.html) about this topic, [Sebastian Raschka](https://twitter.com/rasbt) wrote about this:

> In practice, it’s relatively rare to have `x=0` in the context of deep learning; hence, we usually don’t have to worry too much about the ReLU derivative at `x=0`. We typically set it to `0`, `1`, or `0.5`.

So there you have it. The correct answer to this question is the fourth choice.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["A Gentle Introduction to the Rectified Linear Unit (ReLU)"](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) for an introduction to ReLU.* The [_Deep Learning_](https://amzn.to/3MqvoTQ) book is a great resource.</p></details>

-----------------------

## Date - 2023-08-10


## Title - Spanish pigeons


### **Question** :

Gabriella was excited about the performance of his model.

She's been working on an app to classify photos of pigeons, and her model is performing very well on all metrics. 

Gabriella knows what she is doing: she split training and test data, used the proper evaluation metrics, balanced her dataset, and reviewed examples regularly to ensure there were no labeling errors.

Finally, Gabriella launched his app. 

Positive feedback started rolling in except from one place: users in Spain complained the performance was awful.

Gabriella was baffled.

**What is the most likely reason for the problem?**


### **Choices** :

- Gabriella didn't train the model long enough to capture all the necessary details of different pigeon species.
- Gabriella's model is too simple, and it couldn't learn the entire dataset of pigeons, leaving out those from Spain.
- Gabriella's model suffers from data or concept drift.
- Gabriella's model suffers from sampling bias. She probably didn't include enough examples of Spanish pigeons.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A common problem in machine learning is that a model that shows promising results during evaluation doesn't perform well when deployed in production. 

While there may be various reasons for that, the story above points us in one particular direction.

Not training the model long enough is unlikely to be a problem here. Gabriella wouldn't be getting good results anywhere if this was the case. Notice that problems seem to only happen in Spain.

Insufficient model capacity can't either be a valid reason for what Gabriella is seeing. If this were the case, the model would be underfitting and give poor results across the board, not localized to a specific region.

Data and concept drift are indeed common problems with models in production. However, they arise when the environment changes over time, as does the input to the model. In this case, the problem appeared straight after deployment.

This leaves us with the only correct answer: The most likely reason for this problem is that Gabriella didn't have enough data from Spain, so her model is struggling to recognize pigeons from that region.

This issue is called "sampling bias." It explains why the problem occurred in one particular country. Sampling bias is difficult to detect during development because the data is missing from the training and test datasets, so we can't notice it while evaluating the model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Sampling bias"](https://en.wikipedia.org/wiki/Sampling_bias) for a complete explanation of this problem.</p></details>

-----------------------

## Date - 2023-08-11


## Title - The meteorologist


### **Question** :

Presley, a meteorologist, is working on predicting weather patterns using machine learning algorithms. 

She needs to split her dataset into training and test sets to train and evaluate her model effectively, and she knows she needs to shuffle the dataset before anything else.

**What is the primary purpose of shuffling a dataset before dividing it into training and test sets?**


### **Choices** :

- Shuffling the data ensures the training and test sets contain the data ordered adequately.
- Shuffling the data ensures that class labels are uniformly distributed across the training and test sets.
- Shuffling the data ensures the training and test sets contain equal samples.
- Shuffling the data ensures the training and test sets contain the same features.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Shuffling a dataset before dividing it into training and test sets is critical.

First, we can ensure that the class labels are evenly distributed across the training and test sets. This helps avoid imbalances in the distribution of class labels, which could negatively impact the model's performance during training and evaluation. For example, if one class is over-represented in the training set but under-represented in the test set, the model might learn to predict the over-represented class well but perform poorly on the under-represented class.

Second, shuffling helps ensure that the training and test sets are equally difficult for the model to learn and evaluate. If the data has some inherent order, such as being sorted by time or some other variable, the model might be exposed to certain patterns only during training, which could lead to overfitting or underfitting. By shuffling the data, we can ensure that the model is exposed to a diverse and representative data sample during training and evaluation, improving its generalization capabilities.

Finally, shuffling helps us prevent the influence of any unintentional order or sequence in the data. For example, if the data was collected in batches and each batch had some unique characteristics, not shuffling the dataset could result in the model learning these batch-specific characteristics, which are irrelevant to the problem. Shuffling the dataset reduces the risk of the model learning irrelevant patterns and improving its ability to generalize to unseen data.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems_](https://amzn.to/3SHGqsu) is one of the best books for machine learning fundamentals.* ["Machine Learning Crash Course"](https://developers.google.com/machine-learning/crash-course) is a great introduction to Machine Learning.</p></details>

-----------------------

## Date - 2023-08-12


## Title - Deep activations


### **Question** :

Tessa is preparing for her upcoming Machine Learning exam.

She is reading about activation functions and how they are essential for creating deep neural networks. Tessa would like to write down a few examples of non-linear activation functions to learn more about them later.

**Which of the following are considered non-linear activation functions?**


### **Choices** :

- Sigmoid
- Hyperbolic Tangent (tanh)
- Rectified Linear Unit
- Leaky Rectified Linear Unit


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>For a neural network to learn complex patterns, we must ensure that the network can approximate any function, not only linear ones. This is why we call it "non-linearities."

The way we do this is by using activation functions. 

An interesting fact: the [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) states that, when using non-linear activation functions, we can turn a two-layer neural network into a universal function approximator. This is an excellent illustration of how powerful neural networks are.

Some of the most popular activation functions are [sigmoid](https://en.wikipedia.org/wiki/Logistic_function), [tanh](https://en.wikipedia.org/wiki/Hyperbolic_functions#Hyperbolic_tangent), [ReLU](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks), and [Leaky ReLU](https://paperswithcode.com/method/leaky-relu).

Therefore, every choice of this question is correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Activation function"](https://en.wikipedia.org/wiki/Activation_function) from Wikipedia to understand more about this topic.* I find the ["Universal approximation theorem"](https://en.wikipedia.org/wiki/Universal_approximation_theorem) fascinating.</p></details>

-----------------------

## Date - 2023-08-13


## Title - Possible opposition


### **Question** :

Alice is contemplating optimizing her deep learning model by incorporating Rectified Linear Units (ReLU) as the activation function in several model layers.

Aware of possible opposition from her teammates, Alice prepares to compile some of the benefits of using ReLU to convince them.

**Which of the following are the advantages of using the ReLU?**


### **Choices** :

- ReLU acts mainly as a linear function, improving our ability to optimize neural networks.
- ReLU saturates around extreme values, enabling faster convergence during backpropagation and facilitating the network's learning.
- ReLU is computationally straightforward to implement.
- ReLU provides better representational sparsity than the sigmoid and tanh activation functions, as it can generate a true zero output.


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>ReLU is a piecewise linear function that outputs the input directly if it is positive and zero if it's otherwise:

```
f(x) = max(x, 0)
```

While it's true that ReLU converts negative input values to zero, leading to function saturation, it doesn't saturate for positive input values because it retains them. Conversely, functions like sigmoid and tanh do saturate, around `0` and `1` for sigmoid and `-1` and `1` for tanh, for both positive and negative inputs.

Importantly, the saturation of ReLU does not accelerate the convergence of backpropagation. Saturation can often be a problem in neural networks, leading to issues such as "[dying ReLU](https://arxiv.org/abs/1903.06733)" or the "[vanishing gradient problem](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/)."

ReLU can convert negative inputs into zero, which is not true for sigmoid and tanh, as they only approximate a zero value. When using ReLU, you can have layers with one or more nodes containing zero values, referred to as "sparse representation." This property simplifies the model and can significantly save computing resources while effectively representing data in lower-dimensional space.

The implementation of ReLU is incredibly simple, especially when compared to sigmoid and tanh, which require complex computational procedures like exponentiation.

Finally, optimizing neural networks is easier when their behavior is as linear as possible. ReLU primarily behaves as a linear function, simplifying the optimization process of neural networks. It's sometimes called a "piecewise linear function" as it exhibits linear behavior for half of its input domain and non-linear for the other half.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["A Gentle Introduction to the Rectified Linear Unit (ReLU)"](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) for an introduction to ReLU.* The [_Deep Learning_](https://amzn.to/3MqvoTQ) book is a great resource.</p></details>

-----------------------

## Date - 2023-08-14


## Title - Over distinctions


### **Question** :

Madeleine, a budding machine learning practitioner, is often puzzled over the distinctions between multi-class and multi-label classification problems.

In multi-class classification, each sample in the dataset is associated with a single class. However, each instance can be tied to one or more categories in multi-label classification. Despite their apparent similarities, these types of problems require different approaches.

Madeleine frequently finds herself in a quandary remembering the appropriate loss function to employ when training a neural network to handle these problems.

**Which two of the following statements correctly address her question?**


### **Choices** :

- Binary cross-entropy is commonly applied as the loss function for multi-label classification problems.
- Categorical cross-entropy is typically the loss function for multi-class classification problems.
- Binary cross-entropy is often used as the loss function for multi-class classification problems.
- Categorical cross-entropy is generally employed as the loss function for multi-label classification problems.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When dealing with [multi-class classification](https://en.wikipedia.org/wiki/Multiclass_classification) tasks, the goal of the output layer of the model is to predict the most suitable class for the input provided to the network. For this purpose, the softmax function is ideal as it converts the output scores to probabilities for each class.

The categorical cross-entropy loss function quantifies the disparity between two probability distributions. Therefore, for multi-class classification tasks, this loss function makes an excellent pair with a softmax output layer.

Conversely, in [multi-label classification](https://en.wikipedia.org/wiki/Multi-label_classification) models, the output layer is designed to deliver independent values. The sigmoid function suits this requirement well, translating the output scores to a value between 0 and 1.

Multi-label classification tasks incorporate principles from binary classification, but multiple outputs are expected instead of a single sigmoid output. Considering a model predicting ten potential classes as a combination of ten binary classifiers can be helpful. The binary cross-entropy loss function, commonly used in training binary classification models, suits this situation well.

Multi-class classification models usually utilize a softmax output layer with the categorical cross-entropy loss function. In contrast, multi-label classification models prefer a sigmoid output layer coupled with the binary cross-entropy loss function.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Binary crossentropy"](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/binary-crossentropy) for an explanation of how Binary Cross-Entropy works.* Check ["Categorical crossentropy"](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy) for an explanation of how Categorical Cross-Entropy works.</p></details>

-----------------------

## Date - 2023-08-15


## Title - The shoe reseller


### **Question** :

Nova worked for a shoe reseller in the computer vision department. 

She led the team that built their deep learning model to recognize shoes from pictures. They used a pre-trained ResNet50 as the foundation and fine-tuned it with a large dataset of shoe images.

Nova's model can classify 1,000 different shoe styles, and it's pretty good!

During Fashion Week, right after the model goes into production, a famous manufacturer releases a new style of their popular shoe brand. Nova's team didn't train their model using this style.

**What would happen if we showed a picture of the new shoes to Nora's model? Select everything that could occur.**


### **Choices** :

- Assuming Nora's team trained the model properly, it should classify the new style correctly.
- The model will return an incorrect style with low confidence.
- The model will return an incorrect style with high confidence.
- The model will return an error because it doesn't recognize the new style.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Nora and her team trained their model on a dataset with 1,000 different classes—each class representing a particular shoe style. This is a classic supervised learning approach: you train a model to classify a sample into a group of pre-defined categories.

Unfortunately, their classification model can't correctly classify a style that the team didn't use during the training process, so the first choice is not correct.

The fourth choice is incorrect: the model will not return an error. [Classification models don't know what they don't know](https://twitter.com/svpino/status/1531968815063769089), so Nora's model will produce a prediction albeit an incorrect one.

We are left with two choices: Would the prediction's confidence be low or high? Intuitively, it would make sense to get a low confidence result. At the end of the day, the model is trying to classify a shoe style it hasn't seen before. Unfortunately, this is not the way it works.

The confidence that we get back from a classification model does not reflect how confident the model is in that prediction concerning the potential universe of possible results. That's how we think, but the "universe" of a model is limited to the categories we used to train it. In Nora's case, they need to look at a 95% confidence result for the other 999 categories, not as "the model it's 95% certain the image belongs to this particular style."

Therefore, we can't predict how the model's confidence will fluctuate for a particular style—either supported or unsupported. We may get a low or high confidence result, but the prediction will be incorrect in every case. Therefore, the second and third choices are both valid.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Out-of-distribution"](https://deepchecks.com/glossary/out-of-distribution/) for a quick introduction to the problem of trying to classify samples that differ from the training data.* A Twitter thread covering why ["Classification models don't know what they don't know"](https://twitter.com/svpino/status/1531968815063769089).</p></details>

-----------------------

## Date - 2023-08-16


## Title - Snapping plants


### **Question** :

Camryn is creating an app to help people identify the type of plant they have just by snapping a photo. She has been studying machine learning and has decided to use a neural network to solve this problem.

She's starting off small and plans to use a neural network with only one layer between the input and output layers. While explaining this to her friends helping her test the app, she wants to use the simplest language possible to avoid confusing them.

**How should Camryn refer to this type of neural network?**


### **Choices** :

- A hidden neural network
- A recurrent neural network
- A shallow neural network
- A deep neural network


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The kind of network Camryn is planning to use for her plant identification app is a "shallow neural network." This term is used in computer science to denote a network with only one hidden layer.

When a neural network consists of multiple hidden layers, we call it a "deep neural network." However, since Camryn's network only includes one layer, it is not considered "deep" but rather "shallow."

The phrases "hidden neural network" and "recurrent neural network" do not apply in this situation. The term "hidden neural network" doesn't exist in computer science terminology. And "recurrent neural network" is a specific kind of network that isn't the one Camryn is implementing for her app.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["But what is a neural network?"](https://www.youtube.com/watch?v=aircAruvnKk) is Grant Sanderson's introduction to neural networks on YouTube. Highly recommended!* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html) is a free online book written by [Michael Nielsen](https://twitter.com/michael_nielsen).</p></details>

-----------------------

## Date - 2023-08-17


## Title - Nut allergies


### **Question** :

Sarah is working on an interesting project for a health app. The app aims to identify whether a food item in an image contains a nut, helping those with nut allergies avoid harmful food items.

The project has generated significant excitement among the team. They've compiled a vast dataset of images to train the model and have chosen to use deep learning to build a binary classifier. The key question they're facing right now is the selection of an appropriate activation function for the final layer of the network.

**Which activation functions could be a good candidate for the output layer?**


### **Choices** :

- Softmax
- Leaky ReLU
- Rectifier Linear Unit (ReLU)
- Sigmoid


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Sarah's team is building a binary classifier. The goal is for the output layer to produce a result that can be easily interpreted.

ReLU and Leaky ReLU functions return the input if it's positive or zero otherwise. Leaky ReLU also allows a small, non-zero output for negative inputs.

However, these functions might not be ideal for the final layer of Sarah's model. If the penultimate layer has successfully gathered all the necessary information for prediction, applying ReLU or Leaky ReLU to the output layer would keep any positive values intact and turn negative values into zero (or a tiny positive result in the case of Leaky ReLU). This wouldn't be useful, as the magnitude of these values wouldn't help differentiate between inputs.

Conversely, the [Sigmoid function](https://en.wikipedia.org/wiki/Logistic_function) can convert its input into a range from 0 to 1. This feature makes it apt for binary classification models. By setting a threshold, we can interpret the model's output. For example, anything equal to or under `0.5` could be interpreted as the absence of a nut, and anything above `0.5` could indicate its presence.

Finally, the [Softmax function](https://en.wikipedia.org/wiki/Softmax_function) can transform a vector of numbers into a vector of probabilities. In the context of the binary classifier that Sarah's team is working on, Softmax could output a vector with two values: the first representing the absence of a nut and the second representing its presence. The team could then select the larger of these two values to make the final prediction.

It's worth noting that Softmax is essentially a generalized form of the Sigmoid function for multi-class cases. However, Softmax reduces to Sigmoid when used in a binary classification context, resulting in the same outcome.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["A Gentle Introduction To Sigmoid Function"](https://machinelearningmastery.com/a-gentle-introduction-to-sigmoid-function/) for a quick introduction to Sigmoid.* ["The Differences between Sigmoid and Softmax Activation Functions"](https://medium.com/arteos-ai/the-differences-between-sigmoid-and-softmax-activation-function-12adee8cf322) is a short comparison between these two functions.</p></details>

-----------------------

## Date - 2023-08-18


## Title - Fish species


### **Question** :

Selah has just received an exciting project at work, where she has to create an object detection model to identify various fish species from underwater footage.

She wants to construct a model that's not just tailored to this specific task but also one that she can adapt for future related tasks. 

Selah needs to have an efficient evaluation method to ensure her model is performing optimally. This method should allow her to assess different versions of her model and pick the most effective one.

**Which evaluation metrics should Selah use to evaluate her model?**


### **Choices** :

- F1 score
- Recall
- ROC Curve
- Precision-Recall Curve


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The recall is a useful metric for object detection, but it doesn't give Selah the complete picture unless it's viewed in conjunction with precision. A model could have a high recall but terrible precision, which would not be useful for Selah. Thus, recall by itself is not the ideal metric.

[ROC Curves](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) pose a problem for object detection problems because they require the notion of True Negatives to calculate the False Positive Rate, which is one of the ROC curve's axes. However, in object detection tasks like Selah's, the number of bounding boxes that don't contain an object of interest is typically too large to calculate True Negatives accurately; hence ROC curves are not usually employed in such scenarios.

Instead, Selah could calculate a [Precision-Recall Curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html). This curve is akin to the ROC curve, but it uses the model's precision, which doesn't rely on True Negatives, making it more suitable for her object detection task.

Lastly, computing the [F1-score](https://en.wikipedia.org/wiki/F-score) is also a good choice because it considers both the model's precision and recall, offering a balanced view of its performance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Classification: ROC Curve and AUC"](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) for an explanation of how to create and interpret a ROC curve.* For more information about Precision-Recall curves, check [Scikit-Learn's documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html).</p></details>

-----------------------

## Date - 2023-08-19


## Title - Satisfaction scores


### **Question** :

Jennifer is a data scientist on a machine learning project involving customer satisfaction scores.

In the dataset she is handling, the feature representing the scores is called "Satisfaction Level" and consists of four unique values: "Very Dissatisfied," "Dissatisfied," "Satisfied," and "Very Satisfied."

Jennifer is discussing the type of this feature with her team. Some members suggest it's nominal, while others argue it's ordinal.

Now, Jennifer wants to settle the dispute by clearly understanding the properties of an ordinal feature.

**Which of the following definitions correctly summarizes what an ordinal feature is?**


### **Choices** :

- An ordinal feature is a categorical variable with a meaningful order.
- An ordinal feature is a categorical variable with ten or more possible values.
- Any feature used in a machine learning model is an ordinal feature.
- An ordinal feature is a categorical variable with fewer than ten possible values.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Jennifer can refer to Jason Brownlee's ["Ordinal and One-Hot Encodings for Categorical Data"](https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/) article for a clear definition:

> [An ordinal variable is a] variable that comprises a finite set of discrete values with a ranked ordering between values.

This means that an ordinal feature is a type of categorical variable but with a clear and meaningful order in its values. In Jennifer's case, the "Satisfaction Level" feature is indeed ordinal as the levels of satisfaction follow a natural order from "Very Dissatisfied" to "Very Satisfied."

The number of potential values doesn't determine whether a feature is ordinal. Also, machine learning models can use various types of features, not just ordinal ones. For instance, features can be nominal or numerical.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Ordinal and One-Hot Encodings for Categorical Data"](https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/) for an explanation about ordinal features.</p></details>

-----------------------

## Date - 2023-08-20


## Title - Get them unstuck


### **Question** :

Rebecca was the first one at the virtual meeting.

She has been assisting a client who has embarked on building a deep-learning model. However, the client's progress has been stalled, and it's Rebecca's role to get them unstuck.

A mere ten minutes into the conversation, Rebecca recognized the problem.

While training their network, during the backpropagation process, the gradients rapidly shrunk until they nearly hit zero, causing the weights in the lower layers to remain static.

Rebecca immediately identified this as the vanishing gradient problem. 

**Which of the following could be causing the model to suffer from the vanishing gradient problem?**


### **Choices** :

- The hidden layers of the model use the ReLU activation function.
- The hidden layers of the model use the tanh activation function.
- The model uses a very large learning rate.
- The hidden layers of the model use the sigmoid activation function.


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>If the gradients of the loss function approach zero, the model will stop learning because the network will stop updating the weights. This phenomenon is known as the [Vanishing Gradient Problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and it's very common when using the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) and [tanh](https://www.sciencedirect.com/topics/mathematics/hyperbolic-tangent-function) activation functions in deep neural networks.

The sigmoid and tanh functions squeeze a large input space into a value between `[0..1]` and `[-1..1]`, respectively. Therefore, large changes in the input of these functions cause small changes in the output. On top of that, both functions saturate when their input grows extremely large or small. Sigmoid saturates at `0` and `1`, and tanh saturates at `-1` and `1`. The derivatives at these extremes are very close to zero. 

While this is not a problem in shallow networks, the gradients may become too small for deeper networks that use these activation functions, and the network ceases to learn.

[ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) helps to address the vanishing gradient problem. It's less likely to saturate, and its derivative is `1` for values greater than zero. Thus, the first choice is incorrect.

While an excessively large learning rate can indeed cause problems in the learning process, typically, it results in instability and the inability of the model to converge rather than the vanishing gradient problem.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The Wikipedia page of ["Vanishing gradient problem"](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) has a great explanation of this issue.* Check ["How to Fix the Vanishing Gradients Problem Using the ReLU"](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/) for an explanation of how ReLU fixes this problem.</p></details>

-----------------------

## Date - 2023-08-21


## Title - Cash register


### **Question** :

Monica is a data scientist at a large grocery store chain.

The grocery store has modern cash register systems, so Monica can access a wealth of sales data. She's fascinated by the massive variety of items customers purchase every day. The store sells everything from fruits and vegetables to household items.

Monica is thinking about creating customer profiles to understand purchasing patterns better. This might help the store to design better sales promotions or arrange products more efficiently. However, she's unsure whether she should segment customers based on the types of products they buy, their total spending, or the time of their purchases.

**If you were in Monica's position, how would you approach this problem?**


### **Choices** :

- Define a few profiles beforehand, and train a supervised learning algorithm to classify every customer into one.
- Use a supervised learning algorithm to produce potentially interesting ways to segment the customers.
- Use an unsupervised learning algorithm to produce potentially interesting ways to segment the customers.
- Use a semi-supervised learning algorithm to process the data, thus taking advantage of both supervised and unsupervised techniques.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Customer segmentation](https://towardsdatascience.com/customer-segmentation-with-machine-learning-a0ac8c3d4d84) is a popular field where you try to find similar characteristics among your customers. It's the perfect opportunity to use unsupervised learning: a clustering algorithm.

For example, Monica could use [K-Means](https://en.wikipedia.org/wiki/K-means_clustering) to find interesting patterns and group together the customers that share them. A critical distinction is that she doesn't need to consider the segments preemptively; the clustering algorithm will find them for her.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Customer Segmentation with Machine Learning"](https://towardsdatascience.com/customer-segmentation-with-machine-learning-a0ac8c3d4d84) for a quick introduction to Customer Segmentation.* ["10 Clustering Algorithms With Python"](https://machinelearningmastery.com/clustering-algorithms-with-python/) will introduce you to 10 different clustering algorithms.</p></details>

-----------------------

## Date - 2023-08-22


## Title - Pulling hairs


### **Question** :

Magdalena has been pulling her hair out.

She's been laboring on this convolutional neural network for ages. She's using mini-batch gradient descent for her image classification task, and the results show the training loss swinging wildly up and down.

It's driving her nuts!

Magdalena is considering one or two last-ditch efforts before she throws in the towel.

**Which of the following steps do you think she should attempt next?**


### **Choices** :

- She should decrease the learning rate to avoid overshooting the local minimum.
- This is likely a problem with the data. Magdalena should revisit her dataset and ensure it's appropriately balanced.
- She should expand the batch size to increase the range of samples in every batch.
- Magdalena should increase the learning rate to take more significant steps toward the gradient.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The issue with the training loss is unlikely to be related to an imbalanced dataset. If that were the root cause, Magdalena would face a model with difficulty learning anything, but she wouldn't see the loss bouncing up and down.

If she increases the learning rate, Magdalena would be able to take more significant steps in the gradient's direction, but this could lead her to overshoot the local minima. If her learning rate is too large, she'll repeatedly overshoot the local minima, and her loss will keep swinging back and forth.

There's a good chance that the fluctuation in loss is due to a high learning rate. Reducing the learning rate could be a good experiment to see if it addresses the issue.

Lastly, if Magdalena uses a very small batch size, poor samples could cause significant shifts in the training loss from one batch to the next. This could also be the reason for the fluctuating loss. Increasing the batch size and ensuring the data is appropriately distributed is another good experiment.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["The wrong batch size is all it takes" ](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) explains how different batch sizes influence the training process of neural networks using gradient descent.* ["What could an oscillating training loss curve represent?"](https://ai.stackexchange.com/questions/14079/what-could-an-oscillating-training-loss-curve-represent) is a StackExchange question that will help you answer this question.* Check ["Why is my training loss fluctuating?"](https://www.researchgate.net/post/Why_is_my_training_loss_fluctuating) for another set of answers covering this problem.</p></details>

-----------------------

## Date - 2023-08-23


## Title - Online pet store


### **Question** :

Lucy's online pet store has seen a significant increase in customers and product variety.

The neural network model Lucy's team had implemented showed signs of distress. The problem was easy to identify: the company collected more data and features from a wider range of customers. The current model wasn't utilizing all the available information, leading to underfitting.

Lucy recognized the need to augment the capacity of her model.

**Which of the following steps can Lucy take to augment the capacity of her neural network model?**


### **Choices** :

- Lucy should increase the regularization applied to the model.
- Lucy should increase the number of nodes on each layer.
- Lucy should raise the learning rate for training the model.
- Lucy should increase the number of hidden layers in the neural network.


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>This piece from ["How to Control Neural Network Model Capacity With Nodes and Layers"](https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers/) offers a good understanding:

> The capacity of a deep learning neural network model controls the scope of the types of mapping functions that it is able to learn. (...) The capacity of a neural network model is defined by configuring the number of nodes and the number of layers.

Lucy has three options for expanding her network's capacity:
* She can add more hidden layers.
* She can increase the number of nodes on each layer.
* She can combine these two strategies.

Increasing the network's capacity will give the model more generalization power, so Lucy should see improved performance as the new model accommodates more data.

The learning rate and regularization methods do not influence the network's capacity.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [How to Control Neural Network Model Capacity With Nodes and Layers](https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers/)* [The capacity of feedforward neural networks](https://www.math.uci.edu/~rvershyn/papers/bv-capacity-neural-networks.pdf)</p></details>

-----------------------

## Date - 2023-08-24


## Title - Allergic reaction


### **Question** :

Rachel needs help with her machine learning project.

Using patient data, she is building a model to predict a specific type of rare allergic reaction. The challenge is that Rachel's dataset contains far more instances of patients without the allergy than those with it, causing her model to misclassify allergic patients as non-allergic frequently.

Getting more data for such a rare allergy is difficult, and strict privacy rules add further complications.

Rachel is seeking other ways to enhance her model.

**Which of the following strategies could Rachel use to improve her model's accuracy?**


### **Choices** :

- Augment the dataset with slightly modified copies of the patient data showing the allergy.
- Reduce the learning rate to allow the model to learn from the underrepresented samples.
- Swap out her model with a Decision Tree, as these are more resistant to imbalanced data.
- Assign a higher weight to the samples indicating the rare allergy.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Dealing with imbalanced datasets is a common challenge in machine learning, and numerous strategies can help address it. Often, the best solution involves a mix of different approaches.

A typically straightforward technique assigns a higher weight to the underrepresented classes in the loss function. This method increases the penalty for the model if it misclassifies a sample from the rare class. In simple terms, this makes certain classes "more important" from the loss function's perspective. All major machine learning frameworks provide a way to adjust the weight assigned to each class, making this a correct choice.

Another practical approach to this problem is augmenting the underrepresented class. Data augmentation involves creating new training data by making small sample changes. In this context, Rachel could introduce minor variations to the patient data.

However, it's worth noting that there are other strategies for dealing with imbalanced datasets. Other techniques could involve oversampling the underrepresented classes, downsampling the dominant class, or creating additional synthetic data.

Finally, Decision Trees aren't inherently more robust to imbalanced data, and reducing the learning rate doesn't directly help the model learn better from the underrepresented class.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check out ["A Gentle Introduction to Imbalanced Classification"](https://machinelearningmastery.com/what-is-imbalanced-classification/) for a description of how to tackle imbalanced problems.</p></details>

-----------------------

## Date - 2023-08-25


## Title - Interested student


### **Question** :

Rayna is a computer science student who has been studying machine learning. She has been particularly interested in using activation functions in neural networks. 

Rayna has been studying the Sigmoid Activation Function, often used in binary classification problems. The Sigmoid function is a smooth, S-shaped function that maps any real-valued number to a value between 0 and 1. 

Rayna is curious about the mathematical properties of the Sigmoid function, especially when it comes to its continuity and differentiability.

**Which of the following is true about the Sigmoid Activation Function?**


### **Choices** :

- The function is neither continuous nor differentiable.
- The function is differentiable but not continuous.
- The function is both continuous and differentiable.
- The function is continuous but not differentiable.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The Sigmoid function is continuous and differentiable.

The Sigmoid function is smooth and differentiable everywhere, including at `x=0`. This is one of the reasons why it was widely used in the early days of neural networks. 

However, the Sigmoid function has some drawbacks, such as the vanishing gradient problem, which can slow the learning process. This is one of the reasons why other activation functions, like ReLU, have become more popular in recent years.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["A Gentle Introduction To Sigmoid Function"](https://machinelearningmastery.com/a-gentle-introduction-to-sigmoid-function/) for a quick introduction to Sigmoid.</p></details>

-----------------------

## Date - 2023-08-26


## Title - The weird puzzle room


### **Question** :

Vivian took her kids to an escape room. They had to solve a bunch of puzzles to get out.

But this room wasn't like every other room. 

They found the final puzzle behind a painting hanging from the wall. Four numbered little doors. The key hid behind one of them and a losing score behind the other three.

Above the doors, they found the question: "What do you think about the depth of a decision tree?"

**What door will let Vivian's family out of the room?**


### **Choices** :

- The optimal value for a decision tree's depth is setting it to the number of training samples minus one.
- The optimal value for a decision tree's depth is setting it to the logarithm of the number of training samples.
- Trying to make the decision tree shallower is usually better than trying to make it deeper.
- Trying to make the decision tree deeper is usually better than trying to make it shallower.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We need to revisit a few concepts and establish constraints to answer this question. How do we know a decision tree has an optimal depth?

First, the depth of a decision tree is the length of the longest path from a root to a leaf. There's a tradeoff to keep in mind related to this value. Deeper trees will lead to more complex models prone to overfitting. Shallower trees will have the opposite problem: less complex models prone to underfitting. 

To determine the optimal depth, we then need to establish how we will measure the performance of the decision tree. Since the goal is to build a model that produces good predictions, I'll assume that our goal is to find the optimal depth to get the best predictions possible on a test dataset.

Let's start with the first choice that argues that we should set the depth at the number of training samples minus one. That's the maximum theoretical depth of a decision tree, but it's not the optimal value. If we make the tree this deep, we will overfit the training data and perform poorly on unseen data.

The second choice is also incorrect. Although this option is probably better than setting the depth to the number of samples, it will still lead to overfitting. For example, for a dataset with 1024 training samples, we will need a decision tree with ten levels and a tree of depth 10, which requires a total of `2^10 = 1024` nodes. This choice will lead to a tree with as many nodes as samples, which will cause the tree to overfit.

Are we better off with a shallower or a deeper tree? Less complex classifiers will usually generalize better than more complex ones. We should remove any non-critical or redundant sections of a decision tree, which generally leads to a much better model. [Decision tree pruning](https://en.wikipedia.org/wiki/Decision_tree_pruning) is one technique used to accomplish this.

If you have two decision trees with the same predictive power in your test dataset, always pick the simpler one. Therefore, we should always set the depth as low as possible without affecting the model's predictive capabilities.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["How to tune a Decision Tree?"](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680) is a great article talking about different ways to tune a decision tree, including the effects of setting the maximum depth hyperparameter.- ["Decision tree pruning"](https://en.wikipedia.org/wiki/Decision_tree_pruning) is a Wikipedia article covering pruning and its effects.- A great article to understand the relationship between the bias-variance tradeoff, overfitting, and under-fitting is ["How To Find Decision Tree Depth via Cross-Validation"](https://towardsdatascience.com/how-to-find-decision-tree-depth-via-cross-validation-2bf143f0f3d6).</p></details>

-----------------------

## Date - 2023-08-27


## Title - Predictive maintenance


### **Question** :

Mariam built a machine learning model that looks at pictures of mechanical components and predicts whether they need maintenance.

After successfully testing the model in a pilot project, the company plans to start using it in one of its warehouses. Before they are ready, Miriam needs to summarize the performance of the pilot project. 

She ran pictures of 100 different components through the model and found the following:

* The model predicted that 7 components needed maintenance. After manually inspecting them, only 5 required work, but the other 2 were okay. 
* The model predicted that 93 components were working as expected but missed 2 that needed maintenance.

**Which of the following is the correct summary of accuracy, precision, and recall for Miriam's model?**


### **Choices** :

- The model's accuracy is 93%, the precision is 29%, and the recall is 71%.
- The model's accuracy is 98%, the precision is 71%, and the recall is 29%.
- The model's accuracy is 90%, the precision is 98%, and the recall is 29%.
- The model's accuracy is 96%, the precision is 71%, and the recall is 71%.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The easiest way to answer this question is to put the information we know in a [confusion matrix](https://articles.bnomial.com/confusion-matrix). Whenever you build a classification model, the confusion matrix will help tremendously. Let's see how.

Here is the confusion matrix with Miriam's results. Notice the small annotations next to each value:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/168438082-c56db22f-935b-4e27-9d01-ca8f881a8ad1.png)

* Miriam's model found 5 true positives (TP). These are the components that the model predicted that needed maintenance that actually needed it.
* There are 2 false positives (FP). These are the other 2 components the model thought erroneously needed maintenance.
* There are 91 true negatives (TN). These are the components the model classified as okay and were indeed fine.
* There are 2 false negatives (FN). These are the 2 components that the model missed as needing maintenance.

We can compute the three metrics we need using this information. Let's start with accuracy:

```
accuracy = (TP+TN)/(TP+TN+FP+FN)
accuracy = (5+91)/(5+91+2+2)
accuracy = 96/100
accuracy = 0.96
```

The [precision](https://en.wikipedia.org/wiki/Precision_and_recall#Precision) of the model is the fraction of components that truly need maintenance among the components that the model predicts need it. We can compute the precision this way:

```
precision = TP/(TP + FP)
precision = 5/(5 + 2)
precision = 5/7
precision = 0.71
```

The [recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall) of the model is the fraction of components that truly need maintenance among all the existing components that need maintenance. We can compute the recall this way:

```
recall = TP/(TP + FN)
recall = 5/(5 + 2)
recall = 5/7
recall = 0.71
```

As you can see, the fourth choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2023-08-28


## Title - Test performance


### **Question** :

Mariah is facing a challenge in her machine learning project, as her model is not performing well on the test data.

She wonders if the training and test datasets have different distributions, leading her to consider adversarial validation as a method to investigate this issue.

**How should Mariah properly implement adversarial validation?**


### **Choices** :

- Merge the training dataset with the test dataset and assign a new binary target, with 1 for test samples and 0 for training samples.
- Merge the training dataset with the test dataset and assign a new binary target, with 0 for test samples and 1 for training samples.
- Divide the test dataset into two subsets and assign a new binary target to each subset, with 1 for the first subset and 0 for the second subset.
- Divide the validation dataset into two subsets and assign a new binary target to each subset, with 1 for the first subset and 0 for the second subset.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Popular validation techniques, like cross-validation, allow you to test your models on unseen data if that data comes from the same distribution as your training dataset. Unfortunately, that's not always the case, and even slight differences between the training and test data will considerably affect the result of your model.

[Adversarial validation](https://articles.bnomial.com/adversarial-validation) is a technique to estimate the difference between your training and test data. [_The Kaggle Book_](https://amzn.to/3kbanRb) introduces it as follows:

> [adversarial validation] was long rumored among Kaggle participants and transmitted from team to team until it emerged publicly, thanks to a post by Zygmunt Zając on his FastML blog.

To set it up, you will create a new dataset by joining the training and test data. The target of that new dataset is a binary variable differentiating the training and test samples. You can determine how easy it's to separate both datasets by running a classifier on that new data.

Adversarial validation relies on computing the [ROC-AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc), a graph showing the True Positive Rate and the False Positive Rate at different classification thresholds. The area under this curve (AUC) measures the model's performance. A perfect model will have an area of `1.0`, while a model that only makes mistakes will have an area of `0.0`.

If you run the classifier and the ROC-AUC is around `0.5`, you will know that the training and test data are not easily distinguishable, which is good because it means the data comes from the same distribution. If the ROC-AUC is too high—closer to `1.0`—the classifier can tell training and test data apart, which means they come from a different distribution.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Adversarial validation"](https://articles.bnomial.com/adversarial-validation) is a great introduction to adversarial validation.* Check ["What is Adversarial Validation?"](https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation/notebook) for a discussion about this technique in Kaggle.* [_The Kaggle Book_](https://amzn.to/3kbanRb) is an amazing reference for those looking to participate in Kaggle.</p></details>

-----------------------

## Date - 2023-08-29


## Title - Distance measures


### **Question** :

Distance measures are critical in many machine learning algorithms: they summarize the relative difference between two objects. K-Nearest Neighbors is an example of an algorithm that uses a distance measure at its core.

Every distance measure doesn't work for every problem. Instead, we must select the appropriate function depending on the nature of the data.

**Here is a list of four distance measures and a quick summary of how they work. Select every one of them that's correct.**


### **Choices** :

- The Euclidean distance between two vectors is the square root of the sum of the squared differences between them.
- The Manhattan distance between two vectors is the sum of their absolute differences.
- The Hamming distance is a generalization of the Euclidean and Manhattan distances that we can tune depending on which distance measure we need.
- The Minkowski distance computes the distance between two binary vectors.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>While the Euclidean and Manhattan distances summary is correct, the Hamming and Minkowski summaries aren't.

The Hamming distance computes the distance between two binary vectors. For example, you can calculate the distance between objects using a one-hot encoded feature.

The Minkowski distance is a generalization of the Euclidean and Manhattan distances. Both of these work with real-value vectors, but the Euclidean distance is the shortest path between objects, while the Manhattan distance is the rectilinear distance between them. Using the Minkowski distance, we can control which approach to use depending on the data.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["4 Distance Measures for Machine Learning"](https://machinelearningmastery.com/distance-measures-for-machine-learning/) for a complete explanation of these four distance measures.* ["Five Common Distance Measures in Data Science With Formulas and Examples"](https://regenerativetoday.com/five-common-distance-measures-in-data-science-with-formulas-and-examples/) is a deeper dive into these distance measures.</p></details>

-----------------------

## Date - 2023-08-30


## Title - Softmax mockery


### **Question** :

Despite years of research, Loretta never paid too much attention to the softmax function and its properties. She never needed it, so imagine her surprise when Alan pushed her about it.

Alan was cooky. He didn't know much but liked to show off with every bit of trivia that crossed his path. More than anything, he enjoyed pissing off Loretta at every opportunity he had.

And here is Alan, bugging Loretta about the softmax function and ready to mock her if she fails.

**Which of the following are correct statements about the softmax function?**


### **Choices** :

- The softmax function is a soft or smooth approximation to the max function.
- The softmax function is a soft or smooth approximation to the argmax function.
- The softmax function converts a vector of `n` values into a probability distribution of `n` possible outcomes.
- Softmax is a differentiable function.


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The name "softmax" is a misnomer. The softmax function is a smooth—soft—approximation of the argmax function, not the max function. To avoid this confusion, some of the literature uses "softargmax" instead, but the machine learning world ran with "softmax" and never looked back.

The softmax function turns a vector of `n` values into another vector of `n` probabilities that sum to 1. These probabilities are proportional to the relative scale of each value in the vector. For example, when we use softmax as the activation function of a neural network's output layer, we get a normalized probability distribution that we can use to interpret the result of multi-class classification models.

Finally, contrary to the argmax function, the softmax function is differentiable.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["The Softmax function and its derivative"](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/) for a complete explanation of softmax.* ["What is the Softmax Function?"](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer) is another great explanation of how the function works.</p></details>

-----------------------

## Date - 2023-08-31


## Title - Vintage shoes


### **Question** :

Sam owns a small store that sells vintage shoes. 

Business is going great, and one of her main issues is determining the price of new inventory as it comes in. Sam would like to create a machine learning model that automatically selects the best price using the historical performance.

Sam was a data scientist in a previous life, so she was surprised when she found some documentation recommending a new loss function: The Root Mean Squared Log Error (RMSLE.) This function seemed better than Root Mean Squared Error (RMSE) for her use case.

**Which of the following are some of the differences between RMSLE and RMSE?**


### **Choices** :

- RMSLE penalizes coming under the actual value much more than coming above the actual value. On the other hand, RMSE penalizes both cases in the same way.
- RMSLE is not sensitive to outliers as RMSE is. Whenever we have outliers, the result of RMSE can explode while RMSLE will scale down the outliers.
- RMSLE measures the error by only focusing on the predicted value. On the other hand, RMSE uses the difference between the predicted and the actual value to compute the error.
- RMSLE focuses on the relative error between two values. When using RMSLE, the scale of the error is not significant. On the other hand, the result of RMSE increases in magnitude if the error scale increases.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Compared to [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation), [RMSLE](https://www.kaggle.com/code/carlolepelaars/understanding-the-metric-rmsle/notebook) is a relatively new metric. Here is the formula:

![RMSLE Formula](https://user-images.githubusercontent.com/1126730/168863453-215ebc00-201b-40f2-8cea-fcd998df3cd7.png)

If you compare this formula with the RMSE formula, you'll notice they are almost the same, with the difference of RMSLE computing the log of both the predicted and actual values. This small difference comes with some essential properties.

First, RMSLE penalizes underestimates much more than overestimates. To understand the reason, take a look at the following charts that I took from ["What's the Difference Between RMSE and RMSLE?"](https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a):

![RMSLE vs RMSE](https://user-images.githubusercontent.com/1126730/168865119-e9947726-a6c3-4a19-9b90-e8993bb3c105.png)

Notice how RMSLE penalizes negative values much more heavily than positive ones, while RMSE equally penalizes both cases. Therefore, the first choice is correct.

The second choice is also correct. The log that RMSLE uses squashes outliers, while RMSE amplifies their effect. If you want your model to remain unaffected by outliers, RMSLE is a good candidate.

The third choice is incorrect because both RMSE and RMSLE use the predicted and actual values. Predicted values alone don't tell us anything; comparing them with the actual value is critical.

Finally, the fourth choice is also correct. For example, imagine a case where the model predicts 30 when the actual value was 40 and another case where the prediction was 300 when the actual value was 400. The RMSE of the second case is ten times more than the RMSE of the first case. However, the RMSLE will score both cases the same. The RMSLE measures the relative error, while the RMSE measures the absolute error.

In summary, the first, second, and fourth choices are correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What's the Difference Between RMSE and RMSLE?"](https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a) covers really well every one of the differences between these two functions.* Another article that covers the differences between some of the most popular functions is ["Evaluation Metrics for Regression models"](https://akhilendra.com/evaluation-metrics-regression-mae-mse-rmse-rmsle/).</p></details>

-----------------------

## Date - 2023-09-01


## Title - Bounding boxes


### **Question** :

Magdalena is working on a model to detect people in footage from security cameras.

She wants to measure the performance of her object detection method, but there is a problem: The bounding boxes predicted by her model never match precisely the bounding boxes in the ground truth data. 

Magdalena needs a metric that tells her how well the two bounding boxes overlap. Based on this metric, she can define a threshold and decide if the predicted bounding box is correct or not.

**Which metric can Magdalena use?**


### **Choices** :

- The percentage of the ground truth bounding box that's covered by the predicted bounding box.
- The percentage of the predicted bounding box that's covered by the ground truth bounding box.
- The intersection area of the predicted and ground truth bounding boxes divided by their union.
- The intersection area of the predicted and ground truth bounding boxes multiplied by their union.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>After a quick look at the choices, the first two options sound like plausible solutions. However, they have fundamental flaws that may allow a bad model to score high on these metrics.

Let's take the percentage of the ground-truth bounding box covered by the prediction. If our model learns to predict huge bounding boxes covering almost the whole image, we will always get a score close to 100%. 

If we take the percentage of the predicted bounding box covered by the ground truth, we have the opposite problem: a small bounding box somewhere in the bounds of the ground truth will give us a 100% score. Therefore the first two choices are incorrect.

An excellent way to deal with these problematic cases is to take the so-called intersection over union measure (IoU for short, also called the [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index)). We compute the intersection area of the bounding boxes and divide it by their union. The union will increase if the predicted bounding box is too big, driving the score down. The small intersection area will lower the score if the bounding box is too small. 

Finally, the last choice is also incorrect. Multiplying the intersection by the union will not give us any relevant or helpful results.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check ["Intersection over Union (IoU) for object detection"](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/) for an entire explanation of this metric.</p></details>

-----------------------

## Date - 2023-09-02


## Title - The second die


### **Question** :

Winter is planning a board game night with her friends and wants to surprise them with a new game. 

The game comes with two dice. One is a 6-sided die, and the other is a huge 24-sided die.

While getting ready, Winter's brother enters the room, curious about the dice. He selects one of the dice from the box and rolls a 4.

**What is the probability that her brother pulled out the 24-sided die?**


### **Choices** :

- The probability is 1/5
- The probability is 2/5
- The probability is 3/5
- The probability is 4/5


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's assume that `A` represents the event of rolling the die and getting a `4`, `B1` represents pulling out the 6-sided die, and `B2` represents pulling out the 24-sided die.

We can compute the probability that Winter's brother picked the 24-sided die using the [Bayes theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem):

```
P(B2|A) = (P(A|B2)*P(B2))/P(A)
P(B2|A) = (1/48)/P(A)

P(A) = P(A|B2)*P(B2)+P(A|B1)*P(B1)
P(A) = 1/48 + 1/12​
P(A) = 5/48

P(B2|A) = (1/48)/(5/48)
P(B2|A) = 1/5​
```

Thus, the answer is `1/5`.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["A Gentle Introduction to Bayes Theorem for Machine Learning"](https://machinelearningmastery.com/bayes-theorem-for-machine-learning/) is a great starting point to understand the Bayes theorem and how to use it.* You can also check the Wikipedia page of [the Bayes theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem).</p></details>

-----------------------

## Date - 2023-09-03


## Title - Recent weeks


### **Question** :

In her Machine Learning course, Samantha has been given an assignment. She's fairly new to the subject, and they've been discussing structured data and different types of features in recent weeks.

Her task is to preprocess a dataset by converting all nominal features into numeric representations using one-hot encoding.

However, Samantha is facing a hurdle: she has forgotten the exact definition of "nominal features."

**Which of the following descriptions correctly describes a nominal feature?**


### **Choices** :

- A nominal feature is a categorical variable with more than ten possible values.
- A nominal feature is a categorical variable with fewer than ten potential values.
- A nominal feature is a categorical variable with an established order.
- A nominal feature is a categorical variable without a meaningful order.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>As mentioned by Jason Brownlee in his ["Ordinal and One-Hot Encodings for Categorical Data"](https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/) article:

> [A nominal variable is a] variable that comprises a finite set of discrete values with no relationship between values.

This means that a nominal feature is a categorical feature where the order of its values doesn't hold any significance. For example, consider a feature "Fruit" with possible values: "Apple," "Banana," and "Cherry." There is no meaningful order among these values. 

The quantity of potential values does not determine whether a feature is nominal. Also, nominal features do not have an inherent order among their values.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Ordinal and One-Hot Encodings for Categorical Data"](https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/) for an explanation of nominal features.</p></details>

-----------------------

## Date - 2023-09-04


## Title - Initializing neural networks


### **Question** :

High-level libraries are usually enough to solve complex problems without worrying about low-level details.

But Gabriella didn't have that luxury. Her research got her very deep into how neural networks work. 

As part of the project, she wants to write a blog post about the influence of different initialization schemes on how neural networks learn. She wants to cover the problems that could happen when we use the wrong initialization.

**Which of the following problems could we face if we don't initialize the weights correctly?**


### **Choices** :

- The network might suffer from vanishing gradients, which could cause a slow down in training.
- The network might suffer from exploding gradients, preventing it from learning correctly.
- Every network neuron might learn the same features during training.
- Every neuron of the network might learn different features during training.


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Correctly initializing a neural network can have a significant impact on convergence.

We may suffer from the vanishing gradient problem if we use small values to initialize the network's weights and the exploding gradient problem if we use large weight values instead. The gradients will become smaller or larger as we move from the output layer toward the input layer during backpropagation. ["Initializing neural networks"](https://www.deeplearning.ai/ai-notes/initialization/) illustrates a specific example with a 9-layer neural network using the identity function as the activation on every layer. 

The third choice is correct: if we initialize the network with zeros, every neuron will learn the same features. Here is an excerpt from ["Initializing neural networks"](https://www.deeplearning.ai/ai-notes/initialization/) explaining the consequences of using the same weight values:

> Thus, both hidden units will have identical influence on the cost, which will lead to identical gradients. Thus, both neurons will evolve symmetrically throughout training, effectively preventing different neurons from learning different things.

The final choice, however, is not a problem. We want every neuron to learn different features.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Initializing neural networks"](https://www.deeplearning.ai/ai-notes/initialization/) is an excellent summary of the importance of weight initialization.* Check ["Weight Initialization Techniques in Neural Networks"](https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78) to learn about different initialization schemes.</p></details>

-----------------------

## Date - 2023-09-05


## Title - Apples and oranges


### **Question** :

Everlee has been delving into the world of computer vision recently.

Specifically, she's concentrating on using deep learning and applying pre-trained models to distinguish between images of apples and oranges.

She has gathered a dataset of 1,000 256 x 256 color images. Half of these images are apples, while the other half are oranges.

While exploring this problem, Everlee had an interesting question:

**What's the appropriate shape for a tensor to hold all this data?**


### **Choices** :

- It can be stored in a tensor of shape `(1000, 256, 256)`
- It can be stored in a tensor of shape `(1000, 256, 256, 1)`
- 2. It can be stored in a tensor of shape `(1000, 256, 256, 2)`
- It can be stored in a tensor of shape `(1000, 256, 256, 3)`


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When dealing with images, it's important to consider the three components of an image: height, width, and color depth. A color image has three color channels (one each for red, green, and blue), while grayscale images have only one.

Assuming our images are 256 x 256 in size, we would need to store the RGB values for each pixel, so our tensor shape would need to be `(256, 256, 3)`. If we were only working with grayscale, our tensor could have a shape of `(256, 256)`, but conventionally, we add a dimension to accommodate color depth, allowing the same structure for both grayscale and color images. So, a single grayscale image would be stored in a `(256, 256, 1)` tensor. Since we're working with color images, we need a tensor of shape `(256, 256, 3)`.

That covers how we store a single image. But what if we want to store 1,000 such images? We add another dimension, giving us a tensor of shape `(1000, 256, 256, 3)`.

Conventionally, tensors holding image data are arranged in the following manner: (samples, height, width, and channels). While there is another convention that places channels before height and width, most practitioners place them at the end.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Deep Learning with Python_, Second Edition](https://amzn.to/3K3VZoy) covers the topic of tensors really well.* Check ["A Gentle Introduction to Tensors for Machine Learning with NumPy"](https://machinelearningmastery.com/introduction-to-tensors-for-machine-learning/) for a quick introduction to tensors and practical code.</p></details>

-----------------------

## Date - 2023-09-06


## Title - Cheating with a model


### **Question** :

Skylar's model had the best performance in the entire class, but there was a problem: the results were too good to be correct. 

The professor decided to review her process step by step. Skylar stood in front of the class and wrote down her process:

1. Skylar loaded the entire dataset in memory.
2. Replaced one column's missing values with the mean of the column and scaled another column using Min-Max Scaling.
3. She then split the dataset into a train and a test set.
4. And finally, she trained her model.

As soon as she finished, the professor knew what the issue was. 

**Which of the following is the reason for the model's inflated performance?**


### **Choices** :

- Skylar loaded the entire dataset in memory. She should have loaded it in batches.
- Skylar transformed her data before splitting the dataset. She should have split the data before transforming it.
- Skylar used the mean of the column to impute missing values. She should have used the median instead.
- Skylar used Min-Max Scaling to transform one of the columns. She should have applied log transformation instead.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>**Rule of thumb:** When your model's results seem too good to be true, they probably are.

The problem with Skylar's model is a data leak: she transformed her dataset before splitting it. When computing the mean of a column or applying [Min-Max Scaling](https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization)), Skylar used the test data, which she isn't supposed to have. 

It's subtle but essential. 

When doing any work with your data, ensure you only look at your train set. Test data is akin to production data: you don't have it during training time and can't make any decisions based on it.

In this example, Skylar imputed missing values using the mean of the entire column, including those samples that later became part of the test set. She also used Min-Max Scaling, which uses the minimum and maximum values of the column. She leaked information from the soon-to-be test dataset into her training process in both cases.

This leak caused Skylar's model to perform much better than others. She cheated without realizing it.

The solution is to split the dataset before doing any transformations. Skylar should compute the training data's mean, minimum, and maximum values and use them to impute and scale the test set.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check [Data Leakage in Machine Learning](https://machinelearningmastery.com/data-leakage-machine-learning/) for an introduction to data leaks and how to prevent them. * The [Wikipedia page on leakages in machine learning](https://en.wikipedia.org/wiki/Leakage_(machine_learning)) covers the topic very well.</p></details>

-----------------------

## Date - 2023-09-07


## Title - Normalizing inputs to a layer


### **Question** :

What if we normalize the input to the layers within a neural network?

Liliana understood the idea of data normalization, but this new approach seemed bold and different.

Fortunately, she found plenty of documentation about this technique, called Batch Normalization. During the training process, Batch Normalization normalizes the input to each layer within the network.

**Liliana wants to write some notes before calling it a day. Which of the following are correct statements about Batch Normalization?**


### **Choices** :

- Batch Normalization rescales the data to a mean of zero and a standard deviation of one.
- Batch Normalization rescales the data between zero and one.
- Batch Normalization makes the network less sensitive to the choice of weight initialization.
- Batch Normalization requires extra computations, slightly increasing the network's total training time.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Batch Normalization is a popular technique used to train deep neural networks. It normalizes the input to a layer during every training iteration using a mini-batch of data. It smooths and simplifies the optimization function leading to a more stable and faster training.

Batch Normalization works by scaling its input—the previous layer's output—to a mean of zero and a standard deviation of one per mini-batch. Thus the first choice is correct and the second incorrect.

Although correctly initializing a network can significantly impact convergence, the stability offered by Batch Normalization makes training deep neural networks less sensitive to a specific weight initialization scheme. Since Batch Normalization normalizes values, it reduces the likelihood of running into vanishing or exploding gradients.

Finally, Batch Normalization does require extra computations, making individual iterations slower. However, it will dramatically reduce the number of iterations needed to achieve convergence, making the training process much faster. Therefore, the fourth choice is incorrect.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["A Gentle Introduction to Batch Normalization for Deep Neural Networks"](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/) is a good introduction to Batch Normalization.* Another great article to understand the impact of Batch Normalization is ["Why is Batch Normalization useful in Deep Neural Networks?"](https://towardsdatascience.com/batch-normalisation-in-deep-neural-network-ce65dd9e8dbf)</p></details>

-----------------------

## Date - 2023-09-08


## Title - Leave one out


### **Question** :

Josie discovered that her model had worse performance when using a different test set.

She started the project by setting aside a portion of her data and built a model that, so far, impressed everyone with a strong performance. But the bubble burst when she trained and tested the model with a different portion of her dataset.

Josie is working with a very small dataset. Her team recommended k-Fold cross-validation, but Josie wanted to take the process to the extreme: she decided to create as many folds as samples in the dataset.

**Which of the following statements correctly describes what Josie should expect to happen:**


### **Choices** :

- Josie's method will be computationally cheaper than fewer folds but result in a more biased model.
- Josie's method will be computationally more expensive than fewer folds but will result in a less biased model.
- Josie's method will be computationally cheaper than fewer folds but result in a more biased model.
- Josie's method will be computationally more expensive than fewer folds and result in a more biased model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Josie's approach has a name: [Leave-one-out cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation), a case of cross-validation where the number of folds equals the number of samples in the dataset. 

To use leave-one-out cross-validation, we build one model for each sample in the dataset. We train each model using all data except one instance we later use to evaluate its performance. Finally, we compute the overall performance by averaging the result of each model.

Assuming Josie is using leave-one-out cross-validation on a dataset with 100 samples, she will need to train 100 models. Compare this with 10-Fold cross-validation, where she will only need to build ten models. Leave-one-out cross-validation is significantly more expensive than a process using fewer folds.

On the other hand, leave-one-out cross-validation will give Josie a less biased model than the 10-Fold cross-validation for this small dataset. Each training set will contain every sample but one, reducing the bias but increasing the final model's variance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["LOOCV for Evaluating Machine Learning Algorithms"](https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/) is an excellent introduction to leave-one-out cross-validation.* This [StackExchange answer](https://stats.stackexchange.com/questions/154830/10-fold-cross-validation-vs-leave-one-out-cross-validation) gives a good explanation of the bias and variance tradeoff of both methods. * Check ["A Quick Intro to Leave-One-Out Cross-Validation (LOOCV)"](https://www.statology.org/leave-one-out-cross-validation/) for a brief introduction to leave-one-out cross-validation.</p></details>

-----------------------

## Date - 2023-09-09


## Title - Universal approximators


### **Question** :

A theoretical question that Daisy always liked to discuss was the ability of machine learning algorithms to approximate any function.

She called them "universal approximators." To classify, they had to approximate any measurable or continuous function up to any desired accuracy.

**Which of the following would you consider universal approximators?**


### **Choices** :

- Neural Networks
- Support Vector Machines using a Gaussian kernel
- Linear Regression
- Boosted Decision Trees


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Except for linear regression, all the other three choices are universal approximators.

Thanks to the [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem), neural networks are a well-known member of this category. The theorem states that, when using non-linear activation functions, we can turn a two-layer neural network into a universal function approximator.

The authors of ["A Note on the Universal Approximation Capability of Support Vector Machines"](https://www.researchgate.net/publication/2531186_A_Note_on_the_Universal_Approximation_Capability_of_Support_Vector_Machines) show that support vector machines with standard kernels can also approximate any measurable or continuous function.

Finally, the ["Universal approximation"](https://kenndanielso.github.io/mlrefined/blog_posts/12_Nonlinear_intro/12_5_Universal_approximation.html) chapter of [_Machine Learning Refined_](https://amzn.to/3OXcHZF) covers trees as one of the standard types of universal approximations.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["A Note on the Universal Approximation Capability of Support Vector Machines"](https://www.researchgate.net/publication/2531186_A_Note_on_the_Universal_Approximation_Capability_of_Support_Vector_Machines) is a paper showing that SVMs with standard kernels can approximate any measurable or continuous function.* Chapter 12.5 ["Universal approximation"](https://kenndanielso.github.io/mlrefined/blog_posts/12_Nonlinear_intro/12_5_Universal_approximation.html) of [_Machine Learning Refined_](https://amzn.to/3OXcHZF) covers kernels, neural networks, and trees as universal approximations.</p></details>

-----------------------

## Date - 2023-09-10


## Title - Missing answers


### **Question** :

Amaya's company surveyed a large group of people. After collecting and digitizing all of the data, she noticed that many participants didn't answer one particular question.

It was clear to Amaya that not disclosing the answer to the question was as important as the answer itself, so she wanted to consider this when preparing the data. The answer to the question was categorical.

Amaya is ready to build a machine learning model but must first deal with the missing answers.

**Which of the following should be the best strategy that Amaya should pursue?**


### **Choices** :

- Amaya should remove the column that contains the missing values.
- Amaya should add a new column to the dataset to flag rows with missing values and then replace those values with a reasonable answer.
- Amaya should replace the missing values with a reasonable answer.
- Amaya should predict the missing values using a separate machine learning model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Amaya knows she must do something with the missing answers before training a machine learning model, and [imputing](https://en.wikipedia.org/wiki/Imputation_(statistics)) these values seem like a good solution. For example, she could replace the missing values with their mean, median, or mode.

But this approach has a problem: Amaya thinks having many participants not disclose the answer to the question is as important as the answer itself. If she replaces the missing values, she will lose that information.

A great approach to avoid losing that information is to add a new binary column to the dataset. This column will flag every participant that didn't answer the question. Assuming avoiding to answer is not a random event, this new column will help with the predictive performance of Amaya's model.

Therefore, the best strategy for Amaya is to add this new column right before imputing missing values.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Handling Missing Values"](https://www.kaggle.com/dansbecker/handling-missing-values) is a Kaggle notebook that goes over a few strategies to handle missing values, including adding an extra column to flag these rows.- Check ["Imputation of missing values"](https://scikit-learn.org/stable/modules/impute.html#imputation-of-missing-values) for some information about how Scikit-Learn handles missing values.- ["How to Handle Missing Data with Python"](https://machinelearningmastery.com/handle-missing-data-python/) is an excellent post discussing different strategies in Python.</p></details>

-----------------------

## Date - 2023-09-11


## Title - Learning pace


### **Question** :

Lisa is working on a deep neural network for her latest project.

Unfortunately, during backpropagation, the gradient values of her network decrease dramatically as the process gets closer to the initial layers, preventing them from learning at the same pace as the last set of layers.

Lisa realizes her model suffers from the vanishing gradient problem. She decides to research every possible option to improve her model.

**Which of the following techniques will make Lisa's model more robust to the vanishing gradient problem?**


### **Choices** :

- Lisa should try ReLU as the activation function since it's well-known for mitigating the vanishing gradient problem.
- Lisa should modify the model architecture to introduce Batch Normalization.
- Lisa should make sure she is initializing the weights properly. For example, using He initialization should help with the vanishing gradient problem.
- Lisa should increase the learning rate to avoid getting stuck in local minima and thus reduce the chance of suffering vanishing gradients.


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>If the gradients of the loss function approach zero, the model will stop learning because the network will stop updating the weights. This phenomenon is known as the [Vanishing Gradient Problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and it's widespread when using the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) and [tanh](https://www.sciencedirect.com/topics/mathematics/hyperbolic-tangent-function) activation functions in deep neural networks. 

In contrast, [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) is a way to solve the vanishing gradient problem. ReLU is much less likely to saturate, and its derivative is `1` for values larger than zero.

[Batch normalization](https://en.wikipedia.org/wiki/Batch_normalization) is another way to mitigate the vanishing gradient problem. Suppose we have a layer that uses a sigmoid activation function. We can normalize the input to that layer to ensure that the values don't reach the edges and stay around the area where derivatives aren't too small. Modifying the input to this layer with batch normalization will mitigate the vanishing gradient problem.

Randomly initializing the weights of the deep network could also be problematic and lead to the vanishing gradient problem. If we use sigmoid or tanh as our activation functions, and many of the weights are initialized with values too small or too large, we will end up with derivatives close to zero. Using [He initialization](https://arxiv.org/abs/1704.08863) should prevent this from happening.

Finally, the vanishing gradient problem has nothing to do with the learning rate used to train the network.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["How to Fix the Vanishing Gradients Problem Using the ReLU"](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/) is a great explanation of how to approach this problem.* Check ["On weight initialization in deep neural networks"](https://arxiv.org/abs/1704.08863). It's an excellent paper covering weight initialization.</p></details>

-----------------------

## Date - 2023-09-12


## Title - Broken bottles


### **Question** :

Parker works at a drink factory concerned with classifying defects as bottles come out of the line. She built a computer vision model to classify bottles into three classes: "ready," "almost ready," and "waste."

Most of the bottles that come out are "ready," and there are only a few samples that classify as "almost ready" or "waste." Parker is very aware of this imbalance.

The factory uses Parker's model to reduce the number of defective bottles that ship to customers (any bottle that's not "ready" is defective.) They consider each of the three classes equally important and wants to ensure the evaluation process reflects that.

**Which metric should Parker use to evaluate her model?**


### **Choices** :

- The accuracy of the model.
- The Micro-average F1-Score of the model.
- The Macro-average F1-Score of the model.
- The Weighted F1-Score of the model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Since Parker is working on a very imbalanced problem, she shouldn't use accuracy to evaluate her model's quality. If 99% of the bottles are "ready," Parker's model will have 99% accuracy, even if she ignores every sample belonging to the other two classes.

The F1-Score is helpful in these situations, but it depends on how we compute it in a multi-class classification problem. 

The Micro-average F1-Score calculates the proportion of correctly classified samples out of all instances, the same as the model's accuracy definition. Therefore, if we use the Micro-average F1-Score, we'll have the same issue we see when using accuracy.

The Weighted F1-Score scales the individual F1-Score of each class. This method favors the majority classes, which in Parker's case are the bottles classified as "ready." The factory considers every category equally important, and that's why the Weighted F1-Score is not the correct answer.

Finally, the Macro-average F1-Score penalizes the model equally for any class that doesn't perform well, regardless of its importance or how many support samples it has. This metric is the correct answer where every category is important, irrespective of how many instances we have in the dataset. This is the metric that Parker should use to evaluate her model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.* ["Micro, Macro & Weighted Averages of F1 Score, Clearly Explained"](https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f) is a great article covering how to compute a global F1-Score metric in multi-class classification problem.* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.</p></details>

-----------------------

## Date - 2023-09-13


## Title - Job descriptions


### **Question** :

Millie finished collecting a dataset with job descriptions and their corresponding salary ranges. 

Unfortunately, there's a lot of noise in the dataset, and Millie is not sure how to proceed.

A colleague recommended bagging, but Millie is unfamiliar with this technique. She looked into it and came up with a few questions.

**Select every statement below that's correct about bagging:**


### **Choices** :

- Bagging is an effective technique to reduce the variance of a model.
- Bagging is an effective technique to reduce the bias of a model.
- Bagging trains a group of models, each using a subset of data selected randomly without replacement from the original dataset.
- Bagging trains a group of models, each using a subset of data selected randomly with replacement from the original dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Bagging is a popular ensemble learning technique.

Bagging trains a group of models in parallel and independently from each other. Each model uses a subset of the data randomly selected with replacement from the original dataset. That's why bagging is also known as "bootstrap aggregating:" because it draws bootstrap samples from the training dataset. 

Bagging is an excellent approach to reducing the variance of a model, but it doesn't help reduce the bias. That's why we often use it with low-bias models, like unpruned decision trees. Here is a relevant quote from ["What is bagging?"](https://www.ibm.com/cloud/learn/bagging) about how it helps reduce the variance of a model:

> Bagging can reduce the variance within a learning algorithm. This is particularly helpful with high-dimensional data, where missing values can lead to higher variance, making it more prone to overfitting and preventing accurate generalization to new datasets.

In summary, the first and fourth choices answer this question correctly.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check IBM's ["What's Bagging?"](https://www.ibm.com/cloud/learn/bagging) summary for a detailed explanation of this technique.* [What is the difference between Bagging and Boosting?](https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/) is a great summary of bagging and boosting and their advantages and disadvantages.</p></details>

-----------------------

## Date - 2023-09-14


## Title - The final exam


### **Question** :

Emersyn was running out of ideas to create new questions for the final exam.

In a last attempt to do something original, she copied an example code fragment from the Keras website and decided to ask a few questions about it:

```
model = keras.Sequential([
    keras.Input(shape=(28, 28, 1)),
    layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Flatten(),
    layers.Dropout(0.5),
    layers.Dense(10, activation="softmax"),
])
```

**Based on the code above, which of the questions Emersyn wrote are correct?**


### **Choices** :

- The kernel and pool size should have the same value. The model will likely fail to learn anything meaningful.
- The default activation function in `MaxPooling2D` layers is ReLU, which is why the code doesn't explicitly use it.
- The Dropout right before the output layer will cut down the number of learnable parameters from 1,600 to 800.
- The softmax activation function in the last layer hints that this is a multi-class classification model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The kernel size in convolutional layers and the pool size of the pooling layer don't need to have the same size. The code fragment is indeed part of a working example, and it's correctly configured.

[`MaxPooling2D`](https://keras.io/api/layers/pooling_layers/max_pooling2d/) layers don't use activation functions. That's the reason the code doesn't specify one. A max pooling operation calculates the maximum value in each patch of each feature map. Keras will throw an error if you try to set an activation function on the `MaxPooling2D` layer.

The Dropout right before the output layer doesn't reduce the number of learnable parameters in half. Instead, the Dropout will set the value of 50% of the neurons to 0.

Finally, the [softmax](https://en.wikipedia.org/wiki/Softmax_function) activation function and the size of the output layer point to a multi-class classification problem. Remember that softmax converts a vector of numbers into a vector of probabilities, which we need in multi-class classification tasks.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For more information about convolutional layers, check ["How Do Convolutional Layers Work in Deep Learning Neural Networks?"](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/).* Check ["A Gentle Introduction to Pooling Layers for Convolutional Neural Networks"](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/) for more information about how max pooling works.* ["A Gentle Introduction to Dropout for Regularizing Deep Neural Networks"](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) is an excellent introduction to Dropout.* ["Simple MNIST convnet"](https://keras.io/examples/vision/mnist_convnet/) is a Keras example showing this particular code fragment.</p></details>

-----------------------

## Date - 2023-09-15


## Title - Architecture outlier


### **Question** :

Briella is studying Deep Learning models and comes across a list of popular architectures. However, she notices that one of the items on the list is not a Deep Learning architecture.

**Can you help Briella identify the one that doesn't belong on this list?**


### **Choices** :

- ResNet
- LSTM
- CIFAR-10
- DenseNet


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>CIFAR-10 is not a deep learning architecture. Instead, it is a widely used dataset for image classification tasks. On the other hand, ResNet, LSTM, and DenseNet are all popular Deep Learning architectures. 

ResNet is mainly used for image classification tasks, LSTM for sequence-to-sequence problems, and DenseNet for various image-related tasks.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) for more information about the dataset.</p></details>

-----------------------

## Date - 2023-09-16


## Title - True Positives


### **Question** :

Here is the picture of a confusion matrix we generated after evaluating a model in a dataset with 100 samples:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186962046-7c076163-5b58-442f-9641-c09f5cf71003.jpg)

Assume that class B represents the outcomes of the model we are interested in finding.

**What's the total of True Positives on this evaluation round?**


### **Choices** :

- True Positives are class A samples the model predicted as class B, so the answer is 7.
- True Positives are class B samples the model predicted as class A, so the answer is 13.
- True Positives are class A samples the model predicted as class A, so the answer is 52.
- True Positives are class B samples the model predicted as class B, so the answer is 28.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We are interested in samples from class  B, which means we will treat class B as our "Positive" samples and class A as our "Negative" samples.

If we replace the classes in the confusion matrix with Positive and Negative instead of "B" and "A," it's much easier to reason about the model's number of True Positives:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/187207950-77e4eccd-0b63-43bc-8f07-be19f690d725.jpg)

True Positives are those samples that we expect to be Positive (class B), and the model predicted as Positive (class B.) Therefore, the correct answer to the question is 28. You can see every combination in the image above.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2023-09-17


## Title - Forest fires


### **Question** :

Lily, an AI researcher at a prominent environmental organization, is working on a project to detect whether a satellite image contains a forest fire.

The project will have a significant impact, and the team has accumulated many images to train the model. They plan to use deep learning to build a binary classifier. The main decision now is about selecting the appropriate activation function for the final layer of the network.

**Which activation functions could be a good candidate for the output layer?**


### **Choices** :

- Rectifier Linear Unit (ReLU)
- Sigmoid
- Tanh
- Softmax


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Lily's team is building a binary classifier. The goal is to design the output layer to produce an easily interpreted result.

ReLU is a function that returns its input if it's positive or zero otherwise. While it is beneficial in the hidden layers of a neural network, it does not seem suitable for the final layer in Lily's model. If the penultimate layer has captured all the necessary information for prediction, applying ReLU to the output layer would leave any positive values untouched and turn negative values into zero. This isn't ideal because we can't use the magnitude of these values to differentiate between inputs.

On the other hand, the [Sigmoid function](https://en.wikipedia.org/wiki/Logistic_function) can convert its input into a range from 0 to 1. This characteristic makes it a suitable choice for binary classification models. By setting a threshold, we can interpret the output of the model. For instance, anything equal to or under `0.5` could represent the absence of a forest fire, and anything above `0.5` could indicate its presence.

The [Tanh function](https://en.wikipedia.org/wiki/Hyperbolic_functions#Hyperbolic_tangent) is another activation function that maps its inputs to a value between -1 and 1. While it is a good choice for hidden layers in some cases, it is unsuitable for binary classification problems where we need an output from 0 to 1.

Finally, the [Softmax function](https://en.wikipedia.org/wiki/Softmax_function) can turn a vector of numbers into a vector of probabilities. In the context of the binary classifier that Lily's team is working on, Softmax could output a vector with two values: the first representing the absence of a forest fire and the second representing its presence. The team could then select the larger of these two values to make the final prediction.

It's worth noting that Softmax is essentially a generalization of the sigmoid function for multi-class cases. However, Softmax reduces to sigmoid when used in a binary classification context, resulting in the same outcome.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["A Gentle Introduction To Sigmoid Function"](https://machinelearningmastery.com/a-gentle-introduction-to-sigmoid-function/) for a quick introduction to Sigmoid.* ["The Differences between Sigmoid and Softmax Activation Functions"](https://medium.com/arteos-ai/the-differences-between-sigmoid-and-softmax-activation-function-12adee8cf322) is a short comparison between these two functions.</p></details>

-----------------------

## Date - 2023-09-18


## Title - Policing crime


### **Question** :

Brielle wants to build a machine learning model that will use traffic violations to predict how to distribute the city's police force.

She wants the model to predict the areas where new violations are likely to occur so the department can reinforce the security around those streets.

**Which of the following is a potential problem that Brielle should consider?**


### **Choices** :

- There won't be any reliable way to evaluate this model.
- The model may suffer from survivorship bias.
- The model may suffer from decline bias.
- The model may create a positive feedback loop.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Evaluating this model doesn't need to be complicated. Assuming that Brielle uses a Supervised Learning approach, she will have several options to assess the quality of the model predictions. Therefore, the first choice is incorrect.

[Survivorship bias](https://en.wikipedia.org/wiki/Survivorship_bias) is when we concentrate on samples that made it past a selection process and ignore those that did not. Nothing in the problem statement indicates that Brielle's model will suffer from this problem.

[Decline bias](https://en.wikipedia.org/wiki/Declinism) refers to the tendency to compare the past to the present, leading to the assumption that things are worse or becoming worse simply because change is occurring. The third choice is not a correct answer either.

Finally, this model may create a [positive feedback loop](https://en.wikipedia.org/wiki/Positive_feedback). The more you patrol a neighborhood, the more traffic violations you'll find. Communities with no police force will never report any violations, while heavily patrolled communities will have the lion's share of transgressions.

The model will use that data and make the problem worse: it will predict that new violations will happen in already problematic areas, sending more police to those communities at the expense of areas with lower reports. A few rounds of this, and you'll have most reports from a few places while violations are rampant everywhere.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check the description of a ["Positive Feedback Loop"](https://en.wikipedia.org/wiki/Positive_feedback) in Wikipedia.* ["How Positive Feedback Loops Are Hurting AI Applications"](https://levelup.gitconnected.com/how-positive-feedback-loops-are-hurting-ai-applications-6eae0304521c) is an excellent article explaining the dangers of positive feedback loops in machine learning.</p></details>

-----------------------

## Date - 2023-09-19


## Title - Learnable parameters


### **Question** :

Arianna is trying to learn how Convolutional Neural Networks work, so she decided to copy an online Keras example to start from somewhere.

Here is the core of the code she put together:

```
model = keras.Sequential([
    keras.Input(shape=(28, 28, 1)),
    layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Flatten(),
    layers.Dropout(0.5),
    layers.Dense(10, activation="softmax"),
])
```

**Based on the above code fragment, what are the correct statements regarding each layer's parameters (weights and biases)?**


### **Choices** :

- The first convolutional layer has a total of 21,632 parameters.
- The first max pooling layer has a total of 5,408 parameters.
- The second convolutional layer has a total of 18,496 parameters.
- The fully-connected layer has a total of 1,600 parameters.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We can compute the number of parameters of a convolutional layer using the following formula:

```
parameters = k * (f * h * w + 1)
```

Where `k` corresponds to the number of output filters from this layer, `f` corresponds to the number of filters coming from the previous layer, `h` corresponds to the kernel height and `w` to the kernel width. The value `1` corresponds to the bias parameter related to each filter. Here is the complete calculation for the first convolutional layer:

```
parameters = k * (f * h * w + 1)
parameters = 32 * (1 * 3 * 3 + 1) = 320
```

Max pooling layers don't have any parameters because they don't learn anything. The input to the first max pooling layer is `26x26x32`, but the layer doesn't have any weights or biases associated with it.

The second convolutional layer does have 18,496 parameters. Let's check:

```
parameters = k * (f * h * w + 1)
parameters = 64 * (32 * 3 * 3 + 1) = 18,496
```

Finally, the fully-connected layer has 1,600 parameters. To compute this, we need to calculate the size of each layer to understand the input to the fully-connected layer:
* Input: `28x28x1 = 784`
* Conv2D: `26x26x32 = 21,632`
* MaxPool2D: `13x13x32 = 5,408`
* Conv2D: `11x11x64 = 7,744`
* MaxPool2D: `5x5x64 = 1,600`

Notice that the above values differ from the number of learnable parameters of each layer, but they are essential to understanding the size of the input to the fully-connected layer.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Understanding and Calculating the number of Parameters in Convolution Neural Networks (CNNs)"](https://towardsdatascience.com/understanding-and-calculating-the-number-of-parameters-in-convolution-neural-networks-cnns-fc88790d530d) for instructions on how to compute the number of learnable parameters.* ["Simple MNIST convnet"](https://keras.io/examples/vision/mnist_convnet/) is a Keras example showing this particular code fragment.</p></details>

-----------------------

## Date - 2023-09-20


## Title - Augmenting the dataset


### **Question** :

Esther had an excellent model already, but she had the budget to experiment a bit more and improve its results.

She was building a deep network to classify pictures. From the beginning, her Achilles' heel has been the size of her dataset. One of her teammates recommended she use a few data augmentation techniques.

Esther was all-in. Although she wasn't sure about the advantages of data augmentation, she was willing to do some research and start using it.

**Which of the following statements about data augmentation are true?**


### **Choices** :

- Esther can use data augmentation to expand her training dataset and assist her model in extracting and learning features regardless of their position, size, rotation, etc.
- Esther can use data augmentation to expand the test dataset, have the model predict the original image plus each copy, and return an ensemble of those predictions.
- Esther will benefit from the ability of data augmentation to act as a regularizer and help reduce overfitting.
- Esther has to be careful because data augmentation will reduce the ability of her model to generalize to unseen images.


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>One significant advantage of data augmentation is its ability to make a model resilient to variations in the data. For example, assuming we are working with images, we can use data augmentation to generate synthetic copies of each picture and help the model learn features regardless of where and how they appear. 

A few popular augmentation techniques when working with images are small rotations, horizontal and vertical flipping, turning the picture to grayscale, or cropping the image at different scales. The [following example](https://www.v7labs.com/blog/data-augmentation-guide) shows four versions of an image generated by changing the original picture's brightness, contrast, saturation, and hue:

![Data augmentation](https://user-images.githubusercontent.com/1126730/179006718-fa88e435-0347-4741-a6e7-6b82266316b3.png)

Data augmentation is also helpful during testing time: Test-Time Augmentation is a technique where we augment samples before running them through a model, then average the prediction results. Test-Time Augmentation often results in better predictive performance.

Instead of predicting an individual sample from the test set, we can augment it and run each copy through the model. Esther is working on a classification problem, so her model will output a softmax vector for each sample. She can then average all these vectors and use the result to choose the correct class representing the original sample.

Using data augmentation, Esther can reduce overfitting and help her model perform better on unseen data. Data augmentation has a regularization effect. Increasing the training data through data augmentation decreases the model's variance and, in turn, increases the model's generalization ability. Therefore, the third choice is correct, but the fourth one is not.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["The Essential Guide to Data Augmentation in Deep Learning"](https://www.v7labs.com/blog/data-augmentation-guide) is an excellent article discussing data augmentation in detail.* Check ["Test-Time augmentation"](https://articles.bnomial.com/test-time-augmentation) for an introduction that will help you make better predictions with your machine learning model.</p></details>

-----------------------

## Date - 2023-09-21


## Title - Grocery fruits


### **Question** :

Serena has been given an intriguing project at her workplace. She has to design an object detection model that identifies different types of fruits in images taken from a grocery store.

Her goal is to create a model that can be easily modified and used for other similar tasks in the future.

To ensure that her model performs at its best, Serena needs a method to evaluate it effectively. This evaluation method should allow her to compare and choose the best among different versions of the model.

**Which evaluation metrics should Serena use to evaluate her model?**


### **Choices** :

- F1 score
- Mean Average Precision (mAP)
- ROC Curve
- Precision-Recall Curve


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The recall is useful for object detection, but it can't provide the full picture unless combined with Precision. High recall and low precision could lead to a model that is not useful. Therefore, Serena cannot rely solely on Recall as her key evaluation metric. 

[ROC Curves](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) are not typically used for object detection tasks, as there's no real concept of True Negatives, which are required to compute the False Positive Rate, one of the axes of the ROC curve. In object detection tasks, the number of bounding boxes that do not contain an object of interest is generally too large to handle effectively.

Instead, Serena could compute a [Precision-Recall Curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html). This curve is similar to the ROC curve but uses the model's precision instead of False Positive Rate, thereby avoiding the problem of True Negatives.

[Mean Average Precision (mAP)](https://medium.com/@timothycarlen/understanding-the-map-evaluation-metric-for-object-detection-a07fe6962cf3) is commonly used in object detection tasks to evaluate the overall performance of a model across all classes. It considers precision and recall and averages them over different Intersection over Union (IoU) thresholds, providing a single scalar value that Serena can use to compare different models.

Lastly, the [F1-score](https://en.wikipedia.org/wiki/F-score) is a good choice, as it considers both the precision and recall of the model, offering a balanced view of the model's performance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Classification: ROC Curve and AUC"](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) for an explanation of how to create and interpret a ROC curve.* For more information about Precision-Recall curves, check [Scikit-Learn's documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html).</p></details>

-----------------------

## Date - 2023-09-22


## Title - Bagging or Boosting?


### **Question** :

Katherine wants to use an ensemble model to process her dataset. 

There's only one question for her to answer: Should she use bagging or boosting?

Both techniques have different advantages and disadvantages, and Katherina wants to ensure she evaluates them correctly before committing to one solution.

**Which of the following statements are true about bagging and boosting?**


### **Choices** :

- Bagging trains individual models sequentially, using the results from the previous model to inform the selection of training samples.
- Boosting trains individual models sequentially, using the results from the previous model to inform the selection of training samples.
- Bagging trains a group of models, each using a subset of data selected randomly with replacement from the original dataset.
- Each model receives equal weight in bagging to compute the final prediction, while boosting uses some way of weighing each model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Ensembling is where we combine a group of models to produce a new model that yields better results than any initial individual models. Bagging and boosting are two popular ensemble techniques.

Bagging trains a group of models in parallel and independently from each other. Each model uses a subset of the data randomly selected with replacement from the original dataset. In contrast, Boosting trains a group of learners sequentially, using the results from each model to inform which samples to use to train the next model.

This summary helps us conclude that the first choice is incorrect, but the second and third choices are correct.

Finally, when computing the final prediction, bagging averages out the results of each model. Boosting, however, weights each model depending on its performance. Therefore, the fourth choice is also correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check [Bagging vs. Boosting in Machine Learning: Difference Between Bagging and Boosting](https://www.upgrad.com/blog/bagging-vs-boosting/) for a detailed comparison between both techniques.* [What is the difference between Bagging and Boosting?](https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/) is another great summary of both techniques and their advantages and disadvantages.</p></details>

-----------------------

## Date - 2023-09-23


## Title - Pooling layers


### **Question** :

Sienna realized she needed more than convolutional layers to process her image dataset.

After stacking a few convolutional layers, her model started to make progress. Unfortunately, only very similar images returned positive results. Sienna discovered that her model lacked translation invariance: it was paying too much attention to the precise location of every feature.

Fortunately, Sienna found out that she could use pooling layers.

**Which of the following statements about pooling layers are correct?**


### **Choices** :

- During the training process, the network will learn the best configuration for the pooling layer.
- A pooling layer with a stride of 2 will cut the number of feature maps from the previous convolutional layer in half.
- Pooling layers create the same number of pooled feature maps.
- Average pooling and max pooling are two of the most common pooling operations.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Pooling layers don't have any learnable parameters. When designing the model, Sienna must specify the pooling operation and configuration she wants to use. 

Pooling layers work on each feature map independently and, depending on the pool size and stride, downsample these feature maps. The result is always a new set of pooled feature maps. Therefore, the second choice is incorrect, but the third is correct.

Finally, Max Pooling and Average Pooling are the two most common pooling operations. Average Pooling computes the average value of each patch, while Max Pooling calculates the maximum value.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Max Pooling in Convolutional Neural Network and Its Features"](https://analyticsindiamag.com/max-pooling-in-convolutional-neural-network-and-its-features/) is a great introduction to Max Pooling.* Check ["A Gentle Introduction to Pooling Layers for Convolutional Neural Networks"](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/) for more information about how pooling layers work.</p></details>

-----------------------

## Date - 2023-09-24


## Title - The output of the network


### **Question** :

Juniper learned that designing a neural network architecture for a supervised classification problem wasn't hard.

Although most of it needed experimentation, one thing Juniper could count on was the design of the output layer of the network.

**Which of the following is a correct statement about the neurons in the output layer of a classification network?**


### **Choices** :

- The number of neurons in the output layer should always match the number of classes.
- The number of neurons in the output layer doesn't necessarily need to match the number of classes.
- The number of neurons in the output layer should always be greater than one.
- The number of neurons in the output layer should always be a multiple of 2.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When working on a multi-class classification problem, setting the output layer to the same number of classes we are interested in predicting is common. But what happens when we are working on a binary classification problem?

A network to solve a binary classification problem doesn't need an output with two neurons. Instead, we can use a single neuron to determine the class by deciding on a cutoff threshold. For example, the result could be positive if the output exceeds 0.5 and negative otherwise.

That means we can solve a problem requiring two classes with a single neuron.

We can stretch the same idea to multi-class classification problems: We could interpret the output layer as a binary result, allowing us to represent multiple classes with fewer neurons. For example, we would need only two neurons to classify instances into four different categories (`00`, `01`, `10`, `11`.) This approach, although not popular, it's possible.

Therefore, the second choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["But what is a neural network?"](https://www.youtube.com/watch?v=aircAruvnKk) is Grant Sanderson's introduction to neural networks on YouTube. Highly recommended!* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html) is a free online book written by [Michael Nielsen](https://twitter.com/michael_nielsen).</p></details>

-----------------------

## Date - 2023-09-25


## Title - Studying decision trees


### **Question** :

Kehlani is one of many software developers that decided to start learning machine learning.

Following the recommendation of her peers, the first algorithm she looked into was decision trees. She found a lot of resemblance with some of the techniques she already knew.

After a few weeks, Kehlani wants to summarize what she learned in an email to her team, but first, she wants you to review it.

**These are the advantages that Kehlani listed. Which of them would you say are actual advantages of decision trees?**


### **Choices** :

- Decision trees are simple to understand and interpret, and we can visualize them.
- Unlike other algorithms, many implementations of decision trees work with missing values and categorical data.
- Decision trees always generalize well and are resistant to overfitting.
- Decision trees require little data preparation compared to other algorithms. For example, they don't need the scaling of data.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Decision Trees](https://en.wikipedia.org/wiki/Decision_tree_learning) are an excellent starting point for developers that want to learn machine learning.

One of the reasons decision trees are popular is because they are easier to understand and interpret than other algorithms. You can explain the output of a tree by boolean logic, which makes them transparent compared to algorithms we can't explain. Kehlani did an excellent job by including this advantage in her email.

Many different implementations of decision trees are very flexible with the data they can process. For example, some implementations can handle missing values and work directly with categorical data. Most machine learning algorithms don't have this luxury and require a much more extensive pre-processing step before training a model. 

Something similar happens with having to scale the data. Decision trees can handle features with different scales. For example, it can take a column representing a person's age with values between 0 and 100 and another with a salary ranging from 20,000 to 500,000. Not every algorithm has this flexibility. Neural networks, for example, struggle when features don't have the same approximate scale.

Decision trees, unfortunately, are prone to overfitting if we don't take careful care of their depth. In other words, unless we ensure our tree doesn't go too deep, it will tend to fit noisy samples and output the wrong prediction. The only example where Kehlani made a mistake is the third option. 

Remember that while a single decision tree is prone to overfitting, using an ensemble of trees is more resistant. Here is a quote from "[To Boost or not to Boost: On the Limits of Boosted Neural Networks](https://arxiv.org/pdf/2107.13600.pdf)": "[these experiments] confirm that training single large decision trees is prone to overfitting while boosted ensembles of decision trees are resistant to overfitting."</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The Scikit-Learn's ["Decision Trees"](https://scikit-learn.org/stable/modules/tree.html) page contains an extensive list of advantages and disadvantages of decision trees.* Check ["To Boost or not to Boost: On the Limits of Boosted Neural Networks"](https://arxiv.org/pdf/2107.13600.pdf), the paper cited above comparing the overfitting tendency of a single Decision Tree versus an ensemble of trees.* ["Decision tree pruning"](https://en.wikipedia.org/wiki/Decision_tree_pruning) covers the process of pruning a tree to reduce overfitting.</p></details>

-----------------------

## Date - 2023-09-26


## Title - Defective chips


### **Question** :

A company is using autoencoders to detect anomalies with the chips they produce.

They trained their autoencoder on millions of pictures of working chips to generate a compressed representation of the common characteristics of a chip in good condition.

They run every new picture in production through the autoencoder to determine whether there's a problem with that particular chip.

**Which of the following is the company's process to determine whether the chip is in good working condition?**


### **Choices** :

- The autoencoder's output is a softmax vector that classifies the picture into two classes. If the largest value in the vector corresponds to the anomaly class, the company knows this is a defective chip.
- Autoencoders work like a binary classification model, where any output value greater than a predefined threshold indicates the input is an anomaly.
- The company computes the error between the model's original input and output. The picture represents a defective chip if the error exceeds a predefined threshold.
- The autoencoder's output is a value indicating how different the input image is from a working chip, so the largest this value is, the more likely it's for the input to be a defective chip.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Autoencoders](https://essays.bnomial.com/autoencoders) are learning algorithms that can help with anomaly detection problems. 

An autoencoder is a neural network that we can split into three sections: an encoder, a bottleneck, and a decoder. The encoder compresses the original input into an intermediate representation, and the decoder reverses the process to reconstruct the original data. The bottleneck sits between the encoder and decoder and is the section that stores the compressed representation of the data. 

In this particular problem, we can train an autoencoder by showing it images of working chips and expecting the model to reproduce the same input image. In other words, the autoencoder's input and expected output are the same. If we compute the similarity between the input and output images, we can determine whether the original picture belongs to a working or defective chip.

A defective chip will have specific visual characteristics that will be impossible for the autoencoder to reproduce. Remember that the autoencoder learned on a dataset of working samples, so it won't have any notion of features that aren't common among chips in good status. 

We should expect a reproduction error larger than normal whenever we input a defective chip into the autoencoder. Therefore, the third option is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Autoencoders"](https://articles.bnomial.com/autoencoders) for an introduction to a learning technique to represent data efficiently using neural networks.</p></details>

-----------------------

## Date - 2023-09-27


## Title - Data augmentation on YouTube


### **Question** :

Amara is writing a script for a YouTube video about data augmentation.

She wants to cover some of the most critical aspects of using the technique on a dataset of pictures.

Here is Amara's list with the key takeaways she wants to leave for her audience.

**Which of the following statements would you let Amara share with her audience?**


### **Choices** :

- Using data augmentation, we can artificially increase the amount of data by generating new samples from existing data.
- Generative Adversarial Networks (GANs) and Style Transfer are advanced techniques that can help with data augmentation.
- Data augmentation has a regularization effect when used to increase the amount of data before training a model.
- Data augmentation always eliminates biases present in the original data.


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The first choice is the definition of data augmentation: we can augment the size of the dataset by generating new samples from our existing data.

Generative Adversarial Networks (GANs) are a popular way to generate synthetic images. [We can also use Style Transfer](https://arxiv.org/abs/1909.01056) to create new data based on existing samples.

The third choice is also correct: Data augmentation has a regularization effect. Increasing the training data through data augmentation decreases the model's variance and, in turn, increases the model's generalization ability.

Although data augmentation can be strategically used to remove certain biases, it may propagate biases already present in the original data. Remember that data augmentation uses existing data as the foundation for any new data, so any problems with the original dataset will persist on the augmented one.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["STaDA: Style Transfer as Data Augmentation"](https://arxiv.org/abs/1909.01056) is a paper illustrating how to use Style Transfer for data augmentation.* ["The Essential Guide to Data Augmentation in Deep Learning"](https://www.v7labs.com/blog/data-augmentation-guide) is an excellent article discussing data augmentation in detail.* Check ["Test-Time augmentation"](https://articles.bnomial.com/test-time-augmentation) for an introduction that will help you make better predictions with your machine learning model.</p></details>

-----------------------

## Date - 2023-09-28


## Title - Six months of data


### **Question** :

Valerie's company gave her access to their entire dataset from day one.

She used six consecutive months of data: the first five months to train the model and the sixth to test it.

After trying multiple times, Valerie's model performance on the training data had a large gap with the performance on the test data. The model works well with the training dataset but struggles to keep up with the test data.

**Which of the following could be valid reasons for this problem?**


### **Choices** :

- Valerie's optimizer is incorrect for this particular problem.
- Valerie is not using the correct loss function for this particular problem.
- Valerie's model doesn't have enough complexity to capture the relevant signal from the data.
- The training data does not come from the same distribution as the test data.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Valerie is training her model on five consecutive months' worth of data and testing it with another month worth of data. If something changed during that time, Valerie might have a test set that fundamentally differs from her training data.

This problem is common, and a way to approach it is to ensure the training and test data come from the same distribution. ["Adversarial validation"](https://articles.bnomial.com/adversarial-validation) is a clever technique you can use to accomplish this.

None of the other three options explain a model that works with the training data but struggles with the test dataset.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Adversarial validation"](https://articles.bnomial.com/adversarial-validation) for a quick introduction to this technique.</p></details>

-----------------------

## Date - 2023-09-29


## Title - Nodes and connections


### **Question** :

June has just started learning about multilayer perceptrons: inputs, outputs, layers, and nodes.

It's a lot for a single day of study, but June doesn't want to go back home without understanding some core ideas.

She devised a simple rule: assuming she had two networks, one with more nodes than the other, this network must also have more connections. 

**What do you think about June's rule?**


### **Choices** :

- June is correct: the network with more nodes will always have more connections than the one with fewer nodes.
- June is incorrect: the network with fewer nodes will always have more connections than the one with more nodes.
- June is incorrect: every multilayer perceptron has the same number of connections.
- June is incorrect: either network may have more connections than the other, regardless of the number of nodes.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>June's rule is incorrect.

Imagine a network with three layers:
* The first layer with 64 nodes
* The second layer with 8
* The third layer with 64 nodes

This network has a total of 136 nodes and 1,024 connections.

The second network will have only two layers:
* The first layer with 64 nodes
* The second layer with 64 nodes also

This network has a total of 128 nodes and 4,096 connections.

Despite the first network having more nodes than the second one (136 versus 128,) the second network has more connections (4,096 versus 1,024.)

Therefore, the correct answer to this question is the fourth choice.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["But what is a neural network?"](https://www.youtube.com/watch?v=aircAruvnKk) is Grant Sanderson's introduction to neural networks on YouTube. Highly recommended!* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html) is a free online book written by [Michael Nielsen](https://twitter.com/michael_nielsen).</p></details>

-----------------------

## Date - 2023-09-30


## Title - Incomplete fields


### **Question** :

Samantha is a data scientist at a leading healthcare organization. 

They've recently received a massive dataset from a medical study, which holds crucial information about patients' vitals. However, she discovered that many values were missing, entire fields were incomplete, and some data was mishandled.

Samantha knows well-structured data is vital for building a successful machine learning model. She decided to take on these issues first before further processing.

During a team meeting, Samantha discusses potential methodologies to address the concerns with the dataset. 

**Which of the following are valid techniques Samantha and her team could use to handle the problems with their data?**


### **Choices** :

- Replace missing values with the mean, median, mode, or other imputation techniques.
- Use a machine learning algorithm that is robust to missing values.
- Predict the missing values using a separate Machine Learning model.
- Drop any rows or columns that contain missing or corrupted data.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Replacing missing values with the mean, median, mode, or other [imputation](https://en.wikipedia.org/wiki/Imputation_(statistics)) techniques is an excellent approach to handling this problem. Remember that, for imputation to work, we need to apply it to those features where most of the data is in good shape and only a few values are missing. If the dataset contains features with that characteristic, imputation will help. 

Sometimes, a row or column is in such poor shape that the best approach is to drop it altogether. Imagine a feature where only a small percentage of rows have values, or most of the data is corrupted. In those cases, getting rid of the data is the appropriate approach. 

A more advanced technique commonly used is to predict missing values using a separate model. For example, you could use a linear regression model to fill in the blanks on one specific column of data. This approach considers the correlation between other features and the column with missing values, potentially producing good results. 

Finally, we can use a particular implementation of a machine learning algorithm that is robust to missing values. For example, we could run the [k-Nearest Neighbors algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) and ignore a column with missing values when computing the distance. However, this depends on the algorithm's implementation, so you must watch out for that.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Handling Missing Values"](https://www.kaggle.com/dansbecker/handling-missing-values) is a practical tutorial on how to handle missing values.</p></details>

-----------------------

## Date - 2023-10-01


## Title - High training


### **Question** :

There's not a lot of context for you other than the following chart showing the training loss of a machine learning model:

![Training Loss Chart](https://user-images.githubusercontent.com/1126730/188470675-c76e29e3-e68c-4eaa-8b05-879bed678b2f.jpg)

As you can see, after finishing training, the loss stays too high.

**What's a reasonable conclusion about this machine learning model?**


### **Choices** :

- The model is overfitting.
- The model is underfitting.
- The model is neither overfitting nor underfitting.
- The model is either overfitting or underfitting, but we can't say for sure.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A good model should capture valuable patterns in the data and discard any noise that doesn't help with predictions. An overfitting model will fit that noise. An underfitting model will not capture the relevant patterns in the dataset. 

An overfitting model should not have any problems with the training data, so we should expect a low training loss. An underfitting model should struggle with the training data, so its training loss will be high.

This model shows a high training loss, which we expect for an underfitting model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.</p></details>

-----------------------

## Date - 2023-10-02


## Title - Useless explanation


### **Question** :

"Remove every ordinal feature. Keep the nominal ones."

Those were the instructions that Brad sent Angelina. "I hate him so much," she couldn't stop thinking.

Angelina's dataset had a lot of columns, and she couldn't fit it all into memory. Removing some of the columns was an excellent place to start, and since Brad had solved the problem already, she asked him.

"But of course" —she kept thinking— "he couldn't have been more useless."

Angelina doesn't know the difference between ordinal and nominal columns. 

**Which of the following is the way forward for Angelina?**


### **Choices** :

- Angelina should remove every categorical feature whose values have a meaningful order.
- Angelina should remove every categorical feature whose values don't have a meaningful order.
- Angelina should remove every numerical feature but leave categorical features.
- Angelina should remove every categorical feature but leave numerical features.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Both ordinal and nominal features refer to categorical columns. 

Ordinal columns have a meaningful order. For example, you could have a feature representing a person's economic status with three possible values: "low," "medium," and "high." Notice how there's a clear order among these values: "low" comes first, then "medium," and finally "high." 

Compare this with a nominal variable that doesn't have a meaningful order between values. For example, a feature representing a "color" will not have any discernible order.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Nominal, ordinal, or numerical variables?"](https://s4be.cochrane.org/blog/2015/07/24/nominal-ordinal-numerical-variables/) to understand the difference between different types of features.</p></details>

-----------------------

## Date - 2023-10-03


## Title - Date of sale


### **Question** :

When Kylie started working for Nike, she didn't believe her first project was at the core of their sales process.

She found a team working on a machine learning model for about a year with mediocre results. After a couple of weeks, Kylie proposed to do some feature engineering around a feature representing the sale date to help the model improve its predictions.

We don't know exactly what the model predicts.

**Which of the following are examples of feature engineering techniques that Kylie could do to improve the model?**


### **Choices** :

- Replacing the feature representing the date of the sale with three separate columns for the year, month, and day.
- Replacing the feature representing the date of the sale with a single value containing the number of seconds since the year started.
- Replacing the feature representing the date of the sale with a single value that contains the number of the month.
- Keeping the date feature untouched and adding a new column representing the number of samples sold during the same month.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We don't know which of these options will be the most useful, but every one of them is an example of feature engineering that could help the model make better predictions.

The first three options are examples of how we can derive new features from a column in our dataset. In this case, Kylie can turn a date field into three components or simplify it by replacing it with a single value that keeps the necessary information for the model. Notice that, in these three cases, Kylie is not introducing anything new. Instead, she is transforming the data so the model can use it.

The fourth option is an example of frequency encoding, where Kylie counts how many products have been sold every month and creates a meta-feature with this information.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.* [_Feature Engineering for Machine Learning_](https://amzn.to/3SsnLAc) is an excellent book covering feature engineering.</p></details>

-----------------------

## Date - 2023-10-04


## Title - Two experiments. Round one.


### **Question** :

Lena ran two experiments training a neural network on a sample dataset. Her goal is to understand how different batch sizes affect the training process.

One of the experiments used a very small batch size, and the other used a batch size equal to all existing training data.

After training and evaluating her models, Lena plotted the training and testing losses over 100 epochs of one of the experiments, and this is what she got:

![Learning Curves](https://user-images.githubusercontent.com/1126730/191095007-c31eddb5-e4b6-481c-b529-9064cf7dda20.jpg)

**Which of the following options is the most likely to be true?**


### **Choices** :

- The model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took less time to train.
- The model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took longer to train.
- The model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took less time to train.
- The model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took longer to train.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The amount of noise in the plot is crucial to answering this question.

The lower the batch size, the more noise we will get in the model's loss. When we only use a few samples, any instances that vary dramatically will cause the loss to swing wildly. When we use a larger batch, no individual sample would have the power to sway the loss too much, so we should expect less noise.

This plot shows a very smooth loss with almost no noise, which is likely the experiment that uses the entire training set during every epoch.

The smaller the batch size, the more times we will need to compute the loss and update the model's weights during backpropagation. For example, if Lena's training set has 800 samples and uses one as her batch size, the algorithm will update the model's weights 800 times. However, if she uses a batch size equal to the size of her training set, the algorithm will update the model's weights 1 time.

Updating the model during every iteration is computationally expensive, so using a larger batch size will usually take less time. Therefore, the third option is probably the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["The wrong batch size is all it takes" ](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) explains how different batch sizes influence the training process of neural networks using gradient descent.* Check ["An overview of gradient descent optimization algorithms"](https://www.ruder.io/optimizing-gradient-descent/) for a deep dive into gradient descent and every one of its variants.</p></details>

-----------------------

## Date - 2023-10-05


## Title - Substantial structure


### **Question** :

Rachel, a data science enthusiast, recently took up a project that deals with a substantial amount of structured data. She's keen to apply the various data preprocessing techniques she learned during her recent machine learning course.

She is asked to transform all the nominal features into a one-hot encoded vector in her project.

There's a catch, though: Rachel has forgotten the precise definition of "nominal features."

**Which of the following descriptions accurately describes a nominal feature?**


### **Choices** :

- A nominal feature is a categorical variable with more than ten possible values.
- A nominal feature is a categorical variable without a meaningful order.
- A nominal feature is a categorical variable with an established order.
- A nominal feature is a categorical variable with fewer than ten potential values.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>As stated by Jason Brownlee in his ["Ordinal and One-Hot Encodings for Categorical Data"](https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/) article:

> [A nominal variable is a] variable that comprises a finite set of discrete values with no relationship between values.

This means that nominal features are categorical, where the order of values carries no significance. For instance, if we have a feature named "Vehicle Type" with possible values: "Car," "Bus," and "Bike," there is no meaningful order among these values. 

The quantity of potential values does not determine whether a feature is nominal. Also, nominal features do not have an inherent order among their values.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Ordinal and One-Hot Encodings for Categorical Data"](https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/) for an explanation of nominal features.</p></details>

-----------------------

## Date - 2023-10-06


## Title - A clever loss function


### **Question** :

Adalynn is using a clever approach with her deep learning model.

During training, Adalynn runs three samples simultaneously through her model:
* An anchor image
* A matching picture representing the same entity as the anchor
* A non-matching picture representing a different entity as the anchor

The loss function she uses takes these three inputs and minimizes the distance between the anchor and the matching sample while maximizing the distance with the non-matching picture.

But Adalynn didn't invent this function.

**Which of the following is the name of the loss function that Adalynn is using?**


### **Choices** :

- Adalynn is using a Contrastive loss function
- Adalynn is using a Binary Cross-entropy loss function
- Adalynn is using a Categorical Cross-entropy loss function
- Adalynn is using a Triplet loss function


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Adalynn is using [Triplet loss](https://en.wikipedia.org/wiki/Triplet_loss), helpful in [similarity learning problems](https://en.wikipedia.org/wiki/Similarity_learning) where the goal is to learn a function that measures how similar two objects are.

The Triplet loss takes the three inputs: the anchor, a positive sample, and a negative sample, and minimizes the anchor—positive distance while maximizing the anchor—negative distance.

The Triplet loss function compares a baseline (the anchor) to positive and negative inputs. It then reduces the distance between the baseline and the positive sample while increasing the distance between the anchor and the negative input.

A popular application of Triplet loss is to teach a model to detect faces. Instead of framing this use case as a classification problem, we can look at it as a similarity learning problem. The goal would be for a network to compare two pictures and output whether they are similar enough. 

Notice that we also use [Contrastive loss](https://medium.com/@maksym.bekuzarov/losses-explained-contrastive-loss-f8f57fe32246) in similarity learning problems, but this function receives two inputs and whether they represent the same concept or not.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What is Triplet loss"](https://deepchecks.com/glossary/triplet-loss/) is a short introduction to this loss function. * The [Wikipedia page of the Triplet loss](https://en.wikipedia.org/wiki/Triplet_loss) function is also a great reference.* Check ["Losses explained: Contrastive Loss"](https://medium.com/@maksym.bekuzarov/losses-explained-contrastive-loss-f8f57fe32246) for an explanation of the Contrastive loss.</p></details>

-----------------------

## Date - 2023-10-07


## Title - Features and data


### **Question** :

Balancing the size of a dataset and the number of features to train a model is always a problem you need to consider.

**Which of the following statements correctly summarizes your thoughts about the relationship between features and dataset size?**


### **Choices** :

- When training a model, as you add more features to the dataset, you often need to increase the dataset's size to ensure the model learns reliably.
- When training a model, adding more features to the dataset increases the amount of information you can extract, allowing you to use smaller datasets and still extract good performance from the data.
- When training a learning algorithm, as you decrease the number of features in your dataset, you need to increase the number of training samples to make up the difference.
- When training a learning algorithm, the features in your dataset are entirely independent of the number of training samples.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Removing features reduces the number of dimensions in our data. It concentrates the samples we have in a lower-dimensional space. We can't replace the information provided by a feature with more data.

Imagine plotting a set of numbers. Since you have only one dimension, they will all lie somewhere in a line. Don't add new values; increase the features by adding a second dimension. Your values now become a set of 2D coordinates `(x, y)`; if you graph them, they will all be somewhere in a plane.

If you compare the 1D line with the 2D plane (or even a 3D space, assuming you add a third dimension,) something will become apparent quick: As we increase the dimensionality of the data, it will be harder and harder to fill up the space with the same points.

This increase in sparsity will make it much harder for the learning algorithm to find interesting patterns. 

Based on this, we can conclude that there's a relationship between features and samples, and the more features we add, the more data points we need.

The first choice is the only correct solution to this question. For a more formal definition, look at the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality): The amount of data needed to extract relevant information increases exponentially with the number of features in your dataset.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["What is the Curse of Dimensionality?"](https://deepai.org/machine-learning-glossary-and-terms/curse-of-dimensionality) for an introduction to the Curse of Dimensionality.</p></details>

-----------------------

## Date - 2023-10-08


## Title - False Negatives


### **Question** :

Here is the picture of a confusion matrix we generated after evaluating a model in a dataset with 100 samples:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186962046-7c076163-5b58-442f-9641-c09f5cf71003.jpg)

Assume that class A represents the outcomes of the model we are interested in finding.

**What's the total of False Negatives on this evaluation round?**


### **Choices** :

- False Negatives are class A samples the model predicted as class B, so the answer is 7.
- False Negatives are class B samples the model predicted as class A, so the answer is 13.
- False Negatives are class A samples the model predicted as class A, so the answer is 52.
- False Negatives are class B samples the model predicted as class B, so the answer is 28.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We are interested in samples from class A which means we will treat class A as our "Positive" samples and class B as our "Negative" samples.

If we replace the classes in the confusion matrix with Positive and Negative instead of "A" and "B," it's much easier to reason about the model's number of False Negatives:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186963826-ec29e2b5-c065-4569-9542-712acab129da.jpg)

False Negatives are those samples that we expect to be Positive (class A), but the model predicted as Negative (class B.) Therefore, the correct answer to the question is 7. You can see every combination in the image above.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2023-10-09


## Title - Scikit-Learn's fan


### **Question** :

So far, Melody is a big fan of Scikit-Learn.

She hasn't used it for too long, but every time she learns something new, there's always an elegant way to do it using Scikit-Learn.

But today is perhaps the exception: Melody finished training a multi-class classification model, and after displaying Scikit-Learn's classification report, she noticed something wasn't right.

She has to analyze the Macro-average, Micro-average, and Weighted F1-Score values for her model, but the classification report doesn't display the Micro-average F1-Score.

**How can Melody move forward?**


### **Choices** :

- The Micro-average F1-Score is the same as the model's accuracy. Melody can run her analysis using the accuracy of her model.
- The Micro-average F1-Score is the same as the model's precision. Melody can run her analysis using the precision of her model.
- The Micro-average F1-Score is the same as the model's recall. Melody can run her analysis using the recall of her model.
- Melody will need to compute the Micro-average F1-Score of her model manually.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When we work on multi-class classification problems, it's simple to compute the F1-Score for each class. Several strategies exist to calculate a global F1-Score representing the entire model's performance: Macro-average F1-Score, Micro-average F1-Score, and Weighted F1-Score.

Scikit-Learn's classification report shows all three of these metrics but uses a different name for the Micro-average F1-Score. That's why Melody is confused.

The Micro-average F1-Score is a metric where we sum all of the contributions from each category to compute an aggregated F1-Score. Micro-average F1-Score calculates the proportion of correctly classified samples out of all samples, which is the model's accuracy definition.

In any multi-class classification problem, the Micro-average F1-Score is the same as the accuracy. That's why Scikit-Learn displays it under the "accuracy" label.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Micro, Macro & Weighted Averages of F1 Score, Clearly Explained"](https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f) is a great article covering how to compute a global F1-Score metric in multi-class classification problem.* Check ["What is the F-Score?](https://deepai.org/machine-learning-glossary-and-terms/f-score) for a short introduction to the Fβ-Score.</p></details>

-----------------------

## Date - 2023-10-10


## Title - First network


### **Question** :

Lisa has completed building her first neural network.

After training the model for about twenty minutes, Lisa noticed that her training loss was not declining as she had expected; instead, it was notably high.

Perplexed by the unexpected scenario, Lisa paused her training and started investigating the possible causes.

After hours of scrutinizing, Lisa proposed a few potential explanations for the problem.

**Which of the following factors could contribute to the loss behaving in such a way?**


### **Choices** :

- The neural network is getting stuck at local minima.
- Lisa is using a learning rate that's too high.
- Lisa is using a learning rate that's too low.
- The regularization that Lisa is using is too aggressive.


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Many factors can cause problems during training. Let's discuss some of the more common ones.

[Regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)) is a beneficial technique to avoid overfitting, but it may prevent the network from learning when it is too aggressive. This happens because regularization imposes a penalty on the network's weights, preventing them from becoming too large. If we aren't careful, the weights may stop changing, the network will stop learning, and the loss will stay high.

Using a learning rate that's too low might also keep the training loss high. The network weights will be updated very slowly with a low learning rate. Unless we run the training process for many iterations, the network will struggle to get to where the loss is sufficiently low.

Finally, the optimization may be stuck at a local minimum. This will cause the loss to stop decreasing altogether. Strategies to overcome this problem include better initializing the network's parameters or increasing the learning rate or momentum to overcome the local minimum.

It's improbable that the problem is caused by a learning rate that's too high. Lisa would see rather significant changes in the loss if this were the case. With a high learning rate, it's also common to see the loss oscillating after some time.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What should I do when my neural network doesn't learn?"](https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn) is an excellent discussion about this topic.</p></details>

-----------------------

## Date - 2023-10-11


## Title - Extra regularization


### **Question** :

Eloise found that adding Batch Normalization was the best approach to improve the training speed of her deep neural network.

But everything comes at a price. Batch Normalization introduces some regularization to the network, and Eloise knows this will affect the Dropout she is currently using.

**How should Eloise adjust the Dropout her network is using?**


### **Choices** :

- After adding Batch Normalization, Eloise should remove the Dropout she is currently using.
- After adding Batch Normalization, Eloise should slightly decrease the amount of Dropout she is currently using.
- After adding Batch Normalization, Eloise should slightly increase the amount of Dropout she is currently using.
- After adding Batch Normalization, Eloise should significantly increase the amount of Dropout she is using.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Batch Normalization is a popular technique used to train deep neural networks. It normalizes the input to a layer during every training iteration using a mini-batch of data. It smooths and simplifies the optimization function leading to a more stable and faster training.

Batch Normalization acts as a regularizer. The mean of a mini-batch of data is a noisier version of the true mean of the population, and when used to normalize the data, it adds randomness to the optimization process. Eloise will need to adjust the Dropout she is using to accommodate this extra regularization.

Keep in mind that Batch Normalization doesn't introduce a lot of regularization, so it's unlikely that Eloise can remove all of the Dropout she is using. She should, however, slightly decrease the Dropout she is using to account for the extra regularization.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"](https://arxiv.org/pdf/1502.03167.pdf) is the original paper introducing Batch Normalization.* ["Intro to Optimization in Deep Learning: Busting the Myth About Batch Normalization"](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/) is an excellent blog post challenging some of the ideas from the original paper.* ["A Gentle Introduction to Batch Normalization for Deep Neural Networks"](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/) is a good introduction to Batch Normalization.</p></details>

-----------------------

## Date - 2023-10-12


## Title - Recurring daughter


### **Question** :

I'm trying to convince my daughter that she will have difficulty using a traditional feedforward network to process comments from her followers on YouTube.

Instead, I recommended a simple Recurrent Neural Network as a good starting point. Going to transformers is overkill, and I'd rather have her look into some foundational ideas first.

**Which of the following do you think I should include in the list of differences between recurrent and traditional networks?**


### **Choices** :

- Traditional neural networks can process inputs of any length. Recurrent Neural Networks require a fixed-size sequence as their input.
- Recurrent Neural Networks capture the sequential information present in the input data. Traditional neural networks don't have this ability.
- Recurrent Neural Networks share weights across time. Traditional networks use different weights on each input node.
- A simple Recurrent Neural Network can consider future inputs to compute their current state. Traditional neural networks can't access future inputs.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNN) are a type of artificial neural network that can process sequential or time series data. Their main difference from traditional networks is their ability to take information from prior inputs to influence the current input and output. This ability allows them to capture any sequential information present in the data. For example, an RNN is ideal for capturing the dependency between words of a sentence.

RNN processes the data sequentially so a model can process sequences of varying sizes. For example, an RNN can process a 5-word and 10-word sentence using the same input structure, unlike a traditional neural network that will need a different input size for each case. 

RNNs share the same weight parameters for every input sample, unlike traditional networks with different weights across each input node. Sharing parameters helps an RNN generalize to sequences of varying lengths and operate similarly on sequences with the same meaning but organized differently. The [accepted answer](https://stats.stackexchange.com/a/318428) to [this question](https://stats.stackexchange.com/questions/221513/why-are-the-weights-of-rnn-lstm-networks-shared-across-time) expands on this topic with an example.

Finally, neither a simple Recurrent Neural Network nor a traditional network can access future inputs to compute their current state. A Bidirectional RNN is a combination of two Recurrent Neural Networks working in opposite directions, one from the beginning to the end of a sequence, and the other, from the end to the beginning of a sequence. A Bidirectional RNN can access future inputs, but not a simple one.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Recurrent Neural Networks"](https://www.ibm.com/cloud/learn/recurrent-neural-networks) for a description of what they are and how they work.* ["An Introduction To Recurrent Neural Networks And The Math That Powers Them"](https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/) is a deeper dive into RNNs.</p></details>

-----------------------

## Date - 2023-10-13


## Title - Coffee beans


### **Question** :

Hadley is building a multi-class classification model to separate coffee beans into five categories.

To measure the model's efficacy, Hadley decided to use the F1-Score.

But there's one problem: Hadley knows how to compute the F1-Score for every class separately, but she needs a single F1-Score value to compare different model versions and choose the best one.

**Select from the following list every metric that Hadley can use to summarize the F1-Score for her model?**


### **Choices** :

- Hadley can compute the Macro-average F1-Score.
- Hadley can compute the Micro-average F1-Score.
- Hadley can compute the ROC F1-Score.
- Hadley can compute the Weighted F1-Score.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When building a classification model, the precision and recall scores are two metrics that indicate how effective the model is. There's a trade-off between these metrics, so higher precision models sacrifice recall and vice versa.

It's hard to compare classifiers unless we have a single metric that summarizes the balance between precision and recall. The Fβ score lets us do that. 

![FBeta-Score](https://user-images.githubusercontent.com/1126730/193036770-0a37f6f7-9a5d-4be3-9b89-7018931140eb.jpg)

When using β = 1, we place equal weight on precision and recall. For values of β > 1, recall is weighted higher than precision; for values of β < 1, precision is weighted higher than recall. F1-Score is the most commonly used, which is the Fβ score with β = 1.

Computing the F1-Score for each class in a multi-class classification problem is simple: F1-Score = 2 × (precision × recall) / (precision + recall), but this leads to the problem Hadley is facing: How can she combine the individual F1-score values into a single F1-Score value?

The Macro-average F1-Score is one approach where we calculate the F1-Score for each category and then average all the results. This method penalizes the model equally for any class that doesn't perform well, regardless of its importance or how many support samples it has.

The Micro-average F1-Score is another approach where we sum all of the contributions from each category to compute an aggregated F1-Score. In this case, we don't use the individual F1-Scores but the overall precision and recall across all samples. This method doesn't favor or penalize any class in particular.

Finally, the Weighted F1-Score is another way of computing a global F1-Score. In this approach, we weigh every individual F1-Score using the number of true labels of each class and then sum them to produce the global F1-Score. This method favors the majority classes because they will be weighted more in the computation.

The ROC F1-Score is a made-up term and therefore is not correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Micro, Macro & Weighted Averages of F1 Score, Clearly Explained"](https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f) is a great article covering how to compute a global F1-Score metric in multi-class classification problem.* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.</p></details>

-----------------------

## Date - 2023-10-14


## Title - High-speed train


### **Question** :

Ladybug and Lemon are working on a machine-learning system to control high-speed trains.

They decided to use k-Nearest Neighbors in one of the system modules but had to stop when they realized they had to work with some data represented in binary form.

Ladybug and Lemon must decide which function to use to compute the distance between these binary columns. They want to measure how much two features with the same length differ.

**Which of the following is the appropriate measure to compute the distance between binary columns?**


### **Choices** :

- Minkowski distance
- Euclidean distance
- Manhattan distance
- Hamming distance


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The Minkowski distance is a generalization of the Euclidean and Manhattan distances. Both of these work with real-value vectors, but the Euclidean distance is the shortest path between objects, while the Manhattan distance is the rectilinear distance between them. Using the Minkowski distance, we can control which approach to use depending on the data. 

Ladybug and Lemon, however, need to measure how much two messages with the same length differ.

The Hamming distance computes the distance between two binary vectors. It's the ideal function for this example.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["4 Distance Measures for Machine Learning"](https://machinelearningmastery.com/distance-measures-for-machine-learning/) for a complete explanation of these four distance measures.* ["Five Common Distance Measures in Data Science With Formulas and Examples"](https://regenerativetoday.com/five-common-distance-measures-in-data-science-with-formulas-and-examples/) is a deeper dive into these distance measures.</p></details>

-----------------------

## Date - 2023-10-15


## Title - Maximum performance


### **Question** :

Claire and Phil aren't on the same page with their plan.

They want to train a machine learning model but want to minimize the number of samples they need to label. Labeling takes too long, and they want to avoid it as much as possible.

Claire argues that they don't need to train with the entire dataset. Instead, she believes they can maximize the model's performance without using all the data. 

Phil disagrees. He argues that the only way to achieve the maximum possible performance is to train with the entire dataset. Since they aren't willing to label all the data, they will need to settle for a mediocre model.

**What's your opinion about this situation?**


### **Choices** :

- Achieving the maximum possible performance without using the entire dataset is theoretically possible but very unlikely.
- They can achieve the maximum possible performance without using the entire dataset by randomly sampling a portion of the data, labeling it, and training the model.
- They can achieve the maximum possible performance without using the entire dataset, but they need a good strategy to sample the data they will label to train the model.
- They will never achieve the maximum possible performance without using the entire dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Claire and Phil do not need to use the entire dataset to build a model that reaches its maximum possible performance. However, they will need a smart strategy to select the data they need to label.

Let's imagine a dataset with two classes that we can represent in two dimensions and a linear model that splits the data into two groups. Any samples around the lines' boundaries that separate both classes are critical in our dataset. Those samples help the model decide how to split the data!

But what about samples far away from the split? They contribute much less to the model, and we don't need them to find the separation between classes. The same happens with duplicate samples or samples that are too similar to existing ones.

Claire and Phil, however, can't depend on randomly sampling the dataset to decide which instances to label. They need a better strategy to determine which samples to pick.

This scenario is an example of [Active learning](https://articles.bnomial.com/active-learning). This learning technique allows us to build a better-performing machine learning model using fewer training labels by strategically choosing the samples to train the model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Active Learning"](https://articles.bnomial.com/active-learning) is a short introduction to active learning and how the process works.</p></details>

-----------------------

## Date - 2023-10-16


## Title - The model's recall


### **Question** :

A team built a binary classification model. They named the classes `A` and `B`.

After finishing training, they evaluated the model on a validation set, and here is the confusion matrix with the results:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186248358-1b39a042-5725-408b-84aa-8dd915a6d99d.jpg)

**Given the above confusion matrix, what of the following correctly represents the model's recall predicting classes `A` and `B`, respectively?**


### **Choices** :

- The model's recall predicting class `A` is 80%, and class `B` is 80%.
- The model's recall predicting class `A` is 88%, and class `B` is 68%.
- The model's recall predicting class `A` is 88%, and class `B` is 88%.
- The model's recall predicting class `A` is 68%, and class `B` is 88%.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To compute the model's recall, we can use the following formula:

```
recall = TP / (TP + FN)
```

Let's start with class `A`. We have 52 true positive samples and 7 false negatives in this example. Substituting these values in our formula:

```
recall = TP / (TP + FN)
recall = 52 / (52 + 7)
recall = 52 / 59
recall = 0.88
```

Therefore, the model's recall at predicting class `A` is 88%.

We can now look at class `B`. We have 28 true positive samples and 13 false negatives in this example. Substituting these values in our formula:

```
recall = TP / (TP + FN)
recall = 28 / (28 + 13)
recall = 28 / 41
recall = 0.68
```
Therefore, the model's recall at predicting class `B` is 68%.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.</p></details>

-----------------------

## Date - 2023-10-17


## Title - Bird-watching schedule


### **Question** :

Isabella developed a Deep Learning model to classify different types of birds in images taken from a bird-watching camera. During the initial experiments, she discovered a learning-rate schedule that provided excellent performance for her model.

She wants to train a new version of her model using the images from a second camera. The cameras are identical and are just positioned at different angles.

Isabella plans to keep the exact model architecture, optimization process, and learning-rate schedule and only change the dataset. She will even train the new model version using the same number of samples.

**Should Isabella anticipate that her learning-rate schedule will also be effective for the second model?**


### **Choices** :

- Yes, because both datasets come from identical hardware. The learning-rate schedule should change whenever we have a dataset from a different target distribution, but that won't happen here.
- Yes, because the learning-rate schedule depends on the number of samples in the dataset.
- Yes, because learning-rate schedules are specific to the optimization mechanisms used by the model, and Isabella plans to keep the same optimization process.
- No, because a new dataset changes the tradeoff between optimization and regularization. The learning-rate schedule is dataset-specific.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's take a look at [this tweet](https://twitter.com/fchollet/status/1508477486882979843) from [François Chollet](https://amzn.to/3K3VZoy):

> PSA: never attempt to use a training schedule from an old dataset on a new dataset. Even if the model, the number of samples, and the target distribution are the same, the intrinsic difficulty of the problem may have changed (tradeoff between optimization and regularization). 

> Learning rate schedules and regularization are fundamentally dataset-specific, far more than model architecture.

Notice that he mentions that even when the dataset size and the target distribution are the same, the learning-rate schedule might not be helpful anymore. Even when we don't change anything about the model architecture or the method we use to train it, a new dataset may change the intrinsic difficulty of the problem.

With a new dataset, we may need more regularization to avoid overfitting, or we may need less of it to ensure the model learns the specifics of the problem. A different dataset leads to a different set of tradeoffs. 

Therefore, Isabel should not reuse the learning-rate schedule but find the best for the second model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* François' book, [_Deep Learning with Python, Second Edition_](https://amzn.to/3K3VZoy), is among the best Deep Learning references you'll find.</p></details>

-----------------------

## Date - 2023-10-18


## Title - Scrubbing movies


### **Question** :

Over the years, Brianna's agency pivoted to work for big Hollywood studios. They scrubbed early releases looking for mistakes and gathering information to determine the potential ratings that a movie would get.

But watching every hour of every film didn't scale, so they used an active learning approach to only select critical scenes for their team to review. 

They created a machine learning model to predict the ratings. They trained this model on a few randomly selected frames from each movie, processed the entire video, and used the output predictions to decide which scenes to review next. They retrained the model with the new labels and repeated several more iterations.

Thanks to this process, the team reduced the review time by more than 60 percent.

**Which of the following is the team's criterion to select which scenes they will manually review after each iteration?**


### **Choices** :

- The team selects any scene their model predicts with high confidence.
- The team selects any scene their model predicts with low confidence.
- The team selects any scenes that the model predicts correctly.
- The team selects any scenes where the model makes a mistake.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Active learning](https://articles.bnomial.com/active-learning) is a learning technique that allows us to build a better-performing machine learning model using fewer training labels by strategically choosing the samples to train the model.

There are many ways to select which scenes will be better for the model, but in summary, we want to strategically choose those that will provide the most value to the model.

The model returns the predicted ratings and the confidence in those predictions. High-confidence samples aren't a challenge for the model, and the amount of information the team will get from labeling them is low. Conversely, low-confidence samples indicate that the model needs help, so the team should focus their time exclusively on them.

Notice that the team doesn't know which mistakes the model makes unless they manually review each scene. Doing this will defeat the purpose of reducing the time they take reviewing footage. They should only rely on the prediction confidence to decide what portions of the video they should consider.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Active Learning"](https://articles.bnomial.com/active-learning) is a short introduction to active learning and how the process works.</p></details>

-----------------------

## Date - 2023-10-19


## Title - Low training and testing


### **Question** :

There's not a lot of context for you other than the following chart showing the training and testing losses of a machine learning model:

![Training and Testing Loss Chart](https://user-images.githubusercontent.com/1126730/188513133-fefa6ed8-9541-4bbe-b1f2-cae6e19c09f7.jpg)

As you can see, after finishing training, both losses are low.

**What's a reasonable conclusion about this machine learning model?**


### **Choices** :

- Your model is overfitting.
- Your model is underfitting.
- Your model is either overfitting or underfitting, but we can't tell.
- Your model is well-fit.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A good model should capture valuable patterns in the data and discard any noise that doesn't help with predictions. An overfitting model will fit that noise. An underfitting model will not capture the relevant patterns in the dataset. 

An overfitting model should not have any problems with the training data but stumble with the testing data. Therefore, we should expect a low training loss and a high testing loss. An underfitting model should struggle with the training and testing datasets, so both of its losses should be high.

A well-fit model, however, should have low training and testing losses, which is what we see in the chart.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.</p></details>

-----------------------

## Date - 2023-10-20


## Title - Junior suggestions


### **Question** :

Amaya leads the team responsible for maintaining a machine learning model that helps run their company's marketing budget. The model has been running for a long time, and Amaya's team makes periodic updates and improvements.

But they want more.

The team aims to find some breakthroughs to improve the model considerably. Surprisingly, the most junior person on the team was the one coming up with two different ideas: 

1. Take each row separately, and count the missing values across all columns. Then add a new column to the dataset with that number.
2. Replace a categorical feature on their dataset with the number of times each value appears across all samples.

**Which of the following would be your recommendation for Amaya regarding these two ideas?**


### **Choices** :

- Amaya shouldn't consider any of these techniques because they aren't valid forms of feature engineering.
- Amaya should only consider the first technique. The second one is not a valid form of feature engineering.
- Amaya should only consider the second technique. The first one is not a valid form of feature engineering
- Amaya should consider both methods.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Amaya should experiment with both techniques to see whether they improve their model. The two suggestions are examples of meta-features based on the rows and columns of the team's dataset.

The first suggestion, for example, will help the model understand which rows have an excess of missing values and which are complete. Of course, there's no guarantee this information is helpful, but that's something Amaya will have to decide based on her experiments.

The second suggestion is an example of frequency encoding. It will make evident to the model the importance of an individual row based on how prominently it appears on the data. For instance, we could replace a job position title with the number of employees with that title.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.* [_Feature Engineering for Machine Learning_](https://amzn.to/3SsnLAc) is an excellent book covering feature engineering.</p></details>

-----------------------

## Date - 2023-10-21


## Title - Classifying bottles


### **Question** :

Esther decided to do something with her machine learning model to get better results from her model.

She was responsible for a deep learning model to classify bottles in a factory. Instead of running every picture through her model, she created two more copies of the image: the first by flipping the original image vertically and the second by zooming in around 10%.

Esther ran all three images through the model. She figured that having multiple images to construct the final answer would give her better results. 

**What's the best way for Esther to use the three softmax vectors that her model will output to make a final decision? Select only one option.**


### **Choices** :

- Esther should select the best one of the three vectors that come out of her model.
- Esther should use the softmax vector that contains the lowest value to compute the final result.
- Esther should use the softmax vector that includes the highest value to calculate the final result.
- Esther should average out every softmax vector and use that new vector to compute the final result.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Esther is using [Test-time augmentation](https://articles.bnomial.com/test-time-augmentation), a technique that relies on augmenting the original image before running it through a model. By doing this, Esther gives the model more opportunities to compute the correct prediction.

When Esther runs three pictures through the model, she will get back three different vectors. How can she use these vectors to determine the final result?

The best approach from the ones provided in this question is to average all three vectors and use the result to compute the correct class. By doing this, Esther will take advantage of each picture to make the final decision.

There's no way to know which vector is "the best one," so the first option is incorrect. Choosing the vector will the lowest value doesn't seem like a great idea to make the best prediction, and although Esther could use the vector with the highest value, averaging all vectors will be a better approach.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check [Test-time augmentation](https://articles.bnomial.com/test-time-augmentation) for an introduction to a technique that will help you make better predictions with your machine learning model.</p></details>

-----------------------

## Date - 2023-10-22


## Title - Know-it-all students


### **Question** :

Athena is preparing for next week's lecture. She teaches computer vision at a prestigious college and has to deal with a couple of know-it-all students.

Athena needs to ensure her material is sharp.

She will cover Max Pooling, when to use it, and its advantages. To finish her lecture, she prepared a quiz just in case the students gave her a hard time.

**Here's Athena's quiz. Which of the following is true about Max Pooling?**


### **Choices** :

- Max pooling will always decrease the number of parameters in the network compared to another network that doesn't use Max pooling.
- Max pooling will always increase the number of parameters in the network compared to another network that doesn't use Max pooling.
- Max pooling will improve the network's ability to detect the same features in different locations within the image.
- Max pooling extracts smooth features by calculating the average value for every patch on the feature map.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>There's a "gotcha" in Athena's quiz. We commonly use Max pooling to downsample feature maps, thus reducing the number of parameters in the network, but it depends on how large we configure the pool size. The first choice of Athena's quiz states that Max pooling will always decrease the number of parameters, but if we use a pool size of 1, there will be no downsampling. Thus, neither the quiz's first nor the second choices are correct.

Max pooling does add translation invariance properties to the network, which is important because we usually care about whether a feature is present in the image rather than where it is. 

Finally, the fourth choice describes how Average pooling works. On the other hand, Max pooling uses the maximum value in a patch rather than the average value.

Athena's students will undoubtedly have a lot of fun with the quiz.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Max Pooling in Convolutional Neural Network and Its Features"](https://analyticsindiamag.com/max-pooling-in-convolutional-neural-network-and-its-features/) is a great introduction to Max Pooling.* Check ["A Gentle Introduction to Pooling Layers for Convolutional Neural Networks"](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/) for more information about how pooling layers work.</p></details>

-----------------------

## Date - 2023-10-23


## Title - The beer factory


### **Question** :

Logan works in the quality department of a beer factory.

She is responsible for maintaining several machine learning models that the factory uses to research and develop new drinks and containers.

While evaluating a new process, Logan put together the following table to show her manager the results. The red chips correspond to defective containers, and the model's results are those within the black lines at the center:

![Model Results](https://articles.bnomial.com/images/article-when-accuracy-doesnt-help-4.jpg)

**Which of the following is the correct f1-score of this model?**


### **Choices** :

- The f1-score of Logan's model is 43%.
- The f1-score of Logan's model is 50%.
- The f1-score of Logan's model is 86%.
- The f1-score of Logan's model is 100%.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The model's f1-score is the harmonic mean between precision and recall. We can use the following formula to compute it:

![F1-Score](https://articles.bnomial.com/images/article-when-accuracy-doesnt-help-3.jpg)

Both the precision and recall of this model are 43%. Substituting this value in the previous formula, we get that the f1-score of Logan's model is also 43%.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.</p></details>

-----------------------

## Date - 2023-10-24


## Title - Computationally expensive


### **Question** :

Elliana's mandate to her team was to build a model as fast as possible and put it out there.

She leads a startup focused on keeping costs down and providing value as quickly as possible. Elliana understands that investing resources into a better model has diminishing returns. 

Her team is trying to decide what process to follow to build their model. They want to prioritize an approach that's as computationally cheap as possible. They have a list of potential choices.

**Looking at the list, which will be the first option the team should discard because of how computationally expensive it is?**


### **Choices** :

- Splitting the dataset and training a model with 80% of the data while testing it with the remaining 20%.
- Using a leave-one-out cross-validation approach to build the model.
- Using a 5-Fold cross-validation approach to build the model.
- Using a 10-Fold cross-validation approach to build the model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The team should scratch off the [leave-one-out cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation) approach right away.

Leave-one-out cross-validation is a variant of cross-validation where the number of folds equals the number of samples in the dataset. 

To use leave-one-out cross-validation, we build one model for each sample in the dataset. We train each model using all data except one instance we later use to evaluate its performance. Finally, we compute the overall performance by averaging the result of each model.

Assuming the team will use leave-one-out cross-validation on a dataset with 10,000 samples, they will need to train 10,000 models. Compare this with 10-Fold cross-validation, where they will only need to build ten models. 

On the other hand, leave-one-out cross-validation will give a more robust estimate of model performance. Each sample has an opportunity to represent the entire dataset and contribute to the final evaluation, and this will result in a reliable and unbiased estimate of model performance.

The team is not interested in better performance and wants a cheaper model to train. Leave-one-out cross-validation is significantly more expensive than every other choice on this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["LOOCV for Evaluating Machine Learning Algorithms"](https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/) is an excellent introduction to leave-one-out cross-validation.* Check ["A Quick Intro to Leave-One-Out Cross-Validation (LOOCV)"](https://www.statology.org/leave-one-out-cross-validation/) for a succinct introduction to leave-one-out cross-validation.</p></details>

-----------------------

## Date - 2023-10-25


## Title - Paul, the octopus


### **Question** :

During the 2010 world cup, an octopus named Paul became famous for correctly predicting the result of 8 soccer matches with no misses. 

Paul's keepers would show him two boxes of food, each decorated with a team's flag. Whichever box Paul ate from first would be his prediction.

The probability of doing what Paul did was a mere 0.39%, hence his rise to fame.

**Which of the following is a correct statement about this story?**


### **Choices** :

- This story is an example of survivorship bias.
- This story is an example of confirmation bias.
- This story is an example of group attribution bias.
- This story doesn't show any biases.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Paul was undoubtedly an impressive octopus, and without taking anything from him rather than magic, this situation shows the effects of [survivorship bias](https://en.wikipedia.org/wiki/Survivorship_bias).

While the cameras focused on Paul because of his success, we don't consider all the other animals that tried but failed. If we assume that 1,000 animals tried to predict all eight games, probabilistically 3.9 of them would have gotten the result correctly.

Of course, nobody cared about animals that failed their predictions but immediately gravitated toward the seemingly supernatural Paul. 

Survivorship bias seriously compromises our ability to determine the odds of something happening. We focus on what we can see (the winners) and ignore what we can't see (the losers). Always seek out stories of failure and learn from them.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The ["Survivorship bias"](https://en.wikipedia.org/wiki/Survivorship_bias) page in Wikipedia is an excellent resource.* Paul has a [Wikipedia page](https://en.wikipedia.org/wiki/Paul_the_Octopus) as well.</p></details>

-----------------------

## Date - 2023-10-26


## Title - Logistic churn


### **Question** :

Sophia is working on a project to predict customer churn using a logistic classifier. 

She knows that the precision of her predictions is heavily influenced by the error function she uses. She wants this function to possess the following characteristics:

* The function should produce a small number if the sample is correctly classified.
* The function should produce a large number if the sample is incorrectly classified.
* The error for a group of samples should be the sum or average for all the samples.

Sophia has a few options but would like your input.

**Which of the following would be the most appropriate error function for a logistic classifier?**


### **Choices** :

- Absolute error: a function that returns the absolute value of the difference between the prediction and the label.
- Square error: a function that returns the square of the difference between the prediction and the label.
- Mean percentage: a function that returns the average error of the differences between predicted and actual values.
- Log loss: a function that returns the negative logarithm of the product of probabilities.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The absolute error, the square error, and the log loss are viable options that meet the characteristics that Sophia is looking for. The "mean percentage" is a fabricated error function that doesn't exist.

Among these three functions, we must evaluate whether they can be used to construct a binary classification model.

Neither the absolute nor square error functions are used in binary classification problems. They don't penalize errors as severely as log loss does, so they aren't a good choice for this type of problem. 

Log loss, on the other hand, is one of the most commonly used error functions and an excellent choice for a binary classifier like the one Sophia is trying to develop.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check ["Logistic Regression for Machine Learning"](https://machinelearningmastery.com/logistic-regression-for-machine-learning/) for an introduction to Logistic Regression.- ["Logistic Regression: Loss and Regularization"](https://developers.google.com/machine-learning/crash-course/logistic-regression/model-training) is a quick summary about the Log-loss and regularization.</p></details>

-----------------------

## Date - 2023-10-27


## Title - Evaluating logistic regression


### **Question** :

Eva is working on a logistic regression model.

She works for an auto manufacturer that wants to predict whether previous customers are potential future buyers.

Eva has a well-balanced dataset and is ready to evaluate her model after a few iterations.

**Which of the following are viable ways for Eva to evaluate her model?**


### **Choices** :

- Eva could evaluate the model by computing its accuracy.
- Eva could evaluate the model by computing its log loss.
- Eva could evaluate the model using the Mean Squared Error.
- Eva could evaluate the model using the Mean Absolute Error.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Eva is building a [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) model to solve a binary classification problem. She is only concerned about two classes: potential buyers and non-potential buyers.

Eva could compute the accuracy of her model and use it to determine how good it is. Accuracy is misleading for imbalanced datasets, but this is not the case.

Eva could also compute the log loss of her model to evaluate it. You can use the log loss to determine how close are the prediction probabilities returned by the model to the actual values. The closer these are, the lower the log loss will be.

Finally, Eva cannot use the [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) (MSE) or the [Mean Absolute Error](https://en.wikipedia.org/wiki/Mean_absolute_error) (MAE) to evaluate her model. These metrics work for regression problems, where the model's output is a continuous value, but Eva is working on a classification problem with a model's output bounded between 0 and 1.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check out ["Logistic Regression for Machine Learning"](https://machinelearningmastery.com/logistic-regression-for-machine-learning/) for an introduction to Logistic regression.- ["Intuition behind Log-loss score"](https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a) is a great explanation of log loss and how it works.</p></details>

-----------------------

## Date - 2023-10-28


## Title - Representing music


### **Question** :

Ada needs to process a dataset of songs from her favorite musician and create a compact representation of what makes her music unique and distinctive.

She read about autoencoders and decided to design a network to solve her problem. But creating the bottleneck to force the model to represent the music wasn't as simple as she thought.

**Which of the following are valid approaches that Ada could follow to design the bottleneck of her autoencoder?**


### **Choices** :

- Ada can introduce an information bottleneck by constraining the number of nodes in the middle part of the network.
- Ada can introduce an information bottleneck by ensuring the middle part of the network has sufficient nodes to represent the input data in its original form.
- Ada can introduce an information bottleneck by using the proper optimization algorithm.
- Ada can introduce an information bottleneck by using a loss function that relies on activating a small number of nodes.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Autoencoders](https://essays.bnomial.com/autoencoders) are learning algorithms that can help with anomaly detection problems. 

An autoencoder is a neural network that we can split into three sections: an encoder, a bottleneck, and a decoder. The encoder compresses the original input into an intermediate representation, and the decoder reverses the process to reconstruct the original data. The bottleneck sits between the encoder and decoder and is the section that stores the compressed representation of the data. 

Regular autoencoders constrain the number of nodes in the bottleneck layers of the network. This characteristic forces the model to learn a compact representation of the data instead of memorizing the input. Therefore, the first choice is correct.

Sparse autoencoders employ a different technique to introduce an information bottleneck. They use a loss function that relies on a small number of nodes. These autoencoders don't need bottleneck layers since penalizing activations is enough to accomplish the same purpose. Therefore, the fourth option is also correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Autoencoders"](https://articles.bnomial.com/autoencoders) for an introduction to a learning technique to represent data efficiently using neural networks.* For more information about sparse autoencoders, check the ["Introduction to autoencoders"](https://www.jeremyjordan.me/autoencoders/) article by Jeremy Jordan.</p></details>

-----------------------

## Date - 2023-10-29


## Title - True Negatives


### **Question** :

Here is the picture of a confusion matrix we generated after evaluating a model in a dataset with 100 samples:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186962046-7c076163-5b58-442f-9641-c09f5cf71003.jpg)

Assume that class B represents the outcomes of the model we are interested in finding.

**What's the total of True Negatives on this evaluation round?**


### **Choices** :

- True Negatives are class A samples the model predicted as class B, so the answer is 7.
- True Negatives are class B samples the model predicted as class A, so the answer is 13.
- True Negatives are class A samples the model predicted as class A, so the answer is 52.
- True Negatives are class B samples the model predicted as class B, so the answer is 28.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We are interested in samples from class B, which means we will treat class B as our "Positive" samples and class A as our "Negative" samples.

If we replace the classes in the confusion matrix with Positive and Negative instead of "B" and "A," it's much easier to reason about the model's number of True Positives:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/187207950-77e4eccd-0b63-43bc-8f07-be19f690d725.jpg)

True Negatives are those samples that we expect to be Negative (class A), and the model predicted as Negative (class A.) Therefore, the correct answer to the question is 52. You can see every combination in the image above.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2023-10-30


## Title - A morning walk


### **Question** :

Maria was taking her morning walk when she decided to modify her deep neural network architecture.

She's been researching different ways to speed up her training process and decided to experiment with Batch Normalization right before one of the hidden layers of her network.

There was only one question left to answer:

**How could Maria use Batch Normalization as part of her model?**


### **Choices** :

- Maria could use Batch Normalization before the activation function of the layer that comes before the layer she wants to affect.
- Maria could use Batch Normalization after the activation function of the layer that comes before the layer she wants to affect.
- Maria could use Batch Normalization right after the layer she wants to affect but before that layer's activation function.
- Maria could use Batch Normalization right after the activation function of the layer she wants to affect.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Batch Normalization is a popular technique used to train deep neural networks. It normalizes the input to a layer during every training iteration using a mini-batch of data. It smooths and simplifies the optimization function leading to more stable and faster training.

Maria has two options to batch-normalize the input of the hidden layer she is interested in: she can use Batch Normalization before the layer she wants to affect, either before or after the activation function. Remember that Batch Normalization will normalize the data that goes into the layer, so Maria needs to ensure to apply it before that layer.

But where is it better to use Batch Normalization? Before or after the previous layer's activation function?

The authors of ["Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"](https://arxiv.org/pdf/1502.03167.pdf) recommend using it right before the activation function:

> The goal of Batch Normalization is to achieve a stable distribution of activation values throughout training, and in our experiments we apply it before the nonlinearity since that is where matching the first and second moments is more likely to result in a stable distribution.

That has been the way many teams have used Batch Normalization, but later, there are experiments showing that Batch Normalization works best when used after the activation function. Here is an excerpt from ["Busting the Myth About Batch Normalization"](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/) from the [Paperspace blog](https://blog.paperspace.com/):

> While the original paper talks about applying batch norm just before the activation function, it has been found in practice that applying batch norm after the activation yields better results. This seems to make sense, as if we were to put an activation after batch norm, then the batch norm layer cannot fully control the statistics of the input going into the next layer since the output of the batch norm layer has to go through an activation. 

In summary, the literature is split regarding the best way to use Batch Normalization. Maria could use it before or after the previous's layer activation function.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"](https://arxiv.org/pdf/1502.03167.pdf) is the original paper introducing Batch Normalization.* ["Intro to Optimization in Deep Learning: Busting the Myth About Batch Normalization"](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/) is the blog post challenging the original paper's position regarding where to use Batch Normalization.* ["A Gentle Introduction to Batch Normalization for Deep Neural Networks"](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/) is a good introduction to Batch Normalization.</p></details>

-----------------------

## Date - 2023-10-31


## Title - Picewise function


### **Question** :

Elliott is a data scientist who has been working with machine learning models. She has been studying the Rectified Linear Unit (ReLU) Activation Function, often used in deep learning models. 

The ReLU function is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.

**Which of the following is true about the Rectified Linear Unit (ReLU) Activation Function?**


### **Choices** :

- The function is continuous but not differentiable.
- The function is differentiable but not continuous.
- The function is both continuous and differentiable.
- The function is neither continuous nor differentiable.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>ReLU is continuous but not differentiable.

While the function is indeed differentiable for values of `x>0` and `x<0`, it is not differentiable for `x=0`. 

Think about it: what is the derivative of the ReLU function at `x=0`? Is it constant? Is it rising? The derivative is not defined at `x=0`, so the function is not differentiable.

This may be surprising because we know that Gradient Descent needs a differentiable function to work. How come it works with ReLU, then?

In a [really good post](https://sebastianraschka.com/faq/docs/relu-derivative.html) about this topic, [Sebastian Raschka](https://twitter.com/rasbt) wrote about this:

> In practice, it’s relatively rare to have `x=0` in the context of deep learning; hence, we usually don’t have to worry too much about the ReLU derivative at `x=0`. We typically set it to `0`, `1`, or `0.5`.

So there you have it. The correct answer to this question is the fourth choice.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["A Gentle Introduction to the Rectified Linear Unit (ReLU)"](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) for an introduction to ReLU.* The [_Deep Learning_](https://amzn.to/3MqvoTQ) book is a great resource.</p></details>

-----------------------

## Date - 2023-11-01


## Title - Regularizing a model


### **Question** :

Charlotte's machine learning model is overfitting.

She needs to find a way to handle it, but before trying anything, she wants to understand her options.

**Which of the following are regularization techniques that Charlotte could consider?**


### **Choices** :

- Validation-based early stopping
- Dropout
- Data augmentation
- Cross-validation


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Validation-based early stopping is a regularization technique that stops the training process as soon as the generalization error of the model increases. In other words, if the model's performance on the validation set starts degrading, the training process stops. 

Dropout is a regularization method that works well and is vital for reducing overfitting. Dropout randomly removes a percentage of the nodes, forcing the network to learn in a balanced way, and tackling a phenomenon that we call "[co-adaptation](https://machinelearning.wtf/terms/co-adaptation/)."

Data augmentation has a regularization effect. Increasing the training data through data augmentation decreases the model's variance and, in turn, increases the model's generalization ability.

Finally, cross-validation is a validation scheme and not a regularization method.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Early Stopping"](https://articles.bnomial.com/early-stopping) for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models.* ["A Gentle Introduction to Dropout for Regularizing Deep Neural Networks"](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) is an excellent introduction to Dropout.* ["The Essential Guide to Data Augmentation in Deep Learning"](https://www.v7labs.com/blog/data-augmentation-guide) is an excellent article discussing data augmentation in detail.</p></details>

-----------------------

## Date - 2023-11-02


## Title - Dropout and loss


### **Question** :

It's the final round of interviews, and Savannah has done wonderfully well.

The final round is more technical. Savannah needs to answer a series of questions to demonstrate her understanding of different fundamental concepts.

While talking about deep learning, the interviewer focuses on the use of Dropout and asks Savannah a question about something she hasn't thought about before:

**How would you expect the training loss to behave if you train your model several times, each using an increasing Dropout rate?**


### **Choices** :

- The training loss will be higher as we increase the Dropout rate.
- The training loss will be lower as we increase the Dropout rate.
- The training loss will start oscillating as we increase the Dropout rate.
- The training loss will stay the same independently of the Dropout rate.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Dropout is a regularization method that works well and is vital for reducing overfitting.

Sometimes, the nodes in a neural network create strong dependencies on other nodes, which may lead to overfitting. An example is when a few nodes on a layer do most of the work, and the network ignores all the other nodes. Despite having many nodes on the layer, you only have a small percentage of those nodes contributing to predictions. We call this phenomenon "[co-adaptation](https://machinelearning.wtf/terms/co-adaptation/)," and we can tackle it using Dropout.

During training, Dropout randomly removes a percentage of the nodes, forcing the network to learn in a balanced way. Now every node is on its own and can't rely on other nodes to do their work. They have to work harder by themselves. 

One crucial characteristic of Dropout will help Savanah answer the question correctly: Like most regularization methods, Dropout sacrifices training accuracy to improve generalization. 

If we run a few training sessions, each using an increasing amount of Dropout, we should see the training loss trend higher. In other words, the more we regularize our model, the harder it will be to learn the training data.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For more information about co-adaptation and how to use Dropout, check ["Improving neural networks by preventingco-adaptation of feature detectors"](https://arxiv.org/pdf/1207.0580.pdf).* ["A Gentle Introduction to Dropout for Regularizing Deep Neural Networks"](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) is an excellent introduction to Dropout.</p></details>

-----------------------

## Date - 2023-11-03


## Title - A decision tree for that?


### **Question** :

Gianna had the opportunity to meet some of her technical heroes at a conference, and of course, she peppered them with questions!

At some point, the conversation centered around a machine learning solution they implemented. Gianna was delighted to hear that they used a decision tree for their solution.

The next day, she was still thinking about the conversation. She didn't have much machine learning experience but knew enough about decision trees to feel validated. It was fantastic to find out that decision trees are useful!

**Which of the following machine learning problems can you solve with a decision tree?**


### **Choices** :

- Binary classification problems where you need to decide the correct category for a sample among two possible choices.
- Multi-class classification problems where you need to decide the correct category for a sample among multiple choices.
- Multi-label classification problems where you need to decide the correct categories for a sample among multiple choices.
- Regression problems where you need to predict a continuous output.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To get straight to the answer, every one of the choices is a correct answer: decision trees are really powerful!

We usually discuss two types of decision trees in machine learning: classification and regression trees. The former covers the first three choices, while the latter covers the fourth choice.

Binary classification problems aim to classify one sample into two different categories. Multi-class classification problems are similar, but they classify samples into more than two categories. Decision trees are a perfect fit for these problems.

[Multi-label classification](https://en.wikipedia.org/wiki/Multi-label_classification) is somewhat different. Here we want to classify a sample into one or more categories. Decision trees can also solve these problems. Check out [Scikit-Learn's implementation](https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification) to see how they tackle multi-label classification.

Finally, decision trees can also solve regression problems where we want to predict a continuous target variable. ["How can Regression Trees be used for Solving Regression Problems?"](https://medium.com/analytics-vidhya/regression-trees-decision-tree-for-regression-machine-learning-e4d7525d8047) is an excellent introduction.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Scikit-Learn's ["Multiclass and multioutput algorithms"](https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification) will introduce you to solve these problems using Scikit-Learn's decision tree implementation.* ["How can Regression Trees be used for Solving Regression Problems?"](https://medium.com/analytics-vidhya/regression-trees-decision-tree-for-regression-machine-learning-e4d7525d8047) is an excellent article about using decision trees for regression tasks.</p></details>

-----------------------

## Date - 2023-11-04


## Title - Weather predictions


### **Question** :

For the first project, Cam wants to work with the weather dataset he found online.

Cam's Machine Learning class was a ton of fun. Their professor let them choose a problem to solve to allow them to showcase what they've learned so far. 

Cam wants to predict the probability of snowing based on four factors: the date, the air temperature, the location, and the air pressure. 

**Which of the following is the most appropriate algorithm that Cam should use to solve this problem?**


### **Choices** :

- Cam should use linear regression.
- Cam should use logistic regression.
- Cam should use K-means.
- Cam should use DBSCAN.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Cam's problem has two possible outcomes: it will snow or it won't, and Cam wants his model to return the probability of it. Logistic regression is an excellent fit for any problem with a binary outcome.

Logistic regression estimates the probability of an event occurring based on a dataset of independent variables. Linear regression, on the other hand, predicts the continuous dependent variable using a dataset of independent variables. 

K-means and DBSCAN are clustering algorithms and are not a good approach for this problem.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Linear Regression vs Logistic Regression"](https://www.javatpoint.com/linear-regression-vs-logistic-regression-in-machine-learning) does a full comparison between linear and logistic regression.* Check ["8 Clustering Algorithms in Machine Learning that All Data Scientists Should Know"](https://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/) for an explanation of K-Means and DBSCAN.</p></details>

-----------------------

## Date - 2023-11-05


## Title - Deep suffering


### **Question** :

Novah's team is struggling with the deep neural network they are working on.

Sadly, during backpropagation, the gradient values of their network drop significantly as the process gets closer to the initial layers, stopping them from learning at the same speed as the last set of layers.

Novah realizes their model is suffering from the vanishing gradient problem. She decides to look into every possible solution to improve their model.

**Which of the following techniques will make Novah's model more resistant to the vanishing gradient problem?**


### **Choices** :

- Novah should make sure they are initializing the weights properly. For example, using He initialization should help with the vanishing gradient problem.
- Novah should increase the learning rate to avoid getting stuck in local minima and thus reduce the chance of suffering vanishing gradients.
- Novah should modify the model architecture to introduce Batch Normalization.
- Novah should try ReLU as the activation function since it's well-known for mitigating the vanishing gradient problem.


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>If the gradients of the loss function get close to zero, the model will stop learning because the network will stop updating the weights. This issue is known as the [Vanishing Gradient Problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and it's common when using the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) and [tanh](https://www.sciencedirect.com/topics/mathematics/hyperbolic-tangent-function) activation functions in deep neural networks. 

On the other hand, [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) is a way to solve the vanishing gradient problem. ReLU is much less likely to saturate, and its derivative is `1` for values larger than zero.

[Batch normalization](https://en.wikipedia.org/wiki/Batch_normalization) is another way to lessen the vanishing gradient problem. Suppose we have a layer that uses a sigmoid activation function. We can normalize the input to that layer to ensure the values don't reach the edges and stay around the area where derivatives aren't too small. Changing the input to this layer with batch normalization will lessen the vanishing gradient problem.

Randomly initializing the weights of the deep network could also be problematic and lead to the vanishing gradient problem. If we use sigmoid or tanh as our activation functions, and many of the weights are initialized with values too small or too large, we will end up with derivatives close to zero. Using [He initialization](https://arxiv.org/abs/1704.08863) should prevent this from happening.

Lastly, the vanishing gradient problem has nothing to do with the learning rate used to train the network.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["How to Fix the Vanishing Gradients Problem Using the ReLU"](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/) is a great explanation of how to approach this problem.* Check ["On weight initialization in deep neural networks"](https://arxiv.org/abs/1704.08863). It's an excellent paper covering weight initialization.</p></details>

-----------------------

## Date - 2023-11-06


## Title - Low training


### **Question** :

There's not a lot of context for you other than the following chart showing the training loss of a machine learning model:

![Training Loss Chart](https://user-images.githubusercontent.com/1126730/188471836-1959ae8e-44dd-4811-9606-2d1c1f9ded0c.jpg)

As you can see, after finishing training, the loss is very low.

**What's a reasonable conclusion about this machine learning model?**


### **Choices** :

- The model may be overfitting, but we can't say for sure.
- The model may be underfitting, but we can't say for sure.
- The model may be well-fit, but we can't say for sure.
- The model is either overfitting or underfitting, but it's not well-fit.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A good model should capture valuable patterns in the data and discard any noise that doesn't help with predictions. An overfitting model will fit that noise. An underfitting model will not capture the relevant patterns in the dataset. 

An overfitting model should not have any problems with the training data, so we should expect a low training loss. An underfitting model should struggle with the training data, so its training loss will be high.

This model shows a low training loss, which we expect from an overfitting model, but this is not enough information to conclude that this model is overfitting. The model might be well-fit, but we can't say unless we evaluate it on a separate dataset. Therefore, the first and third options are correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.</p></details>

-----------------------

## Date - 2023-11-07


## Title - Salary leak


### **Question** :

There was a massive data leak, and for some mysterious reason, you came across a dataset full of compensation data from top tech companies in Silicon Valley.

You thought it wouldn't hurt to play around with the data for a little bit. You could finally build a model to predict future compensation based on the different attributes of each employee.

But one thing becomes apparent from the start: You need to cut down useless features to build something useful.

Dimensionality reduction to the rescue. You haven't done it before and want to ensure you are doing it correctly.

**How should you apply dimensionality reduction to your data?**


### **Choices** :

- Reduce the dimensions of the training dataset. It's unnecessary to use dimensionality reduction on the test dataset.
- Reduce the dimensions of the entire dataset. Split your data into training and test right after.
- Reduce the dimensions of the training dataset, then reduce the dimensions of the test dataset. We can use different dimensionality reduction techniques as long as both splits end up with the same features.
- Reduce the dimensions of the training dataset, then apply the same transformations to the test dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>You found there are too many features to create a useful model. The Curse of Dimensionality states that, as the dimensionality of the data increases, the amount of data needed to train a learning algorithm grows exponentially. 

There are different techniques to reduce the dimensionality of a dataset. They all follow the same principle: you start with a dataset, reduce its dimensionality, and obtain a new dataset with fewer features. Depending on the technique, the final dataset may contain a subset of the initial features or even have entirely different columns not present in the initial dataset.

The first choice argues that you only need to worry about reducing the dimension of the training dataset. That's incorrect. How can you test a model trained with a dataset containing different features? 

The second choice argues for reducing the dimensionality of the entire dataset and splitting the data right after that. Dimensionality reduction algorithms like PCA will use information about the whole dataset to produce new features. If we apply this algorithm to all of our data—including the test data, which we aren't supposed to know about— we'll leak details from the test data into the training set. 

The third choice is also incorrect. You need to apply dimensionality reduction separately to the training and test datasets and make sure you use the same transformations from the training data on the test data.

For example, imagine your dimensionality reduction technique creates a new feature based on the mean of another two columns. If you compute this mean separately on the train and test data, the resulting columns will come from different mean values. You need to avoid this problem by using what the fourth choice suggests: apply the same transformations and use the same information from the training and testing sets.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["What is the Curse of Dimensionality?"](https://deepai.org/machine-learning-glossary-and-terms/curse-of-dimensionality) for an introduction to the Curse of Dimensionality.* For an explanation of data leakages, check ["Data Leakage in Machine Learning"](https://machinelearningmastery.com/data-leakage-machine-learning/).* ["Introduction to Dimensionality Reduction for Machine Learning"](https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/) covers different examples of dimensionality reduction.</p></details>

-----------------------

## Date - 2023-11-08


## Title - Two experiments. Round two.


### **Question** :

Lena ran two experiments training a neural network on a sample dataset. Her goal is to understand how different batch sizes affect the training process.

One of the experiments used a very small batch size, and the other used a batch size equal to all existing training data.

After training and evaluating her models, Lena plotted the training and testing losses over 100 epochs of one of the experiments, and this is what she got:

![Learning Curves](https://user-images.githubusercontent.com/1126730/191095137-39c02bc1-a0f7-42bf-8c90-a331f9811ab9.jpg)

**Which of the following options is the most likely to be true?**


### **Choices** :

- The model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took less time to train.
- The model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took longer to train.
- The model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took less time to train.
- The model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took longer to train.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The amount of noise in the plot is crucial to answering this question.

The lower the batch size, the more noise we will get in the model's loss. When we only use a few samples, any instances that vary dramatically will cause the loss to swing wildly. When we use a larger batch, no individual sample would have the power to sway the loss too much, so we should expect less noise.

This plot shows a fair amount of noise, so this is likely the experiment that uses a small batch size.

The smaller the batch size, the more times we will need to compute the loss and update the model's weights during backpropagation. For example, if Lena's training set has 800 samples and uses one as her batch size, the algorithm will update the model's weights 800 times. However, if she uses a batch size of 32, the algorithm will update the model's weights 25 times (800 / 32 = 25.)

Updating the model during every iteration is computationally expensive, so using a smaller batch size will usually take longer. Therefore, the second option is probably the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["The wrong batch size is all it takes" ](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) explains how different batch sizes influence the training process of neural networks using gradient descent.* Check ["An overview of gradient descent optimization algorithms"](https://www.ruder.io/optimizing-gradient-descent/) for a deep dive into gradient descent and every one of its variants.</p></details>

-----------------------

## Date - 2023-11-09


## Title - Divisible numbers


### **Question** :

Lilly needs to pick a number divisible by 3 or by 7.

She can multiply them and use 21, but she needs to pick one number that's not larger than 20.

**Assuming Lilly selects a random number between 1 and 20, what's the probability it will be divisible by 3 or 7?**


### **Choices** :

- The probability is 12%
- The probability is 35%
- The probability is 40%
- The probability is 56%


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's assume that A is the event where Lilly selects a number divisible by 3, and B is where her number is divisible by 7. We want to find the probability of `A u B`, which denotes the event where Lilly selects a number divisible by 3 or 7.

The numbers divisible by 3 are 3, 6, 9, 12, 15, and 18. The numbers divisible by 7 are 7 and 14. Since the intersection of these two sets is empty, we know that events A and B are mutually exclusive, therefore:

```
P(A u B) = P(A) + P(B)
P(A u B) = 6/20 + 2/20
P(A u B) = 8/20
P(A u B) = 40%
```</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Cartoon Guide to Statistics_](https://amzn.to/3eg9iIo) is a fun and instructive introduction to probabilities and statistics.</p></details>

-----------------------

## Date - 2023-11-10


## Title - Increasing KNN's K


### **Question** :

Nyla started experimenting with her k-Nearest Neighbor (KNN) implementation. 

After trying many different things, she decided to increase the value of `K` until she landed in a good spot.

Unfortunately, Nyla can't explain why what she did work.

**What happened as Nyla increased the value of `K`?**


### **Choices** :

- As Nyla increased the value of K, she reduced the algorithm's variance and bias.
- As Nyla increased the value of K, she increased the algorithm's variance and bias.
- As Nyla increased the value of K, she increased the algorithm's variance and reduce its bias.
- As Nyla increased the value of K, she reduced the algorithm's variance and increase its bias.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The smaller the value of `K`, the more variance and less bias KNN will exhibit. For example, if we use `K = 1`, a single sample close to our observation will cause the algorithm to return the wrong prediction. Imagine an observation surrounded by many instances from class A and only one from class B that's closer than everything else. Since `K=1`, the algorithm will incorrectly predict the observation as class B. 

Conversely, the larger the value of `K`, the less variance and more bias KNN will exhibit. Since KNN uses an average or majority voting, no individual sample will cause the algorithm to return the wrong prediction. Setting `K` to a value that's too large will make the algorithm underfit because it won't capture the variance in the dataset.

Here is a quote from ["Why Does Increasing k Decrease Variance in kNN?"](https://towardsdatascience.com/why-does-increasing-k-decrease-variance-in-knn-9ed6de2f5061):

> If we take the limit as `K` approaches the size of the dataset, we will get a model that just predicts the class that appears more frequently in the dataset [...]. This is the model with the highest bias, but the variance is 0 [...]. High bias because it has failed to capture any local information about the model, but 0 variance because it predicts the exact same thing for any new data point.

In summary, the smaller the value of `K` is, the lower the bias and the higher the variance. The larger the value of `K` is, the higher the bias and the lower the variance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Why Does Increasing k Decrease Variance in kNN?"](https://towardsdatascience.com/why-does-increasing-k-decrease-variance-in-knn-9ed6de2f5061) is a great article diving into the relationship of `k` and the variance of KNN.* For a more general introduction to the bias-variance trade-off, check ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/).</p></details>

-----------------------

## Date - 2023-11-11


## Title - Testing loss going nuts


### **Question** :

There's not a lot of context for you other than the following chart showing the training and testing losses of a machine learning model:

![Training and Testing Loss Chart](https://user-images.githubusercontent.com/1126730/188513097-b11e85a0-9bd2-436c-b8e4-9a0595c1cff9.jpg)

As you can see, the training loss is low after finishing training, but the testing loss stops decreasing at some point and starts climbing.

**What are reasonable conclusions about this machine learning model?**


### **Choices** :

- Your model is underfitting.
- You are training a model for too many epochs.
- Your model is well-fit to the point where the testing loss increases. It overfits after that.
- Your model is well-fit to the point where the testing loss increases. It underfits after that.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A good model should capture valuable patterns in the data and discard any noise that doesn't help with predictions. An overfitting model will fit that noise. An underfitting model will not capture the relevant patterns in the dataset. 

An overfitting model should not have any problems with the training data but stumble with the testing data. Therefore, we should expect a low training loss and a high testing loss. An underfitting model should struggle with the training and testing datasets, so both of its losses should be high. A well-fit model, however, should have low training and testing losses.

A couple of things happen in this chart. First, up to the point the testing loss starts increasing, the model is well-fit. Then, the testing loss suddenly starts increasing, which means we trained the model for too long. Right when this happens, the model starts overfitting. It's memorizing the training data, it's not generalizing anymore.

A simple technique to fix this issue is [Early stopping](https://articles.bnomial.com/early-stopping). In short, we want to stop training right before the model overfits.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.* Check ["Early stopping"](https://articles.bnomial.com/early-stopping) for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models.</p></details>

-----------------------

## Date - 2023-11-12


## Title - Increasing lambda


### **Question** :

It's time for London to regularize her Linear Regression model. 

She is getting great results on the training data, but her model is overfitting: London's validation error is too high.

But London doesn't know exactly how to use the lambda (λ) parameter that controls the regularization on her model, so she decided to increase it and see what happens.

**Which of the following will eventually happen as London continuously increases the value of λ?**


### **Choices** :

- London's model will eventually underfit, and the validation error will increase.
- London's model will eventually overfit, and the validation error will increase.
- London's model will eventually underfit, and the validation error will decrease.
- London's model will eventually overfit, and the validation error will decrease.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>London can control the model's regularization using the lambda (λ) parameter. She can tune this parameter to decide how much she wants to penalize the model's flexibility. 

Increasing the value of λ will reduce the value of the coefficients, thus reducing the variance of the model. 

Since London's model is overfitting, reducing the variance is a good step. As the value of λ increases, the model will start losing essential information, and its bias will increase, leading to underfitting and a higher validation error.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Regularization in Machine Learning"](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a) for a detailed explanation of how regularization works.* ["Fighting Overfitting With L1 or L2 Regularization: Which One Is Better?"](https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization) will give you a complete introduction to L1 and L2 regularization.</p></details>

-----------------------

## Date - 2023-11-13


## Title - Off-the-shelf encoder


### **Question** :

Marley used an off-the-shelf encoder to process a dataset of images and create embeddings for each. The next step for her is to try and interpret the latent space of her embeddings.

A specific question she has is related to the dimensionality of the space. She can control the encoder's output, so Marley wonders whether a high-dimensional space would be better than a lower-dimensional one.

**Which of the following statements about the dimensionality of a latent space are correct?**


### **Choices** :

- A high-dimensional latent space is more sensitive to specific features from the input object than a low-dimensional latent space.
- A high-dimensional latent space is less sensitive to specific features from the input object than a low-dimensional latent space.
- A high-dimensional latent space is more prone to overfitting than a low-dimensional latent space.
- A low-dimensional latent space is more prone to overfitting than a high-dimensional latent space.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In machine learning, we use "latent space" to refer to a multi-dimensional space containing a meaningful internal representation of objects and where similar points appear closer together. 

Latent spaces usually have a lower dimensionality than the feature space used to draw specific data points. Because of this, projecting an object in a latent space is typically a dimensionality reduction exercise.

The more dimensions in the latent space, the more sensitive it is to specific features from the input objects. In other words, small changes in the input data could cause significant variations in their representation in latent space. This makes high-dimensional spaces more likely to overfit than low-dimensional spaces.

On the other hand, the lower the latent space's dimensionality, the less sensitive to small changes in the input data. The more the encoder compresses the data, the fewer details will make it into the latent space. Low-dimensional spaces capture the essential features of the input data and are more robust to overfitting.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check the [Latent space's Wikipedia page](https://en.wikipedia.org/wiki/Latent_space) for more information.</p></details>

-----------------------

## Date - 2023-11-14


## Title - Grid competition


### **Question** :

Sutton wanted to win the competition. She knew the only path forward was to dedicate as much time as possible to tune her model and squeeze as much performance as possible from it.

Sutton used a library to tune her model's hyperparameters on the test set. She used Grid Search to try to cover as many combinations as possible and chose the hyperparameters that gave her the lowest error.

**What do you think about Sutton's chances of winning the competition?**


### **Choices** :

- Sutton's approach gives her a fighting chance.
- Sutton's approach will produce a model that fails to run.
- Sutton's approach will likely result in an overfit model.
- We need more information to determine whether Sutton has a chance of winning the competition.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Sutton used the test set to choose the best hyperparameters, which could lead to a model that overfits the testing dataset. 

This model will have trouble with unseen data because Sutton optimized it to work well on the test dataset. Instead, she should have used a validation set for hyperparameter tuning.

The test set is helpful for a final evaluation of the model, but anytime we use it to make changes to the model, we risk overfitting it.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Overview of hyperparameter tuning"](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview) is a great introduction to hyperparameters and the process of finding their optimal value.* ["Train, Validation, Test Split for Machine Learning"](https://blog.roboflow.com/train-test-split/) goes into detail about the importance of each split.</p></details>

-----------------------

## Date - 2023-11-15


## Title - The model's f1-score


### **Question** :

A team built a binary classification model. They named the classes `A` and `B`.

After finishing training, they evaluated the model on a validation set, and here is the confusion matrix with the results:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186248358-1b39a042-5725-408b-84aa-8dd915a6d99d.jpg)

**Given the above confusion matrix, what is the f1-score of this binary classification model at predicting class `A`?**


### **Choices** :

- The f1-score of the model at predicting class `A` is 52%.
- The f1-score of the model at predicting class `A` is 80%.
- The f1-score of the model at predicting class `A` is 84%.
- The f1-score of the model at predicting class `A` is 88%.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To compute the model's f1-score at predicting class `A`, we can use the following formula:

```
f1-score = 2 * (precision * recall) / (precision + recall)
```

We can simplify this formula in the following way:

```
f1-score = TP / (TP + 1/2 * (FP + FN))
```

In this example, we have 52 true positives, 13 false positives, and 7 false negative samples. Substituting these values in our formula:

```
f1-score = TP / (TP + 1/2 * (FP + FN))
f1-score = 52 / (52 + 1/2 * (13 + 7))
f1-score = 52 / (52 + 1/2 * 20)
f1-score = 52 / (52 + 10)
f1-score = 52 / 62
f1-score = 0.84
```

Therefore, the model's f1-score at predicting class `A` is 84%.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.</p></details>

-----------------------

## Date - 2023-11-16


## Title - Loss up and down


### **Question** :

The worst thing that could happen to Thea is happening: she is training a neural network, and her training loss goes up and down and never settles:

![Learning Curves](https://user-images.githubusercontent.com/1126730/191616550-37c5cea8-1bc6-4008-b30e-ec9eeceb9c86.jpg)

Up to this point, she is using mini-batch gradient descent to train the network. She has talked to people and collected some feedback. She put it all together in the list below.

**Which of the following will help Thea solve the problem?**


### **Choices** :

- Use a lower learning rate.
- Use a higher learning rate.
- Decrease the batch size to train the network.
- Increase the batch size to train the network.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Increasing the learning rate will allow the model to take more significant steps in the gradient's direction, but it may miss the local minima. If it does, it will have to go back, and the same will happen again. A learning rate that's too high could be causing the oscillation, so Thea shouldn't keep increasing it. Conversely, reducing the learning rate is an excellent next step that could solve the problem.

Thea is using mini-batch gradient descent to train her model. The more she decreases the batch size, the more every sample will influence the shape of the loss. In a small batch, one instance could swing the loss significantly. Therefore, Thea should consider increasing the batch size to smooth the loss.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["The wrong batch size is all it takes" ](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) explains how different batch sizes influence the training process of neural networks using gradient descent.* ["What could an oscillating training loss curve represent?"](https://ai.stackexchange.com/questions/14079/what-could-an-oscillating-training-loss-curve-represent) is a StackExchange question that will help you answer this question.* Check ["Why is my training loss fluctuating?"](https://www.researchgate.net/post/Why_is_my_training_loss_fluctuating) for another set of answers covering this problem.</p></details>

-----------------------

## Date - 2023-11-17


## Title - Compact activation function


### **Question** :

Here is a simple and compact implementation of a neural network in Python:

![Neural network](https://user-images.githubusercontent.com/1126730/196750241-0a53d7bf-d821-43e9-bf5c-560e6ca842d4.png)

**Which of the following is the activation function used in this network?**


### **Choices** :

- This implementation uses a ReLU activation function.
- This implementation uses a sigmoid activation function.
- This implementation uses a softmax activation function.
- This implementation doesn't use an activation function.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The code defines the output layer in Line 9. 

To compute the output of this second layer, we need to multiply the second set of weights (`W2`) by the output of the previous layer (`layer1`). Then we wrap this result with the activation function.

In this case, the output layer uses a sigmoid activation function:

![Sigmoid](https://user-images.githubusercontent.com/1126730/196755518-311dd425-676e-4c85-be1f-467694879c30.jpg)</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Neural Networks and Deep Learning_](http://neuralnetworksanddeeplearning.com/index.html) is a free online book written by [Michael Nielsen](https://twitter.com/michael_nielsen) with a great introduction to neural networks.</p></details>

-----------------------

## Date - 2023-11-18


## Title - Sparse labels


### **Question** :

Evie was reading the code in an [online article](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) and noticed that the author used Keras' `SparseCategoricalCrossentropy` as the loss function to train the neural network:

```
model.compile(
    optimizer=SGD(learning_rate=0.01), 
    loss="sparse_categorical_crossentropy", 
    metrics=["accuracy"]
)
```

Evie was familiar with the general concept, but she wasn't sure how this was different from the regular `CategoricalCrossentropy.`

**When should you use `SparseCategoricalCrossentropy` instead of `CategoricalCrossentropy`?**


### **Choices** :

- When the labels in the dataset are integer values.
- When the labels in the dataset are one-hot encoded.
- When the labels in the dataset are categorical values.
- When the labels in the dataset have a lot of sparse values.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Keras' `SparseCategoricalCrossentropy` computes the cross-entropy loss between the labels and predictions. It works when the labels in the dataset are integer values, for example, `1`, `2`, and `3`.

Keras' `CategoricalCrossentropy`, on the other hand, has the same function but works when the labels in the dataset are one-hot encoded, for example, `[1, 0, 0]`, `[0, 1, 0]`, and `[0, 0, 1]`.

The [article](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) that Evie saw generates a random dataset and assigns an integer to each of the three classes. That's the reason the network uses a `sparse_categorical_crossentropy` loss.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Here is Keras' [SparseCategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) and [CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) documentation.* ["The wrong batch size is all it takes"](https://articles.bnomial.com/the-wrong-batch-size-is-all-it-takes) is the article that inspired this question.</p></details>

-----------------------

## Date - 2023-11-19


## Title - False Positives


### **Question** :

Here is the picture of a confusion matrix we generated after evaluating a model in a dataset with 100 samples:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186962046-7c076163-5b58-442f-9641-c09f5cf71003.jpg)

Assume that class A represents the outcomes of the model we are interested in finding.

**What's the total of False Positives on this evaluation round?**


### **Choices** :

- False Positives are class A samples the model predicted as class B, so the answer is 7.
- False Positives are class B samples the model predicted as class A, so the answer is 13.
- False Positives are class A samples the model predicted as class A, so the answer is 52.
- False Positives are class B samples the model predicted as class B, so the answer is 28.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We are interested in samples from class A which means we will treat class A as our "Positive" samples and class B as our "Negative" samples.

If we replace the classes in the confusion matrix with Positive and Negative instead of "A" and "B," it's much easier to reason about the model's number of False Positives:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186963826-ec29e2b5-c065-4569-9542-712acab129da.jpg)

False Positives are those samples that we expect to be Negative (class B), but the model predicted as Positive (class A.) Therefore, the correct answer to the question is 13. You can see every combination in the image above.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2023-11-20


## Title - Fastest hit


### **Question** :

Freya finished her first trial competition for the Olympics. Her hit was the fastest she's ever run!

After finishing every hit, the judges computed the final scores. Freya's trainer mentioned that the mode time was 25 seconds!

**Which of the following is the correct interpretation of the trainer's comment?**


### **Choices** :

- None of the runners ran faster than 25 seconds.
- Anyone who ran in 26 seconds was below average.
- More runners ran in 25 seconds than any other time.
- Anyone who ran in 24 seconds was above average.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The mean, median, and mode are different ways to measure the center of a dataset. Each of these metrics tries to summarize a set of values using a single number.

The mode of a dataset represents the most frequent number. The correct interpretation of Freya's trainer comment is that 25 seconds was the most common time among all runners.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Mean Median Mode: What They Are, How to Find Them"](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/) for a complete explanation of the Mean, Median, and Mode.</p></details>

-----------------------

## Date - 2023-11-21


## Title - Nuanced conversations


### **Question** :

Twitter is not a great place to have nuanced discussions.

I don't think anyone was surprised to see people picking apart Journey's tweet.

She tried to explain Linear Regression in less than 280 characters and, unsurprisingly, had to leave many details out.

But not every comment she received was correct. Turns out that you can't listen to every person online!

**Which of the following comments are factual statements about Linear Regression?**


### **Choices** :

- Linear Regression is an Unsupervised Learning technique useful for solving Regression problems.
- You can use Linear Regression to predict a continuous dependent variable with the help of independent variables.
- Linear Regression aims to find the best sigmoid curve that can accurately predict the output for the continuous dependent variable.
- In Linear Regression, the relationship between the dependent and independent variables must be linear.


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Linear Regression is probably the most popular Supervised Learning technique in machine learning. Its goal is to fit the best line through the data to predict a continuous output. 

The algorithm uses a set of independent variables to predict a continuous dependent variable. For example, a person's age, salary, or home price. 

Finally, for Linear Regression to work, we must ensure that the relationship between the inputs and the output is linear. A Linear Regression model won't give us good predictions if the relationship isn't linear. Sometimes, this condition means we must transform the input features before using Linear Regression. For example, if you have a variable with an exponential relationship with the target variable, you can use log transform to turn the relationship linear.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Linear Regression for Machine Learning"](https://machinelearningmastery.com/linear-regression-for-machine-learning/) is an introduction to Linear Regression.</p></details>

-----------------------

## Date - 2023-11-22


## Title - Same distribution


### **Question** :

Kehlani suspected that her training and test data didn't come from the same distribution.

To prove it, she mixed all the data, removed the target variable, and added a new binary target that contained a value of 1 for each training sample and a value of 0 for each test sample.

She then trained a new binary classification model using this new dataset to see whether she could separate the samples based on whether they belonged to the training or test sets.

Kehlani used a ROC curve to evaluate the results of the model. She looked at the area under the curve to make her final determination.

**Based on Kehlani's strategy, which of the following is the correct way to evaluate her model?**


### **Choices** :

- If the area under the curve is close to 1.0, the training and test samples come from the same distribution, and if it's close to 0.5, they come from different distributions.
- If the area under the curve is close to 0.5, the training and test samples come from the same distribution, and if it's close to 1.0, they come from different distributions.
- If the area under the curve is close to 1.0, the training and test samples come from the same distribution, and if it's close to 0.0, they come from different distributions.
- The area under the curve is not a good metric to determine whether this new model performs accurately.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Kehlani's strategy is called ["Adversarial validation"](https://essays.bnomial.com/adversarial-validation), and it's a clever technique to determine whether your training and test data come from the same distribution.

After she created the new model, she decided to use a ROC curve and evaluate the results based on the area under the curve (AUC.) This metric measures the area underneath the ROC curve, which ranges between 0.0 and 1.0.

The more accurate the model results are, the higher the area under the curve will be. Therefore, if the model can tell the training and test samples apart, the closer the AUC will be to 1.0, and Kehlani can conclude that both sets don't come from the same distribution.

Conversely, the closer the model performs to random chance, the worse it is at telling training samples apart from test samples. In this case, Kehlani will see the AUC close to 0.5, which means that both sets come from the same distribution.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Adversarial validation"](https://articles.bnomial.com/adversarial-validation) for a quick introduction to this technique.* Check ["Classification: ROC Curve and AUC"](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) for an explanation of how to create and interpret a ROC curve.</p></details>

-----------------------

## Date - 2023-11-23


## Title - Something wrong


### **Question** :

"There's something wrong with your network."

That was the start of a message Camille received from her friend. She posted the plot of a neural network training loss online, and Camille's friend reached out to let her know.

Camille is using gradient descent for the first time, so she appreciated the help. 

"Do you see what happened around the ninety epoch? The loss increases for a moment before coming back down again until the end. You don't want that,"—concluded the message.

**What would you do if you were in Camille's shoes?**


### **Choices** :

- Camille should decrease the learning rate. That should stop the loss from increasing during training.
- Camille should increase the learning rate. That should stop the loss from increasing during training.
- Camille should use Early Stopping at around the ninety epoch.
- Camille shouldn't do anything because her network has no problem.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Gradient descent moves downhill on average, so a network that learns appropriately should see the loss decrease over the training session. However, individual updates can move in the opposite direction, causing the loss to fluctuate up and down.

Camille's plot shows the loss increasing momentarily, but it immediately starts decreasing. That's normal, and Camille shouldn't worry about it.

Since the training process seems to be working correctly, modifying the learning loss might improve the results, but there's nothing Camille needs to fix. If she uses [Early Stopping](https://articles.bnomial.com/early-stopping), she will prevent the network from improving further.

In summary, Camille shouldn't do anything at this point.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["An overview of gradient descent optimization algorithms"](https://www.ruder.io/optimizing-gradient-descent/) for a deep dive into gradient descent and every one of its variants.* Check ["Early Stopping"](https://articles.bnomial.com/early-stopping) for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models.</p></details>

-----------------------

## Date - 2023-11-24


## Title - KNN's runtime


### **Question** :

Evangeline is working with a dataset with a single column of data. 

She wants to run k-Nearest Neighbors (KNN), but only if the algorithm is fast enough when making predictions. Evangeline doesn't have any control over how the data is stored.

**Assuming there are _n_ samples in the dataset, which of the following will be the runtime of Evangeline's KNN at prediction time?**


### **Choices** :

- The runtime that Evangeline should expect is _O(1)_
- The runtime that Evangeline should expect is _O(n)_
- The runtime that Evangeline should expect is _O(log n)_
- The runtime that Evangeline should expect is _O(n²)_


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>k-Nearest Neighbors' runtime is _O(nd)_ because we need to compute the distance to each feature of every sample. Here _n_ represents the number of instances, and _d_ the number of features. 

Evangeline is working with a single feature, so _d_ = 1. Therefore, in this case, the runtime of KNN is _O(n)_.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Here is a [Stack Exchange answer](https://stats.stackexchange.com/q/219664) that covers KNN's runtime complexity in detail.</p></details>

-----------------------

## Date - 2023-11-25


## Title - Latest project


### **Question** :

Blakely is a data scientist who has been working on machine learning models. She has been particularly focused on using activation functions in neural networks for her latest project.

Blakely has been exploring the Sigmoid activation function, commonly used in binary classification problems. The Sigmoid function is a smooth, S-shaped function that maps any real-valued number to a value between 0 and 1.

**Which of the following is true about the Sigmoid Activation Function?**


### **Choices** :

- The function is neither continuous nor differentiable.
- The function is differentiable but not continuous.
- The function is continuous but not differentiable.
- The function is both continuous and differentiable.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The Sigmoid function is continuous and differentiable.

The Sigmoid function is smooth and differentiable everywhere, including at `x=0`. This is one of the reasons why it was widely used in the early days of neural networks. 

However, the Sigmoid function has some drawbacks, such as the vanishing gradient problem, which can slow the learning process. This is one of the reasons why other activation functions, like ReLU, have become more popular in recent years.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["A Gentle Introduction To Sigmoid Function"](https://machinelearningmastery.com/a-gentle-introduction-to-sigmoid-function/) for a quick introduction to Sigmoid.</p></details>

-----------------------

## Date - 2023-11-26


## Title - A few achievements


### **Question** :

We have seen Deep Learning become a force over the last two decades.

A few critical algorithmic improvements allowed the community to start training deep neural networks reliably.

**Which of the following are some achievements we have made since the early 2000s?**


### **Choices** :

- We proposed the idea of neural networks to replace multi-layer perceptrons.
- We created better activation functions for neural layers
- We came up with better weight-initialization schemes
- We created better optimization schemes


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In 1944, Warren McCulloch and Walter Pitts proposed the idea of neural networks, so they have been around for quite some time. But it wasn't until the late 2000s when the community started making significant improvements that enabled the popularity of Deep Learning. 

In [Deep Learning with Python, Second Edition](https://amzn.to/3K3VZoy), François Chollet writes referring to the history of Deep Learning:

> This changed around 2009-2010 with the advent of several simple but important algorithmic improvements (...):
>
> * Better activation functions for neural layers
> * Better weight-initialization schemes
> * Better optimization schemes

These were some of the algorithm improvements that allowed the community to start using deep learning.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check [_Deep Learning with Python, Second Edition_](https://amzn.to/3K3VZoy) for a great introduction to Deep Learning.</p></details>

-----------------------

## Date - 2023-11-27


## Title - Denoising images


### **Question** :

Olive is playing with the MNIST dataset of handwritten digits.

She decided to build a denoising autoencoder. To train it, she takes every image, generates a copy with random salt and pepper noise, and uses it as the input to the autoencoder. She compares the output of the network with the clean image to compute the reconstruction error.

**Which category of learning better represents Olive's autoencoder?**


### **Choices** :

- This autoencoder is an example of a supervised learning algorithm.
- This autoencoder is an example of a semi-supervised learning algorithm.
- This autoencoder is an example of a self-supervised learning algorithm.
- This autoencoder is an example of a simple-supervised learning algorithm.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>One of the applications of [Autoencoders](https://essays.bnomial.com/autoencoders) is removing noise from images. 

An autoencoder is a neural network that we can split into three sections: an encoder, a bottleneck, and a decoder. The encoder compresses the original input into an intermediate representation, and the decoder reverses the process to reconstruct the original data. The bottleneck sits between the encoder and decoder and is the section that stores the compressed representation of the data. 

In this example, Olivia is training the autoencoder without needing to label the dataset manually. The input to the network is an image with autogenerated noise, and the "label" is the same clean image. The lack of manual annotations pushes this autoencoder outside of the supervised-learning domain.

Semi-supervised learning problems require at least a few annotations we use to train and decide on what other annotations we should procure. Simple-supervised learning is not an existing learning approach.

We can classify this particular autoencoder as self-supervised learning. Notice that the literature also classifies autoencoders as unsupervised, which is also correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Autoencoders"](https://articles.bnomial.com/autoencoders) for an introduction to a learning technique to represent data efficiently using neural networks.* Read the accepted answer under ["What is self-supervised learning in machine learning?"](https://ai.stackexchange.com/questions/10623/what-is-self-supervised-learning-in-machine-learning) Stack Exchange question.</p></details>

-----------------------

## Date - 2023-11-28


## Title - Porting dropouts


### **Question** :

Valentina is porting her PyTorch code over to TensorFlow. 

Her new company bought Valentina's startup, and the first order of business was to migrate the code base and integrate it into the new company's ecosystem.

The deep learning model they built uses Dropout on a few hidden layers. Valentina knows that both TensorFlow's and Pytorch's implementations are the same: they zero out some of the nodes and scale the remaining. However, she doesn't remember exactly how these work.

**Assuming Valentina is looking at a Dropout with a rate of `0.2`, which of the following are correct?**


### **Choices** :

- A Dropout rate of `0.2` will set 80% of the nodes to zero.
- A Dropout rate of `0.2` will set every node to zero with a probability of 20%.
- A Dropout rate of `0.2` will scale the value of every remaining node—those not set to zero—by multiplying them by `1.25`.
- A Dropout rate of `0.2` will scale the value of every remaining node—those not set to zero—by multiplying them by `0.2`.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Dropout is a regularization method that works well and is vital for reducing overfitting.

Sometimes, the nodes in a neural network create strong dependencies on other nodes, which may lead to overfitting. An example is when a few nodes on a layer do most of the work, and the network ignores all the other nodes. Despite having many nodes on the layer, you only have a small percentage of those nodes contributing to predictions. We call this phenomenon "[co-adaptation](https://machinelearning.wtf/terms/co-adaptation/)," and we can tackle it using Dropout.

During training, Dropout randomly removes a percentage of the nodes, forcing the network to learn in a balanced way. Now every node is on its own and can't rely on other nodes to do their work. They have to work harder by themselves. 

Valentina is right. Both [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) and [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) use the same implementation of Dropout that follows this process:

1. It zeros out every node of the layer with the specified probability. In this case, a Dropout with a `0.2` rate will zero out every node with a 20% probability.
2. It scales the remaining nodes to account for the missing values. The scaling factor is `1/(1-rate)`. In this case, if we can substitute the rate by `0.2`, we get that it will scale nodes by `1.25`.

Therefore, the second and third choices answer this question correctly.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For more information about co-adaptation and how to use Dropout, check ["Improving neural networks by preventingco-adaptation of feature detectors"](https://arxiv.org/pdf/1207.0580.pdf).* ["A Gentle Introduction to Dropout for Regularizing Deep Neural Networks"](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) is an excellent introduction to Dropout.* Here is [TensorFlow's implementation of Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout).* Here is [PyTorch's implementation of Droput](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html).</p></details>

-----------------------

## Date - 2023-11-29


## Title - Three gradients


### **Question** :

Here is a chart showing the gradient of three different functions:

![Gradients](https://user-images.githubusercontent.com/1126730/198387377-cee47720-8729-409f-a119-7280d547f73d.jpg)

These gradients correspond to Sigmoid, Tanh, and ReLU, but we don't know exactly the order.

**Which of the following correctly identifies the three functions?**


### **Choices** :

- Function 1 is the gradient of Tanh, function 2 is the gradient of Sigmoid, and function 3 is the gradient of ReLU.
- Function 1 is the gradient of Sigmoid, function 2 is the gradient of Tanh, and function 3 is the gradient of ReLU.
- Function 1 is the gradient of ReLU, function 2 is the gradient of Tanh, and function 3 is the gradient of Sigmoid.
- Function 1 is the gradient of Sigmoid, function 2 is the gradient of ReLU, and function 3 is the gradient of Tanh.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Here is the Python code you can use to plot the gradient of the three functions:

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def deriv_sigmoid(x):
    return sigmoid(x) * (1 - sigmoid(x))

def tanh(x):
    return 2 / (1 + np.exp(-2 * x)) - 1

def deriv_tanh(x):
    return 1 - tanh(x) ** 2

def deriv_relu(x):
    return (x > 0) * 1

x = np.arange(-5., 5., 0.2)
fn1 = deriv_tanh(x)
fn2 = deriv_sigmoid(x)
fn3 = deriv_relu(x)

plt.figure(figsize=(8, 6), dpi=80)

plt.plot(x, fn1, label = "function 1")
plt.plot(x, fn2, label = "function 2")
plt.plot(x, fn3, label = "function 3")

plt.legend()
plt.show()
```

As you see, function 1 is the gradient of Tanh, function 2 is the gradient of Sigmoid, and function 3 is the gradient of ReLU.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For a complete description of the Sigmoid function, check the ["Logistic function"](https://en.wikipedia.org/wiki/Logistic_function) Wikipedia page.* For a complete description of the Tanh function, check the ["Hyperbolic Functions"](https://en.wikipedia.org/wiki/Hyperbolic_functions) Wikipedia page.* For a complete description of the ReLU function, check the ["Rectifier (neural networks)"](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) Wikipedia page.</p></details>

-----------------------

## Date - 2023-11-30


## Title - Dataset of applicants


### **Question** :

Annabelle wanted to understand who was applying to her company's open jobs.

What do these people have in common? 

Annabelle had access to the entire dataset of applicants and had plenty of information about them.

**What would be your recommendation for Annabelle?**


### **Choices** :

- Annabelle should use a supervised learning algorithm.
- Annabelle should use an unsupervised learning algorithm.
- Annabelle should use a reinforcement learning algorithm.
- Annabelle should use a semi-supervised learning algorithm.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Annabelle doesn't know what characteristics the applicants share, so a clustering algorithm should be a good initial step.

For example, she could use [K-Means](https://en.wikipedia.org/wiki/K-means_clustering) to find interesting patterns and group the applicants that share them. A critical distinction is that you don't need to consider these groups preemptively; the clustering algorithm will find them for you. 

Clustering algorithms are part of unsupervised learning, so the second choice is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Customer Segmentation with Machine Learning"](https://towardsdatascience.com/customer-segmentation-with-machine-learning-a0ac8c3d4d84) for a quick introduction to Customer Segmentation.* ["10 Clustering Algorithms With Python"](https://machinelearningmastery.com/clustering-algorithms-with-python/) will introduce you to 10 different clustering algorithms.</p></details>

-----------------------

## Date - 2023-12-01


## Title - The model's precision


### **Question** :

A team built a binary classification model. They named the classes `A` and `B`.

After finishing training, they evaluated the model on a validation set, and here is the confusion matrix with the results:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186248358-1b39a042-5725-408b-84aa-8dd915a6d99d.jpg)

**Given the above confusion matrix, what is the precision of this binary classification model at predicting class `B`?**


### **Choices** :

- The precision of the model at predicting class `B` is 28%.
- The precision of the model at predicting class `B` is 52%.
- The precision of the model at predicting class `B` is 80%.
- The precision of the model at predicting class `B` is 88%.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To compute the model's precision at predicting class `B`, we can use the following formula:

```
precision = TP / (TP + FP)
```

In this example, we have 28 true positive samples and 7 false positive samples. Substituting these values in our formula:

```
precision = TP / (TP + FP)
precision = 28 / (28 + 7)
precision = 28 / 35
precision = 0.8
```

Therefore, the model's precision at predicting class `B` is 80%.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.</p></details>

-----------------------

## Date - 2023-12-02


## Title - Every city


### **Question** :

Camilla was feeling the Curse of Dimensionality in her bones.

Her dataset had a column representing each city in the U.S. It was an important feature, and she couldn't eliminate it. Camilla needed that feature, but one-hot encoding produced thousands of new columns, making building her binary classifier with that dataset cumbersome.

Fortunately, she found a way around her issue: she replaced each column's category with the posterior probability of the target being positive on the presence of that category.

And it worked! 

**What's the name of this encoding technique?**


### **Choices** :

- Camilla used Label encoding.
- Camilla used Ordinal encoding.
- Camilla used Target encoding.
- Camilla used Posterior encoding.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Label encoding replaces each category with a consecutive number starting from 0. It's useful when the order of the categories doesn't matter. On the other hand, Ordinal encoding works similarly to Label encoding, but we use it when the order of categories matters. Camilla didn't use any of these two encoding techniques. 

Target encoding is another technique that helps process categorical features with high cardinality. In a dataset where the target is binary, you replace each category with the posterior probability of the target value being positive or 1.

Finally, Posterior encoding is not an encoding technique at the time of this writing.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["6 Ways to Encode Features for Machine Learning Algorithms"](https://towardsdatascience.com/6-ways-to-encode-features-for-machine-learning-algorithms-21593f6238b0) to see some of these encoding techniques in action.* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.* [_Feature Engineering for Machine Learning_](https://amzn.to/3SsnLAc) is an excellent book covering feature engineering.</p></details>

-----------------------

## Date - 2023-12-03


## Title - High training and testing


### **Question** :

There's not a lot of context for you other than the following chart showing the training and testing losses of a machine learning model:

![Training and Testing Loss Chart](https://user-images.githubusercontent.com/1126730/188513012-1ec37e8e-8c1d-46b9-bff7-d01bedf269a0.jpg)

As you can see, after finishing training, both losses are high.

**What's a reasonable conclusion about this machine learning model?**


### **Choices** :

- Your model is overfitting.
- Your model is underfitting.
- Your model is either overfitting or underfitting, but you can't tell.
- Your model is working fine.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A good model should capture valuable patterns in the data and discard any noise that doesn't help with predictions. An overfitting model will fit that noise. An underfitting model will not capture the relevant patterns in the dataset. 

An overfitting model should not have any problems with the training data but stumble with the testing data. Therefore, we should expect a low training loss and a high testing loss. An underfitting model should struggle with the training and testing datasets, so both of its losses should be high.

This model shows high training and testing losses, which we expect from an underfitting model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.</p></details>

-----------------------

## Date - 2023-12-04


## Title - Confusion Matrix


### **Question** :

Most people summarize the performance of their model using a single high-level metric. For example, accuracy is a popular way to explain how the model is doing.

While helpful, this doesn't give us enough information about the quality of the predictions and the mistakes the model makes.

A confusion matrix is a tool we can use to zoom into a model and surface important information.

**Which of the following sentences is true about a confusion matrix?**


### **Choices** :

- A confusion matrix helps analyze the performance of a binary classification model, but it doesn't work for multi-class classification models.
- A confusion matrix helps analyze the performance of multi-class classification models, and it doesn't work for binary classification models.
- A confusion matrix helps analyze the performance of classification models, including binary and multi-class models.
- A confusion matrix helps analyze the performance of a regression model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A confusion matrix is one of the simplest and most popular tools to analyze the performance of a classification model. It breaks down each class and the number of correct and incorrect predictions the model makes. It gives us immediate access to the model's errors and their type.

We use confusion matrices with binary and multi-class classification models, but we don't use them in regression problems.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2023-12-05


## Title - Live audience


### **Question** :

Piper started building a simple neural network from scratch in front of a live audience.

Her talk focused on the fundamental principles of neural networks, and writing the code wasn't too hard.

But there was a problem, and it took Piper an eternity to find and fix it: she forgot to initialize the network weights, so they were all zeroes.

**Which of the following was the clue for Piper to realize there was a problem?**


### **Choices** :

- The network learned, but it took a significantly long time.
- The network learned, but it started overfitting quickly.
- The network's loss started oscillating up and down.
- The network neurons evolved symmetrically, learning the same features.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Correctly initializing a neural network can have a significant impact on convergence.

Initializing the network weights with zeros will lead to every neuron learning the same features. Here is an excerpt from ["Initializing neural networks"](https://www.deeplearning.ai/ai-notes/initialization/) explaining the consequences of using the same weight values:

> Thus, both hidden units will have an identical influence on the cost, leading to identical gradients. Thus, both neurons will evolve symmetrically throughout training, effectively preventing different neurons from learning different things.

Since none of the other choices are possible, this symmetry tipped Piper about the problem.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Initializing neural networks"](https://www.deeplearning.ai/ai-notes/initialization/) is an excellent summary of the importance of weight initialization.* Check ["Weight Initialization Techniques in Neural Networks"](https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78) to learn about different initialization schemes.</p></details>

-----------------------

## Date - 2023-12-06


## Title - Halting training


### **Question** :

Kim is recording a YouTube video where she wants to explain how early stopping works when training a neural network.

The fundamental insight she wants her audience to understand is how the technique decides the exact moment it needs to halt the training process.

**Assuming that Kim will use the validation loss as the metric to watch, which of the following is the correct way to configure early stopping?**


### **Choices** :

- The training process should stop once the validation loss has increased for several consecutive iterations.
- The training process should stop once the validation loss has decreased for several successive iterations.
- The training process should stop once the validation loss hits its maximum value.
- The training process should stop once the validation loss decreases from its immediate previous value.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Kim is using the validation loss of the model to explain how [early stopping](https://articles.bnomial.com/early-stopping) works. When training a model, its validation loss will decrease as the model learns, but it will increase as soon as it starts overfitting. The goal when using early stopping is to catch this reversal and halt the training process.

The second and third choices argue about the trigger happening when the validation loss decreases, but that's the opposite of what we want. While the validation loss drops, everything is fine, and we should let the model train. When the validation loss hits its maximum value, it's too late to stop the training process, so the second choice is also incorrect. 

The correct answer is the first choice: early stopping should halt the training process as soon as the validation loss has increased for several consecutive iterations.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check ["Early stopping"](https://articles.bnomial.com/early-stopping) for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models.</p></details>

-----------------------

## Date - 2023-12-07


## Title - Amazon's warehouse


### **Question** :

Faith works in an Amazon warehouse. 

Every day, thousands of packages leave the facility to their destination, and Faith has been working on a few machine learning models to predict workload over time.

When building one of the models, she found out that her dataset had too many categorical features. Faith doesn't want to deal with a sparse dataset, so she needs your help to select the appropriate way to encode those columns.

**How can Faith encode the categorical columns and avoid getting a sparse dataset?**


### **Choices** :

- Faith should use One-Hot encoding.
- Faith should use Target encoding.
- Faith should leave the categorical features as they are.
- Faith should remove every categorical feature from the dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Faith cannot leave the features as they are because many different machine learning models can't process categorical features directly. She can't remove them because she will lose all their predictive capabilities.

Faith has to encode these features.

One-Hot encoding creates a new feature for each unique value of the original categorical variable. For example, a "weather" feature with three values will get Faith three new features, one for each value of the original "weather" column. Unfortunately, using One-Hot encoding with many categorical features will result in a sparse dataset.

Target encoding is another technique that helps process categorical features with high cardinality. Target encoding replaces the categories of a column with the average target value of all data points belonging to that category. In other words, Faith will convert each categorical feature into a numerical one and won't have to deal with a sparse dataset.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["6 Ways to Encode Features for Machine Learning Algorithms"](https://towardsdatascience.com/6-ways-to-encode-features-for-machine-learning-algorithms-21593f6238b0) to see some of these encoding techniques in action.* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.* [_Feature Engineering for Machine Learning_](https://amzn.to/3SsnLAc) is an excellent book covering feature engineering.</p></details>

-----------------------

## Date - 2023-12-08


## Title - Choosing a project


### **Question** :

Elsie wants to practice what she studied about Recurrent Neural Networks (RNN).

She is a practical person, so she wants to choose a project where she can use an RNN. There are several options.

**Which of the following projects do you think Elsie could choose to practice what she learned about RNNs?**


### **Choices** :

- Predict the sales of her company's main product.
- Determining the safety of attachments sent by customers in their email correspondence.
- Transcribing handwritten correspondence from customers.
- Transcribing voicemails from customers.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A [recurrent neural network](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNN) is an artificial neural network that uses sequential or time series data. Their main difference from traditional networks is their ability to take information from prior inputs to influence the current input and output.

Because of the nature of written text and speech, Elsie should pick any of the transcription problems. Handwritten text and speech are sequential, making them ideal for an RNN. RNN models are heavily used in natural language processing and speech recognition. 

Predicting sales and classifying email attachments don't seem like a good application of RNNs.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["What are recurrent neural networks?"](https://www.ibm.com/cloud/learn/recurrent-neural-networks) for a description of what they are and how they work.* ["An Introduction To Recurrent Neural Networks And The Math That Powers Them"](https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/) is a deeper dive into RNNs.</p></details>

-----------------------

## Date - 2023-12-09


## Title - Trigger metric


### **Question** :

Rhea is building a multi-class classification neural network and doesn't have too much time to regularize it.

Instead, she wants to keep her model as unconstrained as possible and use early stopping as her regularization technique. She knows early stopping is simple to implement and very effective.

Rhea needs to decide how to configure it correctly. Specifically, she must decide which metric to watch to trigger early stopping.

**From the list below, what options could Rhea consider to configure early stopping for her model?**


### **Choices** :

- Use the training loss as the metric to watch.
- Use the accuracy on the training set as the metric to watch.
- Use the validation loss as the metric to watch.
- Use the accuracy on the validation set as the metric to watch.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Early stopping](https://articles.bnomial.com/early-stopping) halts the training process as soon as overfitting starts. It watches a pre-configured metric to determine the appropriate time.

Rhea shouldn't use the training set to decide whether her model is overfitting. As she trains for longer, the loss and accuracy on her training set will move continuously in the right direction. Instead, measuring the model's performance on a separate, hold-out validation set is the right approach.

Rhea could use the validation loss to halt the training process. When training a model, its validation loss will decrease as the model learns, but it will increase as soon as it starts overfitting. The model's accuracy could also work as a trigger because it will increase while the model learns but decrease as quickly as it begins overfitting.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check ["Early stopping"](https://articles.bnomial.com/early-stopping) for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models.</p></details>

-----------------------

## Date - 2023-12-10


## Title - Recall is more important


### **Question** :

Here is the Fβ score formula:

![FBeta-Score](https://user-images.githubusercontent.com/1126730/193036770-0a37f6f7-9a5d-4be3-9b89-7018931140eb.jpg)

The Fβ score lets us combine precision and recall into a single metric.

Let's say you want to use this formula to measure a model where a higher recall is more important.

**What's the correct value for the β parameter to achieve this?**


### **Choices** :

- β = 0
- β = 0.5
- β = 1
- β = 2


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The Fβ score lets us combine precision and recall into a single metric. When using β = 1, we place equal weight on precision and recall. For values of β > 1, recall is weighted higher than precision; for values of β < 1, precision is weighted higher than recall.

Therefore, the correct answer to this question is β = 2.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What is the F-Score?"](https://deepai.org/machine-learning-glossary-and-terms/f-score) is a short introduction to this metric.* ["Micro, Macro & Weighted Averages of F1 Score, Clearly Explained"](https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f) is a great article covering how to compute a global F1-Score metric in multi-class classification problem.* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2023-12-11


## Title - Bag of chips


### **Question** :

Amira brought a bag to the event. She wanted people to relax before a long day, so she thought of lighting the mood with a fun exercise.

"There are 15 red chips, 25 blue chips, and only 5 black chips in this bag,"—she told the audience. "If I put my hand inside and pick a chip randomly, what's the probability it's neither red nor black?"

**Select the correct probability from the following options:**


### **Choices** :

- The probability is 3/9
- The probability is 3/25
- The probability is 5/8
- The probability is 5/9


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Amira wants to compute the probability of picking a blue chip since she doesn't want it to be red or black.

There are 45 chips in the bag, 25 of which are blue. Therefore, the probability of picking a blue chip is 25/45, which simplifies down to 5/9.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Cartoon Guide to Statistics_](https://amzn.to/3eg9iIo) is a fun and instructive introduction to probabilities and statistics.</p></details>

-----------------------

## Date - 2023-12-12


## Title - KNN's summary


### **Question** :

Kaia has never worked with k-Nearest Neighbors (KNN) before, but that was her teammates' suggestion. 

She decided to do some research and look into how KNN works.

**Here is the summary she put together. Which of the following are correct statements about KNN?**


### **Choices** :

- KNN is considered an unsupervised learning algorithm.
- KNN stores the entire dataset in memory to make a prediction.
- KNN does not create a predictive model during training time.
- KNN makes predictions on the fly by calculating the similarity between a sample and the observations in the dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>K-Nearest Neighbors (KNN) is a supervised learning algorithm that doesn't create a predictive model from a training dataset to make predictions. In KNN, there's no need for a training phase. Instead, the algorithm computes a prediction during inference time.

KNN uses the entire dataset and looks for a pre-determined number of instances closest to the observation we want to classify to determine to which group the sample belongs. To do this, KNN stores the entire dataset in memory and computes new predictions on the fly.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Understand the Fundamentals of the K-Nearest Neighbors (KNN) Algorithm"](https://heartbeat.comet.ml/understand-the-fundamentals-of-the-k-nearest-neighbors-knn-algorithm-533dc0c2f45a) for an introduction to KNN.</p></details>

-----------------------

## Date - 2023-12-13


## Title - Underlying function


### **Question** :

Harlow has access to a simple dataset with only one numerical feature and a target value.

She wants to build a couple of models to predict the target value using her dataset. She will try two different functions:

* Model A: `y = mx + b`
* Model B: `y = x² + mx + b`

Harlow split her dataset in two: 80% for training both models and 20% for testing them.

**Which model will get the best results on Harlow's testing set?**


### **Choices** :

- Model A will have the best results on the testing set.
- Model B will have the best results on the testing set.
- Both Model A and Model B will have similar results on the testing set.
- We need more information to decide which model will do better.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We don't know anything about the dataset, so we can't decide which model will do better.

For example, if the data comes from a linear model, Model A may have an advantage, while Model B will be better if the data comes from a quadratic function.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Linear Regression for Machine Learning"](https://machinelearningmastery.com/linear-regression-for-machine-learning/) is an introduction to Linear Regression.</p></details>

-----------------------

## Date - 2023-12-14


## Title - Function differences


### **Question** :

Angela needs an activation function for her neural network and wants to decide between Sigmoid and Tanh.

She knows the functions are different but needs a reminder about their specific differences.

**Which of the following are correct statements when comparing Sigmoid and Tanh?**


### **Choices** :

- Sigmoid is an s-shaped-looking function. Tanh is not.
- Around the value 0, Sigmoid has a larger gradient than Tanh.
- Around the value 0, Tanh has a larger gradient than Sigmoid.
- Tanh produces values centered around 0. Sigmoid's output is always positive.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The Sigmoid function takes an input "x" and squeezes it between 0 and 1. When plotted, Sigmoid is an s-shaped curve:

![Sigmoid Plot](https://user-images.githubusercontent.com/1126730/197406367-7a06343e-f5ed-4409-b11d-6ee92eef7171.jpg)

The Tanh function also has a similar-looking s-shape. The function squeezes the input "x" between -1 and 1. Notice how Tanh looks like a stretched and shifted version of Sigmoid:

![Tanh plot](https://user-images.githubusercontent.com/1126730/197406179-9f80cd61-2241-489e-98a7-bd63bf74beba.jpg)

When training neural networks, we use the derivative of the activation function. If we compute the gradient of Sigmoid and Tanh and plot them, we will see the following:

![Gradients](https://user-images.githubusercontent.com/1126730/198387224-79c11611-f666-4df2-9b58-c661fa5900e8.jpg)

Notice how the gradient of Tanh around 0 is around four times larger than the gradient of Sigmoid.

Finally, Tanh produces values centered around 0—between -1 and 1—, while Sigmoid's output is always positive—between 0 and 1.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Activation Functions: Sigmoid vs. Tanh"](https://www.baeldung.com/cs/sigmoid-vs-tanh-functions) for a comparison between these two functions.* For a complete description of the Sigmoid function, check the ["Logistic function"](https://en.wikipedia.org/wiki/Logistic_function) Wikipedia page.* For a complete description of the Tanh function, check the ["Hyperbolic Functions"](https://en.wikipedia.org/wiki/Hyperbolic_functions) Wikipedia page.</p></details>

-----------------------

## Date - 2023-12-15


## Title - Counting leaf nodes


### **Question** :

Myla built a decision tree that counts the number of features in a dataset with the value 1.

The dataset only contains binary features that are either zero or one. Myla's decision tree works with any number of features.

**Assuming a dataset with 8 binary features, how many leaf nodes would Myla's decision tree have?**


### **Choices** :

- Her decision tree needs 8 leaf nodes.
- Her decision tree needs 16 leaf nodes.
- Her decision tree needs 64 leaf nodes.
- Her decision tree needs 256 leaf nodes.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Myla's decision tree will need 256 leaf nodes to work with a dataset of 8 features. As a general rule, for a dataset with _d_ features, Myla will require a decision tree with 2ᵈ leaf nodes.

As an example, imagine we have a dataset with a single feature. The decision tree will need 2 leaf nodes: one to capture the case where the feature has a value of 0 and another to capture when the feature has a value of 1:

![Decision tree with one feature](https://user-images.githubusercontent.com/1126730/197406448-5dda78a5-0011-4525-aa6b-084ed46805a4.jpg)

For 2 features, the possible combinations are four: both features are 0, the first feature is 0 and the second is 1, the first feature is 1 and the second is 0, and both features are 1. The possible resultant count is 0, 1, or 2:

![Decision tree with two features](https://user-images.githubusercontent.com/1126730/197406460-edb7b72f-246f-4c53-8598-d5da4fd7c01c.jpg)

Generally, for _d_ features, the decision tree will need 2ᵈ leaf nodes.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Decision tree learning"](https://en.wikipedia.org/wiki/Decision_tree_learning) is the Wikipedia page where you can read about decision trees.- ["How to tune a Decision Tree?"](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680) is a great article talking about different ways to tune a decision tree, including the effects of setting the maximum depth hyperparameter.</p></details>

-----------------------

## Date - 2023-12-16


## Title - Hard to read


### **Question** :

Zara tried to read Stable Diffusion's paper but realized there were too many terms that she didn't understand.

Early on, she stumbled upon the idea of a "latent space." Zara had never heard about that before, so she decided to do some research.

By the end of the day, Zara had a good idea of what was a latent space and wrote a summary with her understanding.

**Which of the following statements are correct regarding the concept of a latent space as we use it in machine learning?**


### **Choices** :

- We can find similar items nearby in latent space.
- Latent spaces are always linear, which makes them easier to interpret.
- The distance between objects in latent space lacks physical units, so every application needs to interpret these values.
- Usually, the data points we project in the latent space have higher dimensionality than the space itself.


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In machine learning, we use "latent space" to refer to a multi-dimensional space containing a meaningful internal representation of objects and where similar points appear closer together. 

It's common for a latent space to be high-dimensional and nonlinear. There are no predefined units to measure the distance between data points in latent space, so interpreting the contents of a latent space is an application-specific and complex task. 

Finally, latent spaces usually have a lower dimensionality than the feature space used to draw specific data points. Because of this, projecting an object in a latent space is typically a dimensionality reduction exercise.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check the [Latent space's Wikipedia page](https://en.wikipedia.org/wiki/Latent_space) for more information.</p></details>

-----------------------

## Date - 2023-12-17


## Title - Leadership meeting


### **Question** :

Annie received the email from her data science manager early in the morning. She needs to show up to the leadership meeting with the results of the latest iteration of their model but trying to decipher technical jargon was never her strong suit.

Her manager sent her a picture of the confusion matrix the team created after validating the model:

![Confusion matrix](https://user-images.githubusercontent.com/1126730/186957993-9bf7a320-0c86-455e-bd42-6a67777beaee.jpg)

The sum of every element in the confusion matrix is 190, but Annie doesn't have much experience here.

**What is the sum of the elements in a confusion matrix?**


### **Choices** :

- The sum of the elements in a confusion matrix always equals the number of correct predictions the model made on the validation set.
- The sum of the elements in a confusion matrix always equals the number of mistakes the model made on the validation set.
- The sum of the elements in a confusion matrix always equals the number of samples of the training set.
- The sum of the elements in a confusion matrix always equals the number of samples of the validation set.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A [confusion matrix](https://articles.bnomial.com/confusion-matrix) is one of the simplest and most popular tools to analyze the performance of a classification model. It breaks down each class and the number of correct and incorrect predictions the model makes. It gives us immediate access to the model's errors and their type.

The sum of every element in a confusion matrix gives us the number of samples in the dataset we used to create the matrix. Since the team built it during the validation process, we can assume it represents the validation set.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.</p></details>

-----------------------

## Date - 2023-12-18


## Title - Too many features


### **Question** :

Rowan's team set up a plan to build a new model. 

Since they had too many features, the first step was taking the entire dataset and performing feature selection. Then, they split the dataset in two: 80% for training the model and 20% for testing it. 

Finally, the team trained a Decision Tree on the training set and used the test data to find the best hyperparameter values for the model.

**Which of the following statements are true regarding this situation?**


### **Choices** :

- The team's approach should work as expected.
- The way the team performed feature selection is problematic.
- The size they chose for each set is problematic.
- The way the team tuned the model's hyperparameters is problematic.


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>There are two problems with the team's approach.

First, they used the entire dataset for feature selection. They should have split the dataset first and performed feature selection on the training and testing data separately. When doing feature selection on the entire dataset, the team risks leaking information from the soon-to-be test data into the model. A leak will lead to a model that performs too well on the existing data but will do poorly on future unseen data.

The second problem is how the team tuned the model's hyperparameters. They used the test set to choose the best hyperparameters, which could lead to a model that overfits to the testing dataset. In other words, this model will have trouble with unseen data because the team optimized it to work well on the test dataset. Instead, the team should have used a validation set for hyperparameter tuning.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Data Leakage in Machine Learning"](https://machinelearningmastery.com/data-leakage-machine-learning/) for an introduction to data leaks and how to prevent them. * ["Train, Validation, Test Split for Machine Learning"](https://blog.roboflow.com/train-test-split/) goes into detail about the importance of each split.</p></details>

-----------------------

## Date - 2023-12-19


## Title - Too many networks


### **Question** :

Why would Konrad want to look into recurrent neural networks?

He is always trying to minimize the amount of work he does. He is primarily a just-in-time person when it comes to learning new things.

And now that he started with deep learning, there are too many different types of neural networks, and he's getting overwhelmed.

**Let's help Konrad by selecting every correct statement regarding the differences between recurrent and traditional neural networks.**


### **Choices** :

- A recurrent neural network can process inputs of any length, unlike traditional networks that require a fixed-size input.
- A recurrent neural network has access to the information it processed at any time in the past, while traditional networks can't access any historical data.
- The model size of a recurrent neural network doesn't increase with the size of its input, unlike a traditional network where a larger input will require a bigger model.
- A recurrent neural network captures the sequential information present in the input data. Traditional neural networks don't have this ability.


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNN) are a type of artificial neural network that can process sequential or time series data. Their main difference from traditional networks is their ability to take information from prior inputs to influence the current input and output. This ability allows them to capture any sequential information present in the data. For example, an RNN is ideal for capturing the dependency between words of a sentence.

RNN processes the data sequentially so a model can process sequences of varying sizes. For example, an RNN can process a 5-word and 10-word sentence using the same input structure, unlike a traditional neural network that will need a different input size for each case. As the size of the input increases, a conventional network will need to accommodate it with a larger model size, while the size of an RNN will stay consistent.

The only incorrect option is the second: while an RNN can access historical information, they have difficulty accessing data from long ago. Their limited ability to process historical information is one of the most significant disadvantages of an RNN.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["What are recurrent neural networks?"](https://www.ibm.com/cloud/learn/recurrent-neural-networks) for a description of what they are and how they work.* ["An Introduction To Recurrent Neural Networks And The Math That Powers Them"](https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/) is a deeper dive into RNNs.</p></details>

-----------------------

## Date - 2023-12-20


## Title - Chest X-Rays


### **Question** :

In 2017, Andrew Ng's team published a paper on Deep Learning for pneumonia detection on chest X-Rays.

They used a dataset with 112,120 images belonging to 30,805 unique patients. They automatically labeled every sample with 14 different pathologies and randomly split the dataset into 80% training and 20% validation. Their process downscaled images to 224x224 pixels before inputting them into a neural network.

After publishing the paper and listening to the community's feedback, they had to redo their experiments.

**What do you think was wrong with their experiment?**


### **Choices** :

- A random split would get pictures from the same patient in training and the validation sets leading to leakage.
- Using too many images in their training dataset is too slow and wouldn't yield better results.
- Their dataset needed to be bigger to solve a problem with 14 classes.
- Downscaling X-Ray pictures to 224x224 is too aggressive and would destroy relevant image information.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The samples in the team's dataset are not independent. Different X-Ray images from the same patient will have similarities that a neural network could use to make a prediction.

For example, a patient might have a scar from a previous surgery or a specific bone density or structure. These clues will help the model make a prediction, so having X-Rays from the same patient in the training and validation sets will create a leaky validation strategy. Here is an excerpt from _[The Kaggle Book](https://amzn.to/3kbanRb)_:

> In a leaky validation strategy, the problem is that you have arranged your validation strategy in a way that favors better validation scores because some information leaks from the training data.

The team fixed the experiment in the third version of their paper. Here is what they did:

> For the pneumonia detection task, we randomly split the dataset into training (28744 patients, 98637 images), validation (1672 patients, 6351 images), and test (389 patients, 420 images). There is no patient overlap between the sets.

Notice how they ensured that there was no overlap between sets.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.* ["Target Leakage in Machine Learning"](https://www.youtube.com/watch?v=dWhdWxgt5SU) is a YouTube presentation that covers leakage, including during the partitioning of a dataset.* The original paper that shows the leaky validation strategy is ["CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning."](https://arxiv.org/pdf/1711.05225v1.pdf). The [third version](https://arxiv.org/pdf/1711.05225.pdf) of the paper fixes the problem.</p></details>

-----------------------

## Date - 2023-12-21


## Title - Decreasing lambda


### **Question** :

Ariel inherited a Linear Regression model. She didn't build the first version, so she isn't sure everything is working correctly.

Although the validation error looks fine, Ariel wants to experiment to determine whether every individual piece makes sense. She wants to start by looking into the lambda (λ) parameter that controls the model's regularization.

**Which of the following will eventually happen as Ariel continuously decreases the value of λ?**


### **Choices** :

- Ariel's model will eventually underfit, and the validation error will increase.
- Ariel's model will eventually overfit, and the validation error will increase.
- Ariel's model will eventually underfit, and the validation error will decrease.
- Ariel's model will eventually overfit, and the validation error will decrease.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Ariel can control the model's regularization using the lambda (λ) parameter. She can tune this parameter to decide how much she wants to penalize the model's flexibility. 

Decreasing the value of λ will increase the value of the coefficients, thus increasing the variance of the model. As this variance increases, the model will eventually overfit, leading to a higher validation error.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Regularization in Machine Learning"](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a) for a detailed explanation of how regularization works.* ["Fighting Overfitting With L1 or L2 Regularization: Which One Is Better?"](https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization) will give you a complete introduction to L1 and L2 regularization.</p></details>

-----------------------

## Date - 2023-12-22


## Title - Restaurant trip


### **Question** :

Sloane took her friends to a restaurant.

They ordered two dozen raw oysters. Sloane, Elliana, and Yasmin had five oysters each, while Ana ate the other 9.

**Which of the following represents the mean of the oysters they ate?**


### **Choices** :

- The mean is 2.
- The mean is 4.
- The mean is 5.
- The mean is 6.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The mean, median, and mode are different ways to measure the center of a dataset. Each of these metrics tries to summarize a set of values using a single number.

The mean of a dataset represents its average. We can find it by adding all data points and dividing by the number of values in the set.

This example has four data points: `5`, `5`, `5`, and `9`. Adding these values and dividing the result by `4` gives us a mean of `6`.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Mean Median Mode: What They Are, How to Find Them"](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/) for a complete explanation of the Mean, Median, and Mode.</p></details>

-----------------------

## Date - 2023-12-23


## Title - Incompatible shapes


### **Question** :

Juliet started by creating a fake dataset. She used Scikit-Learn's `make_blobs()` function:

```
X, y = make_blobs(
    n_samples=1000, 
    centers=3, 
    n_features=2
)
```

One thousand samples were enough data points for Juliet to experiment with neural networks. She used Keras to create a simple model and compiled it as follows:

```
model.compile(
    optimizer=SGD(learning_rate=0.01), 
    loss="categorical_crossentropy", 
    metrics=["accuracy"]
 )
```

But when she tried to fit the model using 32 samples as the `batch_size`, she got the following error: 

> Shapes (32, 1) and (32, 3) are incompatible.

**Which of the following should solve Juliet's problem?**


### **Choices** :

- Juliet should not use 32 samples as her batch size. Instead, she should use a smaller value.
- Juliet should use the value 3 for the `n_features` parameter when calling the `make_blobs()` function.
- Juliet should use `sparse_categorical_crossentropy` as her loss function.
- Juliet should one-hot encode the labels returned by the `make_blobs()` function.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Keras' `SparseCategoricalCrossentropy` computes the cross-entropy loss between the labels and predictions. It works when the labels in the dataset are integer values, for example, 1, 2, and 3.

Keras' `CategoricalCrossentropy`, on the other hand, has the same function but works when the labels in the dataset are one-hot encoded, for example, `[1, 0, 0]`, `[0, 1, 0]`, and `[0, 0, 1]`.

Juliet is using `make_blobs()` to create a dataset, and this function returns integer values to enumerate the different clusters. Therefore, Juliet should one-hot encode these values, or she needs to replace the loss function with `sparse_categorical_crossentropy`. Either option works.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Here is Keras' [SparseCategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) and [CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) documentation.* And here is [Scikit-Learn's make_blobs()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) function documentation.</p></details>

-----------------------

## Date - 2023-12-24


## Title - The clinical trial


### **Question** :

With more than five years of experience, Alani was an expert in dealing with tabular data. 

In her last consulting gig, the company gave her access to a large dataset with patients' information from a clinical trial. Alani needs to predict which patients are more likely to drop off, and she decided to use XGBoost to create her model.

There's only one problem. The dataset has a few columns with missing values. Not enough for Alani to drop the columns, but she still needs to decide what to do.

**Which of the following should be the best way to deal with the missing values?**


### **Choices** :

- Alani should replace every missing value with the column's mean.
- Alani should replace every missing value with the column's median.
- Alani should replace every missing value with the column's mode.
- Alani should keep the missing values untouched.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Fortunately for Alani, she uses XGBoost, an algorithm that supports missing values and has a clever way to deal with them.

In ["XGBoost: A Scalable Tree Boosting System,"](https://arxiv.org/pdf/1603.02754v3.pdf) the authors explain the algorithm's process to handle missing values. They call it "Sparsity-aware Split Finding":

> (...) we propose to add a default direction in each tree node (...). When a value is missing in the sparse matrix x, the instance is classified into the default direction. There are two choices of default direction in each branch. The optimal default directions are learnt from the data.

In other words, XGBoost learns how to classify samples with missing values by learning the best possible replacement for those values. This approach is better than replacing the value with an arbitrary choice like the feature's mean, median, or mode. Unless Alani knows something specific about those features, she should keep the missing values and let the algorithm deal with them.

Although XGBoost has a clever approach for missing values, it's still generic and won't be better than a specific strategy that tackles each feature individually. A complete understanding of the problem and using that information to modify the data is always the best strategy.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["XGBoost: A Scalable Tree Boosting System"](https://arxiv.org/pdf/1603.02754v3.pdf) is the XGBoost paper that explains the "Sparsity-aware Split Finding" approach.</p></details>

-----------------------

## Date - 2023-12-25


## Title - Muscle memory


### **Question** :

Brooke is working on a machine learning multi-class classification neural network and used a softmax activation function on the output layer.

But Brooke did it because her muscle memory kicked in. She isn't sure why softmax is the right way to go.

**Which of the following statements is true about the softmax activation function when used in the output layer of a neural network?**


### **Choices** :

- The softmax function turns the network's input into a vector of probabilities that sum to 1.
- The softmax function turns a vector of real values into a sorted vector of probabilities that sum to 1.
- The softmax function turns a vector of real values into a vector of probabilities that sum to 1.
- The softmax function turns the network's input into a vector of probabilities that sum to 0.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The softmax function turns a vector of real values into another vector of probabilities that sum to 1. These probabilities are proportional to the relative scale of each value in the vector.

When used as the activation function of the output layer of a neural network, softmax converts the scores from the previous layer to a normalized probability distribution. This is a convenient way to interpret the results of a multi-class classification model.

Noticed that the first and fourth choices argue about softmax converting the network's input. This would only be true if the network doesn't have hidden layers, and the input layer is connected directly to the output. Moreover, the fourth choice claims that the sum of the vector values will be zero, which is incorrect.

Finally, the second choice argues about a "sorted vector of probabilities." Softmax doesn't sort the output vector, so this option is also incorrect.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check ["Softmax Activation Function with Python"](https://machinelearningmastery.com/softmax-activation-function-with-python/) for more information about the Softmax function and a way to implement it.- ["What is the Softmax Function"](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer) is a great summary of this function.</p></details>

-----------------------

## Date - 2023-12-26


## Title - First chapter


### **Question** :

Adelaide was about to finish the first chapter of her book. It was about neural networks, and Adelaide wanted to write a few exercises before closing it.

She found that an excellent way to summarize the chapter was with a multi-choice question:

**Which of the following sentences are true about neural networks?**


### **Choices** :

- We can only optimize neural networks using the Gradient Descent algorithm.
- Neural networks can only find the optimal solution for convex problems.
- Neural networks can use a mix of activation functions.
- Neural networks can approximate any function when using non-linear activations.


### **Answer** :

<details><summary>CLICK ME</summary><p>0011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Gradient descent is an iterative optimization algorithm used to find the local minimum of a function. The algorithm works by taking steps proportional to the negative of the function's gradient at the current point. Gradient descent is an excellent choice to optimize neural networks, but it's not the only way. We can use, for example, the [Adam algorithm](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning), a combination of the AdaGrad and RMSProp algorithms.

Using Gradient descent, we can find the optimal solution for a concave problem, but it's not guaranteed. The algorithm might converge to a local minimum instead of the global minimum.

When setting up a neural network, we can use a mix of activation functions. For example, ReLU in the hidden layers and Softmax in the output layer of a classification model.

Finally, thanks to the [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem), we can turn a two-layer neural network into a universal function approximator when using non-linear activation functions.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Gradient Descent For Machine Learning"](https://machinelearningmastery.com/gradient-descent-for-machine-learning/) for a description of the algorithm.* Check the ["Universal approximation theorem"](https://en.wikipedia.org/wiki/Universal_approximation_theorem) on Wikipedia for more information about the power of neural networks.* ["Gentle Introduction to the Adam Optimization Algorithm for Deep Learning"](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning) is an excellent explanation of the Adam algorithm.</p></details>

-----------------------

## Date - 2023-12-27


## Title - Using log loss


### **Question** :

Wren learned about the log loss for the first time.

Log loss, also known as cross-entropy loss, indicates how close a prediction probability comes to the target value.

Wren wants to use what she learned right away.

**Which of the following problems is a good candidate for Wren to practice the log loss?**


### **Choices** :

- Predict the house price given a dataset with the number of bedrooms of every house in the neighborhood.
- Determine a person's location in an image, given a dataset of pictures from a security camera.
- Determine whether tomorrow will rain, given a dataset with the weather patterns over the last few years.
- Group customers from a store depending on their buying patterns.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Log loss is a function that we commonly use in classification problems. It returns the negative logarithm of the product of probabilities.

Log loss indicates how close a prediction probability is to the target value. The more the predicted probability differs from the target value, the higher the log loss.

From the list of options, the only classification problem is determining whether it will rain on any given day: it's a binary choice. The other three options are regression, object detection, and clustering.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Understanding binary cross-entropy / log loss: A visual explanation"](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) is an excellent introduction to binary cross-entropy.- Another article that helps understand binary cross-entropy is ["Binary Cross Entropy/Log Loss for Binary Classification"](https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/).</p></details>

-----------------------

## Date - 2023-12-28


## Title - Sigmoid's interpretation


### **Question** :

Lennon is starting to understand the need for activation functions when building neural networks.

She is still relatively new and doesn't grasp the details of some of the most popular activation functions, especially because some formulas aren't straightforward to understand.

For example, here is the Sigmoid formula:

![Sigmoid](https://user-images.githubusercontent.com/1126730/196755518-311dd425-676e-4c85-be1f-467694879c30.jpg)

**Which of the following is a correct interpretation of this function?**


### **Choices** :

- The output of the Sigmoid function could be any real number.
- The output of the Sigmoid function could be any integer number.
- The output of the Sigmoid function is a real number between `-1` and `1`.
- The output of the Sigmoid function is a real number between `0` and `1`.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The Sigmoid function takes a value as input and outputs another real value between `0` and `1`. We say that Sigmoid "squeezes" the input into that range.

Sigmoid is continuously differentiable, and its derivative is simple to compute. This, together with its fixed output range, make the Sigmoid function one of the most popular activation functions.

Here is the plot of the Sigmoid function:

![Sigmoid Plot](https://user-images.githubusercontent.com/1126730/197406367-7a06343e-f5ed-4409-b11d-6ee92eef7171.jpg)</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For a complete description of the Sigmoid function, check the ["Logistic function"](https://en.wikipedia.org/wiki/Logistic_function) Wikipedia page.</p></details>

-----------------------

## Date - 2023-12-29


## Title - Throwing the towel


### **Question** :

Delaney was ready to throw the towel.

She's been paying careful attention to her machine learning professor. After they reviewed gradient descent in detail, Delaney knew something important: Gradient descent moves downhill, so she should expect the loss to decrease over time.

But here she is, looking at a training loss that goes up in several places. The first neural network she trains, and she can't shake the feeling she is doing something wrong:

![Learning Curves](https://user-images.githubusercontent.com/1126730/192155439-4a4a9f25-ed44-4b72-994b-59a6f2743fd5.jpg)

**How can Delaney fix this problem?**


### **Choices** :

- Delaney should decrease the batch size. That should stop the loss from increasing during training.
- Delaney should increase the batch size. That should stop the loss from increasing during training.
- Delaney should use Early Stopping to stop the process before the loss increases.
- Delaney shouldn't do anything because her network has no problem.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Gradient descent moves downhill on average, so a network that learns appropriately should see the loss decrease over the training session. However, individual updates can move in the opposite direction, causing the loss to fluctuate up and down.

Delaney's plot shows the loss increasing a few times momentarily, but it immediately starts decreasing. That's normal, and Delaney shouldn't worry about it.

Delaney can modify the batch size to improve her results, but just by looking at the information from this question, there's nothing to fix. If she uses [Early Stopping](https://articles.bnomial.com/early-stopping), she will prevent the network from improving further.

In summary, Delaney shouldn't do anything at this point.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["An overview of gradient descent optimization algorithms"](https://www.ruder.io/optimizing-gradient-descent/) for a deep dive into gradient descent and every one of its variants.* Check ["Early Stopping"](https://articles.bnomial.com/early-stopping) for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models.</p></details>

-----------------------

## Date - 2023-12-30


## Title - Just in time


### **Question** :

Lila had to turn in an implementation of a machine-learning technique to solve a toy exercise. She had to write the code from scratch.

She had a simple dataset: only a few features and rows of data. 

Lila decided to implement a solution that wouldn't require training and would classify new samples just in time.

**Which of the following was the technique that Lila used?**


### **Choices** :

- Lila used Linear Regression
- Lila used a Decision Tree
- Lila used k-Nearest Neighbors
- Lila didn't use any of the above.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Unlike Linear Regression and Decision Trees, k-Nearest Neighbors (KNN) is an algorithm that doesn't create a predictive model from a training dataset to make predictions. In KNN, there's no need for a training phase. Instead, the algorithm computes a prediction during inference time.

KNN uses the entire dataset and looks for a pre-determined number of instances closest to the observation we want to classify to determine to which group the sample belongs.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Understand the Fundamentals of the K-Nearest Neighbors (KNN) Algorithm"](https://heartbeat.comet.ml/understand-the-fundamentals-of-the-k-nearest-neighbors-knn-algorithm-533dc0c2f45a) for an introduction to KNN.</p></details>

-----------------------

## Date - 2023-12-31


## Title - The model's accuracy


### **Question** :

A team built a binary classification model. They named the classes `A` and `B`.

After finishing training, they evaluated the model on a validation set, and here is the confusion matrix with the results:

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/186248358-1b39a042-5725-408b-84aa-8dd915a6d99d.jpg)

**Given the above confusion matrix, what is the accuracy of this binary classification model?**


### **Choices** :

- The accuracy of the model is 28%.
- The accuracy of the model is 52%.
- The accuracy of the model is 80%.
- The accuracy of the model is 88%.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To compute the model's accuracy, we must divide the number of correct predictions by the number of total predictions.

In this example, the model made 100 predictions on the validation set. 80 of those predictions were correct: 52 corresponding to class `A`, and 28 to class `B`. Therefore, the accuracy of the model is 80%.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.</p></details>

-----------------------

## Date - 2024-01-01


## Title - Class notes


### **Question** :

It's been almost eight years since Dakota's first machine learning class, and she decided to dust her old notes and share them online.

Surprisingly, while reviewing one of her introductory classes, she found one mistake in her description of Linear Regression.

**Below, you have four bullets that Dakota wrote. Which of them is not correct?**


### **Choices** :

- Linear Regression is a Supervised Learning technique useful for solving Regression problems.
- We use Linear Regression for predicting a categorical dependent variable with the help of independent variables.
- Linear Regression aims to find the best line that can accurately predict the output for the continuous dependent variable.
- In Linear Regression, the relationship between the dependent and independent variables must be linear.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Linear Regression is probably the most popular Supervised Learning technique in machine learning. Its goal is to fit the best line through the data to predict a continuous output. 

The algorithm uses a set of independent variables to predict a continuous dependent variable. For example, a person's age, salary, or home price. 

Finally, for Linear Regression to work, we must ensure that the relationship between the inputs and the output is linear. A Linear Regression model won't give us good predictions if the relationship isn't linear. Sometimes, this condition means we must transform the input features before using Linear Regression. For example, if you have a variable with an exponential relationship with the target variable, you can use log transform to turn the relationship linear. 

Therefore, Dakota's second bullet point is the only one that's not correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Linear Regression for Machine Learning"](https://machinelearningmastery.com/linear-regression-for-machine-learning/) is an introduction to Linear Regression.</p></details>

-----------------------

## Date - 2024-01-02


## Title - Line 10 of the network


### **Question** :

Here is a simple and compact implementation of a neural network in Python:

![Neural network](https://user-images.githubusercontent.com/1126730/210234829-44179024-a0e1-4be0-95f5-6c3e2d2d77ed.png)

Line 10 computes the update we will add to the weights connecting the hidden and output layers.

**Which of the following explains what happens on that line?**


### **Choices** :

- The code computes the update by multiplying the derivative of the error by the derivative of the network's output.
- The code calculates the update by multiplying the derivative of the output by the target value.
- The code computes the update by multiplying the error by the network's output.
- There's an error in how the code calculates the update.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We need to update the weights of the network during the backpropagation process. We start from the final layer of the network and move sequentially to the beginning.

We can apply the chain rule to calculate the update to the second set of weights: The multiplication between the derivative of the error and the output's layer derivative.

The error represents how far the output is from the target values. In this example, the error is the square of the difference between `layer2` and the target values `y`. The derivative of the error is `2 * (layer2 - y)`.

Notice that the output layer (`layer2`) uses a sigmoid function; hence its derivative is `layer2 * (1 - layer2)`.

Therefore, Line 10 computes the update by multiplying the derivative of the error by the derivative of the network's output.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Neural Networks and Deep Learning_](http://neuralnetworksanddeeplearning.com/index.html) is a free online book written by [Michael Nielsen](https://twitter.com/michael_nielsen) with a great introduction to neural networks.</p></details>

-----------------------

## Date - 2024-01-03


## Title - Neighborhood groceries


### **Question** :

A neighborhood grocery store wants to segment its online customers to send personalized advertising to their inboxes.

Alivia is in charge of the team that will implement a solution, but she has no previous experience building a system like this.

**How should Alivia approach this problem?**


### **Choices** :

- Alivia should use a decision tree to find and group similar customers.
- Alivia should use a decision tree to group customers into a predefined list of categories.
- Alivia should use a clustering algorithm to find and group similar customers.
- Alivia should use a clustering algorithm to group customers into a predefined list of categories.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Customer segmentation](https://towardsdatascience.com/customer-segmentation-with-machine-learning-a0ac8c3d4d84) is a popular field where you try to find similar characteristics among your customers. It's the perfect opportunity to use unsupervised learning: a clustering algorithm.

Alivia doesn't know what characteristics the applicants share, so she can't predefine the categories on which she wants to segment the customers. Instead, she needs to use a clustering algorithm to find those groups.

For example, she could use [K-Means](https://en.wikipedia.org/wiki/K-means_clustering) to find interesting patterns and group the customers that share them. A critical distinction is that you don't need to consider these groups preemptively; the clustering algorithm will find them for you.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Customer Segmentation with Machine Learning"](https://towardsdatascience.com/customer-segmentation-with-machine-learning-a0ac8c3d4d84) for a quick introduction to Customer Segmentation.* ["10 Clustering Algorithms With Python"](https://machinelearningmastery.com/clustering-algorithms-with-python/) will introduce you to 10 different clustering algorithms.</p></details>

-----------------------

## Date - 2024-01-04


## Title - Noisy training loss


### **Question** :

Lucia finished training her deep learning model. She got the training history and plotted the train and validation loss of the model.

Unfortunately, she didn't like what she saw. 

It was tough to look at the chart and draw conclusions from it: The training loss was too noisy, and Lucia would have to fix it if she wanted a better understanding of her model.

**Which strategies should Lucia follow to reduce the noise in the training loss? Select all that apply.**


### **Choices** :

- Lucia should increase her learning rate.
- Lucia should decrease her learning rate.
- Lucia should increase her batch size.
- Lucia should decrease her batch size.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>If Lucia increases the learning rate, her model will take larger steps in the gradient direction, but it may miss the local minima. When it misses the local minima, it will start oscillating contributing to the noise Lucia sees in the chart. Reducing the learning rate will help with this problem.

If Lucia uses a small batch size, the optimizer will only see a small portion of the data during every cycle. This introduces noise in the training process because the gradient of the batch may take the model in entirely different directions. Here is Jason Brownlee on ["How to Control the Stability of Training Neural Networks With the Batch Size"](https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/): "Smaller batch sizes are noisy, offering a regularizing effect and lower generalization error."

Based on this idea, if Lucia decreases the batch size, the noise will get worse, so she wants to increase the size to reduce the oscillations of the training loss.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>*  Check ["How to Control the Stability of Training Neural Networks With the Batch Size"](https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/) for a full description of how the batch size affects the stability of the training process.* ["What could an oscillating training loss curve represent?"](https://ai.stackexchange.com/questions/14079/what-could-an-oscillating-training-loss-curve-represent) is a StackExchange question covering different strategies to reduce the noise in the training loss.* ["Why is my training loss fluctuating?"](https://www.researchgate.net/post/Why_is_my_training_loss_fluctuating) is another public question covering the same topic.</p></details>

-----------------------

## Date - 2024-01-05


## Title - Purchasing products


### **Question** :

Collins was working on a machine learning model to predict the likelihood of a customer purchasing a product. She was hired by an e-commerce company that wanted to use her model to personalize their sales promotions.

After several weeks of development, Collins had two models that performed well on the validation data. However, she noticed that the models had different strengths and weaknesses. One model had a higher precision but a lower recall than the other.

Collins wanted to find the best overall model to deploy in production, but she wasn't sure how to choose between them.

**What is the best way for Collins to decide which model is the best overall?**


### **Choices** :

- Collins should fix the precision at 95% and choose the model with the higher recall.
- Collins should fix the recall at 95% and choose the model with the higher precision.
- There's no objective way to decide which model is best. Collins should pick either one of them.
- Collins should compute the area under the ROC curve for both models and choose the one with the higher value.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Which model is better usually depends on the application. A high recall is more important than precision for some use cases, while in others, it is not. 

The first two options suggest fixing one particular metric and choosing the model that performs the best on the other one. This is a valid approach, but it's not what Collins needs. She wants to determine which model is the best, but fixing either recall or precision won't return the best overall model since we would always prioritize one of the metrics.

For example, imagine that we tune both models to a recall above `95%` and then pick the one with the higher precision. There's no guarantee that the model we choose is the best possible overall—the one that better balances recall and precision. Instead, we ensured that the model we picked was the best, with a recall above `95%`.

To find the best overall model, Collins should compute the area under the ROC curve (Receiver Operating Characteristic) and choose the model with the higher value. 

A ROC curve shows the True Positive and False Positive Rates at different classification thresholds. The area under this curve measures the performance of the model. A perfect model will have an area of `1.0`, while a model that only makes mistakes will have an area of `0.0`. Therefore, choosing the model with the higher area will give Collins the best overall model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["ROC and AuC"](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) for an explanation of how the Area under the Curve on a ROC works.* ["ROC metrics"](https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics) will give you more information about the different metrics you can compute from a ROC.</p></details>

-----------------------

## Date - 2024-01-06


## Title - Robotics department


### **Question** :

Alayah works in the robotics department at a tech company. She has been tasked with building a model that recognizes and identifies different objects.

Alayah decides to use a transformer model for this task. She knows that transformers are a type of deep learning model that has been proven effective in object recognition tasks.

**Which of the following are characteristics of transformers?**


### **Choices** :

- Transformers use attention mechanisms to allow the model to focus on specific parts of the input.
- Transformers can handle variable-length output sequences but only fixed-size input sequences.
- Transformers are based on convolutional neural networks and use filters to extract features from the input data.
- Transformers can only work with numeric data.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Transformers are a type of deep learning model that has been widely used in Natural Language processing tasks, such as machine translation and text summarization. They were introduced in the 2017 paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al.

One of the key characteristics of transformers is their use of attention mechanisms. Attention allows the model to focus on specific input parts, which is useful when processing long data sequences. This differs from traditional recurrent neural networks, which use feedback loops to process the input data.

Another characteristic of transformers is their ability to handle variable-length input and output sequences. This is useful when dealing with natural language data, which often has varying lengths.

Transformers are not based on convolutional neural networks (CNNs.) They are based on self-attention mechanisms, while CNNs are based on convolutional layers, which apply a set of filters to the input data to extract features. 

Transformers are not limited to working with numerical data. They can also process categorical data.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check the ["Attention is All You Need"](https://arxiv.org/abs/1706.03762) paper by Vaswani et al. for an introduction to Transformers.* The [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) is a fantastic visual guide that explains the inner workings of the Transformer architecture in an easy-to-understand manner.</p></details>

-----------------------

## Date - 2024-01-07


## Title - Low training. High testing


### **Question** :

There's not a lot of context for you other than the following chart showing the training and testing losses of a machine learning model:

![Training and Testing Loss Chart](https://user-images.githubusercontent.com/1126730/188513143-7dc8ec67-17a3-4ce9-919c-7a2851bf4d49.jpg)

As you can see, after finishing training, the training loss is low, but the testing loss is high.

**What's a reasonable conclusion about this machine learning model?**


### **Choices** :

- Your model is overfitting.
- Your model is underfitting.
- Your model is either overfitting or underfitting, but we can't tell.
- Your model is well-fit.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A good model should capture valuable patterns in the data and discard any noise that doesn't help with predictions. An overfitting model will fit that noise. An underfitting model will not capture the relevant patterns in the dataset. 

An overfitting model should not have any problems with the training data but stumble with the testing data. Therefore, we should expect a low training loss and a high testing loss. An underfitting model should struggle with the training and testing datasets, so both of its losses should be high. A well-fit model, however, should have low training and testing losses.

The chart shows the model doing well with the training data but struggling with the testing set. Therefore, the model is overfitting.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Overfitting and Underfitting with Learning Curves"](https://articles.bnomial.com/overfitting-underfitting-learning-curves) for an introduction to two fundamental concepts in machine learning through the lens of learning curves.</p></details>

-----------------------

## Date - 2024-01-08


## Title - Auto industry


### **Question** :

Hallie works in the auto industry. She is tasked with building a model to predict the likelihood of a car accident based on various factors such as the driver's age, weather conditions, and the car's make and model.

Hallie decides to use a decision tree for this task. She knows that decision trees are powerful models that can help her understand the relationships between the input features and the target variable.

**Which of the following are characteristics of decision trees?**


### **Choices** :

- Decision trees are non-parametric models that make no assumptions about the data distribution.
- Decision trees can handle both numerical and categorical data.
- Decision trees split the data into subsets based on random features.
- Decision trees are difficult to interpret and explain to non-technical stakeholders.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Decision trees are a popular and powerful tool in machine learning and data science. They are a type of non-parametric model, which means that they make no assumptions about the distribution of the data. This allows them to work well with various data types, including numerical and categorical data.

One of the main strengths of decision trees is their ability to split the data into subsets based on the most important features, not random features. This allows them to identify the key relationships between the input features and the target variable. The resulting tree structure is easy to interpret and explain to non-technical stakeholders, making them a useful tool for making decisions in real-world scenarios.

One of the benefits of decision trees is that they are easy to interpret and explain to non-technical stakeholders. The tree structure shows the sequence of decisions made to classify the data, which can help us understand the relationships between the input features and the target variable.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Decision tree learning"](https://en.wikipedia.org/wiki/Decision_tree_learning) is the Wikipedia page where you can read about decision trees.* The Scikit-Learn's ["Decision Trees"](https://scikit-learn.org/stable/modules/tree.html) page contains an extensive list of advantages and disadvantages of decision trees.</p></details>

-----------------------

## Date - 2024-01-09


## Title - Digit distribution


### **Question** :

The MNIST dataset is a collection of scanned images of handwritten digits. 

It’s a modified subset of two datasets collected by the United States National Institute of Standards and Technology and one of the most popular datasets out there.

Here is a simple way you can load this dataset in a [Google Colab](https://colab.research.google.com/) notebook:

```
from keras.datasets import mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()
```

The dataset contains every digit from 0 to 9.

**Could you help us determine what of the following statements are correct about the dataset?**


### **Choices** :

- There are a total of 7,293 images of digit 7.
- The most popular digit in the dataset is 1, and the least popular in the train set is 9.
- The digit 5 represents a mere 9% of the images in the test set.
- There are 80,000 images among both train and test sets.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We need to write some code to answer this question.

Assuming that we start with the code snippet provided in the problem statement, we have the dataset split into a train and a test set, so we need to [concatenate](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) them to make the rest of the code simpler:

```
import numpy as np
labels = np.concatenate((y_train, y_test))
```

Notice that we don't need to worry about `X_train` and `X_test` because those arrays contain the images. We can answer the question by looking at the labels only.

[Numpy's `unique()` function](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) will allow us to group and count the labels:

```
count = np.unique(labels, return_counts=1)[1]
count_train = np.unique(y_train, return_counts=1)[1] 
count_test = np.unique(y_test, return_counts=1)[1]
```

If we print the count corresponding to digit 7, we will find out that there are 7,293 images:

```
print(count[7])
```

To determine the most popular digit in the dataset and the least popular in the train set, we can use the `argmax` and `argmin` functions in the corresponding arrays:

```
print(np.argmax(count))
print(np.argmin(count_train))
```

By running this code, you'll find out that 1 is indeed the most popular digit in the entire dataset, but 5 is the least popular in the train set.

We can compute the percentage that digit 5 represents in the test set with the following line:

```
print(count_test[5] / y_test.shape[0])
```

The result is `0.089`.

Finally, we can find the total number of images by printing `labels.shape[0]`. We have 70,000 images, not 80,000.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For more information, you can check Numpy's [concatenate](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) and [unique](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) functions.</p></details>

-----------------------

## Date - 2024-01-10


## Title - Two bad models


### **Question** :

Chloe wasn't confident about the results of her homework, and her professor confirmed her fears.

Two of the models she had to train weren't good enough. The first was overfitting, while the second was underfitting.

She got some feedback: the professor suggested analyzing the models from a bias and variance perspective and taking the necessary steps to fix them.

Chloe is still learning. She wasn't sure what to do.

**Which of the following is the correct conclusion about Chloe's models?**


### **Choices** :

- The overfitting model has a high bias, and the one underfitting has a high variance.
- The overfitting model has a high bias, and the one underfitting has a low variance.
- The overfitting model has a low bias, and the one underfitting has a high variance.
- The overfitting model has a low bias, and the one underfitting has a low variance.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>To answer this question, we need to focus on the impact of bias on overfitting models and variance on underfitting models.

Let's start with the bias, and what [Jason Brownlee](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) has to say about it: "Bias are the simplifying assumptions made by a model to make the target function easier to learn."

In other words, bias refers to the assumptions the model makes to simplify the process of finding answers. The fewer assumptions it makes, the less biased the model is.

Take any linear model, for example. They are highly biased because these models assume a linear relationship between the observations and the target variable. Because of these assumptions, linear models can easily underfit the training data. On the other hand, decision trees have a high variance. They don't make too many assumptions about the target function, so they are more likely to overfit.

Therefore, high biased models are prone to underfitting, while high variance models are prone to overfitting. This narrows the correct answer to either the third or the fourth choice.

Here is what [Jason](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) has to say about variance: "Variance is the amount that the estimate of the target function will change if different training data was used."

Variance refers to how much the answers given by the model will change if we use different training data. The simpler the model is, the less likely the results will vary if we use different training data, while more complex models will have the opposite problem. Often, linear models are low-variance, and nonlinear models are high-variance.

Therefore, high variance models will be prone to overfit, while low variance models will be prone to underfit. Looking at the two candidate choices left, the fourth is the correct answer to this question.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Here is Jason Brownlee's article I mentioned before: ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/).* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).* In case you like the simplicity of Twitter threads, here is one for you about this topic: ["Bias, variance, and their relationship with machine learning algorithms"](https://twitter.com/svpino/status/1390969728504565761).</p></details>

-----------------------

## Date - 2024-01-11


## Title - Tiny number


### **Question** :

Here is Phoebe's experiment:

She split her dataset into training and testing. She then set the testing data aside and created five other random splits from the training data, each with 20% of the samples.

Phoebe then trained a model on each of the five groups and tested them on the testing data to determine the accuracy of each model. She grabbed the results and computed the variance between them:

![Pseudocode](https://user-images.githubusercontent.com/1126730/199803699-e056bcea-133f-45b1-9c52-8e68d75f5bed.png)

The final result was a tiny number.

**What can you say about the model Phoebe used in her experiment?**


### **Choices** :

- Phoebe's model is high bias and low variance.
- Phoebe's model is high bias and high variance.
- Phoebe's model is low bias and low variance.
- Phoebe's model is low bias and high variance.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>After Phoebe trains each model and computes their accuracy on the testing set, a couple of things could happen:

1. The accuracy of each model is significantly different.
2. The accuracy of each model is relatively similar.

We know that Phoebe split the training dataset into five random groups, so we should expect each of these groups to be similar. If the model's performance on each differs significantly, we know Phoebe is using a high-variance model.

On the other hand, if the model's performance is very similar across all five groups, we know we are looking at a stable, high-bias model that doesn't change much with new data.

The variance between the five accuracies is tiny, so Phoebe must be using a high-bias and low-variance model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) for an introduction to bias and variance.* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).</p></details>

-----------------------

## Date - 2024-01-12


## Title - Outstanding performance


### **Question** :

Cecilia was finally ready to start building the model supporting her research.

She works with a high-dimensional but relatively small dataset. One of the largest companies in the world is sponsoring Cecilia's work, so she has a lot of pressure to deliver good results.

After years of questionable evaluation practices from the research community, the sponsor wants to ensure that Cecilia provides an accurate estimate of model performance.

**Which of the following approaches should Cecilia use to build her model?**


### **Choices** :

- Cecilia should use leave-one-out cross-validation to build her model.
- Cecilia should train the model on the entire dataset and later evaluate it on the same data.
- Cecilia should train the model on a portion of the data and use the rest to evaluate the model.
- Cecilia should use k-fold cross-validation with 5 to 10 folds.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Cecilia should use [leave-one-out cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation).

Leave-one-out cross-validation is a variant of cross-validation where the number of folds equals the number of samples in the dataset. 

To use leave-one-out cross-validation, we build one model for each sample in the dataset. We train each model using all data except one instance we later use to evaluate its performance. Finally, we compute the overall performance by averaging the result of each model.

Leave-one-out cross-validation is usually expensive to run. Assuming Cecilia will use leave-one-out cross-validation on a dataset with 10,000 samples, she will need to train 10,000 models. Compare this with 10-Fold cross-validation, where she will only need to build ten models. Fortunately, Cecilia is not working with a large dataset, so this shouldn't be a problem.

With small datasets, leave-one-out cross-validation will give Cecilia a more robust estimate of model performance. Each sample has an opportunity to represent the entire dataset and contribute to the final evaluation, and this will result in a reliable and less biased estimate of model performance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["An experimental comparison of cross-validation techniques for estimating the area under the ROC curve"](https://www.sciencedirect.com/science/article/abs/pii/S0167947310004469#br000160) is a paper comparing several cross-validation techniques.* ["LOOCV for Evaluating Machine Learning Algorithms"](https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/) is an excellent introduction to leave-one-out cross-validation.* Check ["A Quick Intro to Leave-One-Out Cross-Validation (LOOCV)"](https://www.statology.org/leave-one-out-cross-validation/) for a succinct introduction to leave-one-out cross-validation.</p></details>

-----------------------

## Date - 2024-01-13


## Title - Decreasing KNN's K


### **Question** :

Diana knows her k-Nearest Neighbor (KNN) implementation uses a value of `K` that's too high.

She wants to start experimenting with a lower value.

**What should Diana expect to happen as she decreases `K`?**


### **Choices** :

- As Diana decreases the value of `K`, she will reduce the algorithm's variance and bias.
- As Diana decreases the value of `K`, she will increase the algorithm's variance and bias.
- As Diana decreases the value of `K`, she will increase the algorithm's variance and reduce its bias.
- As Diana decreases the value of `K`, she will reduce the algorithm's variance and increase its bias.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The smaller the value of `K`, the more variance and less bias KNN will exhibit. For example, if we use `K = 1`, a single sample close to our observation will cause the algorithm to return the wrong prediction. Imagine an observation surrounded by many instances from class A and only one from class B that's closer than everything else. Since `K=1`, the algorithm will incorrectly predict the observation as class B. 

Conversely, the larger the value of `K`, the less variance and more bias KNN will exhibit. Since KNN uses an average or majority voting, no individual sample will cause the algorithm to return the wrong prediction. Setting `K` to a value that's too large will make the algorithm underfit because it won't capture the variance in the dataset.

Here is a quote from ["Why Does Increasing k Decrease Variance in kNN?"](https://towardsdatascience.com/why-does-increasing-k-decrease-variance-in-knn-9ed6de2f5061):

> If we take the limit as `K` approaches the size of the dataset, we will get a model that just predicts the class that appears more frequently in the dataset [...]. This is the model with the highest bias, but the variance is 0 [...]. High bias because it has failed to capture any local information about the model, but 0 variance because it predicts the exact same thing for any new data point.

In summary, the smaller the value of `K` is, the lower the bias and the higher the variance. The larger the value of `K` is, the higher the bias and the lower the variance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Why Does Increasing k Decrease Variance in kNN?"](https://towardsdatascience.com/why-does-increasing-k-decrease-variance-in-knn-9ed6de2f5061) is a great article diving into the relationship of `k` and the variance of KNN.* For a more general introduction to the bias-variance trade-off, check ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/).</p></details>

-----------------------

## Date - 2024-01-14


## Title - Geometric transformations


### **Question** :

We can't understand how deep learning networks work without looking into different tensor operations like addition, multiplication, etc. In the end, the magic mostly boils down to many of these operations chained together.

Here is something fun.

Every tensor operation has a corresponding geometric interpretation. For example, the addition operation represents the action of [translating](https://en.wikipedia.org/wiki/Translation_(geometry)) an object. Adding a vector to a set of points representing a 2D object is equivalent to moving the object by a certain amount in a specific direction.

**Which of the following descriptions are correct geometric interpretations of tensor operations in a 2D plane?**


### **Choices** :

- We can rotate an object by multiplying it with a 2x2 matrix with the following structure: `[[cos(θ), -sin(θ)], [sin(θ), cos(θ)]]`.
- We can scale an object by a dot product with a diagonal 2x2 matrix with the following structure: `[[h, 0], [0, v]]`.
- We get a linear transform by adding an arbitrary matrix.
- An affine transform combines a linear transform and a translation.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>When we talk about [rotating](https://en.wikipedia.org/wiki/Rotation_(mathematics)) an object, we can think of moving each point of that object circularly around a center. Assuming that we use a column vector to represent the coordinate of each point, we can use matrix multiplication to rotate the object.

First, we need a rotation matrix R. This is the matrix we will multiply with the object's coordinates to obtain the new set of rotated coordinates. The structure of this matrix R to rotate an object counterclockwise is `[[cos(θ), -sin(θ)], [sin(θ), cos(θ)]]` where θ represents the rotation angle. Therefore, the first choice is correct.

We can [scale](https://en.wikipedia.org/wiki/Scaling_(geometry)) an object using the dot product with a matrix S, but this time the matrix will have a different structure. Vertical and horizontal scaling of a 2D object requires a 2x2 matrix containing the horizontal and vertical factors—by how much we want to scale the object in each direction.

Let's assume we want to scale the object in half horizontally but keep it as-is vertically. The horizontal factor should be 0.5, and the vertical factor should be 1.0. The scaling matrix S will be `[[0.5, 0], [0, 1.0]]`. Notice how this is a diagonal matrix because it only contains non-zero values in the diagonal. Therefore, the second choice is correct.

The third choice argues that a [linear transform](https://en.wikipedia.org/wiki/Linear_transformation) is an addition operation with an arbitrary matrix, but this is incorrect. The core of a linear transformation is the dot product, not an addition operation. Notice that both scaling and rotation are linear transforms.

Finally, the fourth choice argues that an [affine transform](https://en.wikipedia.org/wiki/Affine_transformation) combines a linear transform and a translation, which is correct:

> If X is the point set of an affine space, then every affine transformation on X can be represented as the composition of a linear transformation on X and a translation of X. 

In summary, the first, second, and fourth choices are correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Deep Learning with Python, Second Edition_](https://amzn.to/3K3VZoy) covers these operations in detail.</p></details>

-----------------------

## Date - 2024-01-15


## Title - One or the other


### **Question** :

Lilith is working on a classification model and wants to find a way to evaluate it depending on whether precision or recall is more important to her client.

She is using the Fβ score.

Lilith understands how to give more weight to precision or recall, but she needs to consider the case where her client only cares about one of the metrics regardless of the value of the other.

**How can Lilith do this?**


### **Choices** :

- If Lilith wants to consider precision only, she should use β = 0.
- If Lilith wants to consider precision only, she should use β < 1.
- If Lilith wants to consider recall only, she should use β > 1.
- If Lilith wants to consider recall only, she should set β = infinity.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Here is the Fβ score formula:

![FBeta-Score](https://user-images.githubusercontent.com/1126730/193036770-0a37f6f7-9a5d-4be3-9b89-7018931140eb.jpg)

The Fβ score lets us combine precision and recall into a single metric. When using β = 1, we place equal weight on precision and recall. For values of β > 1, recall is weighted higher than precision; for values of β < 1, precision is weighted higher than recall.

If Lilith's client wants only to consider precision regardless of the recall value, Lilith can use β = 0. Notice how the Fβ score becomes precision when β = 0. Conversely, If Lilith's client wants only to consider recall regardless of the precision value, Lilith can use β = infinite.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["What is the F-Score?"](https://deepai.org/machine-learning-glossary-and-terms/f-score) is a short introduction to this metric.* ["Micro, Macro & Weighted Averages of F1 Score, Clearly Explained"](https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f) is a great article covering how to compute a global F1-Score metric in multi-class classification problem.* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2024-01-16


## Title - Binary features


### **Question** :

Your mission is to build a decision tree.

You'll work with a dataset where every feature has a value of 0 or 1. The dataset can have any number of features.

You want the decision tree to learn a function that outputs how many features in a sample have a value of 0.

**Assuming the dataset has _n_ rows and _d_ features, how many leaf nodes would your decision tree have?**


### **Choices** :

- 2ⁿ leaf nodes
- 2ᵈ leaf nodes
- 2n leaf nodes
- 2d leaf nodes


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>For a dataset with d features, you will require a decision tree with 2ᵈ leaf nodes.

As an example, imagine we have a dataset with a single feature. The decision tree will need 2 leaf nodes: one to capture the case where the feature has a value of 0 and another to capture when the feature has a value of 1:

![Decision tree with one feature](https://user-images.githubusercontent.com/1126730/197406448-5dda78a5-0011-4525-aa6b-084ed46805a4.jpg)

For 2 features, the possible combinations are four: both features are 0, the first feature is 0 and the second is 1, the first feature is 1, and the second is 0, and both features are 1. The possible resultant count is 0, 1, or 2:

![Decision tree with two features](https://user-images.githubusercontent.com/1126730/197406460-edb7b72f-246f-4c53-8598-d5da4fd7c01c.jpg)

Generally, for _d_ features, the decision tree will need 2ᵈ leaf nodes.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Decision tree learning"](https://en.wikipedia.org/wiki/Decision_tree_learning) is the Wikipedia page where you can read about decision trees.- ["How to tune a Decision Tree?"](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680) is a great article talking about different ways to tune a decision tree, including the effects of setting the maximum depth hyperparameter.</p></details>

-----------------------

## Date - 2024-01-17


## Title - Not enough experience


### **Question** :

Gabriela works for Microsoft and is trying to build a machine-learning model that can accurately classify emails as spam or not spam. 

She knows that choosing the right hyperparameters can greatly improve the model's performance, but she doesn't have much experience with hyperparameter tuning.

**Which of the following statements summarizes the core goal of hyperparameter tuning?**


### **Choices** :

- Hyperparameter tuning is about choosing the set of optimal samples from the data to train a model.
- Hyperparameter tuning is about choosing the set of optimal features from the data to train a model.
- Hyperparameter tuning is about choosing the set of hypotheses that better fit the goal of the model.
- Hyperparameter tuning is about choosing the optimal parameters for a learning algorithm to train a model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We use the term "hyperparameter" to refer to the settings we can use to control the learning process. We set these "knobs" and "levers" before training a model. In contrast, we use "parameters" to refer to variables internal to the model whose values we estimate (learn) during the learning process using data.

A good way of thinking about this:

* Parameters: We learn their values during training. We do not set their values manually.
* Hyperparameters: The settings we fix before the learning process. We cannot learn these values during training.

Each model has different hyperparameters. For example, you can control the depth of a decision tree or the step size during the optimization process of a neural network.

Understanding this should be enough to analyze the four choices for this question.

Hyperparameters have nothing to do with the data. They aren't about features or samples. A good set of hyperparameters will indirectly lead to a better-fitted model, but "tuning hyperparameters" is not about "choosing a better hypothesis."

The correct answer is choosing the best parameters to tune a learning algorithm.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Overview of hyperparameter tuning"](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview) is a great introduction to hyperparameters and the process of finding their optimal value.* [Hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization)* [What is the Difference Between a Parameter and a Hyperparameter?](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)</p></details>

-----------------------

## Date - 2024-01-18


## Title - Fraud detection


### **Question** :

Milani's team has been working on a fraud detection algorithm for months.

They used the best practices and followed all the necessary steps: they split their data into training and test sets, they used a variety of metrics to evaluate their model, they balanced their dataset, and they reviewed their examples regularly to ensure there were no labeling errors.

Finally, the model was ready, and Milani's team deployed it to production.

The algorithm was a huge success. The team received much praise, and the number of fraud cases detected increased by more than 50% in the first month.

However, Milani soon noticed something strange. All the fraud cases detected by the model were in the US.

**What is the most likely reason for this problem?**


### **Choices** :

- The model is too simple and couldn't learn the entire dataset of fraud cases, leaving out those from other countries.
- The model is suffering from data or concept drift.
- The model didn't train long enough to capture all the details of different fraud cases.
- The model suffers from sampling bias. It probably didn't include enough examples of fraud cases from other countries.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A common problem in machine learning is that a model that shows promising results during evaluation doesn't perform well when deployed in production.

While there may be various reasons for that, the story above points us in one particular direction.

The first choice, a simple model, can't be the correct answer. If that were the case, the model would be underfitting and give poor results across the board, not only in the US.

Data and concept drift are indeed common problems with models in production. However, they arise when the environment changes over time, and so does the input to the model. In this case, the problem appeared straight after deployment.

Training the model longer is unlikely to solve this problem. The model is already performing well, and this issue only appears in the US. If the model wasn't trained long enough, it would be underperforming in general, not just in a specific region.

This leaves us with the only correct answer: The most likely reason for this problem is that Milani's team didn't have enough data from other countries, so the model struggles to recognize fraud cases from those regions.

This issue is called "sampling bias." It explains why the problem occurred in one particular country. Sampling bias is difficult to detect during development because the data is missing from the training and test datasets, so we can't notice it while evaluating the model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Sampling bias"](https://en.wikipedia.org/wiki/Sampling_bias) for a complete explanation of this problem.</p></details>

-----------------------

## Date - 2024-01-19


## Title - Graphic designer


### **Question** :

Lena is a graphic designer who has been working on a new project for her client. 

She needs to determine the probability that the client will approve of the design she has created. 

To do this, she uses an algorithm that considers several factors, including the number of revisions the client has requested in the past and the overall design quality. The equation for the algorithm is:

```
probability = σ(a + 0.27b + 0.18c)
```

Where:

* `a`: Indicates the number of revisions the client has requested in the past.
* `b`: Indicates the overall quality of the design on a scale from 1 to 10.
* `c`: Indicates the length of time Lena has spent working on the design.
* `σ`: Is the sigmoid function.

**Assuming the client has requested two revisions in the past, the design has a quality rating of 8, and Lena has spent 10 hours working on the design, what would be the probability that the client will approve of the design?**


### **Choices** :

- The probability is `0.99`
- The probability is `0.81`
- The probability is `0.62`
- The probability is `0.45`


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We need to understand how to compute the Sigmoid function to answer this question. The Sigmoid function takes a value as input and outputs another real value between `0` and `1`. We say that Sigmoid "squeezes" the input into that range.

Here is the formula:

![Sigmoid](https://user-images.githubusercontent.com/1126730/196755518-311dd425-676e-4c85-be1f-467694879c30.jpg)

In Python, the Sigmoid function looks like this:

```
import math

def sigmoid(x):
    return 1/(1 + math.exp(-x))
```

The probability that the client will approve the design can be calculated by plugging the appropriate values for `a`, `b`, and `c` into the equation. In this case, `a` would be 2,`b` would be 8, and `c` would be 10. The equation would look like this:

```
probability = sigmoid(2 + 0.27 * 8 + 0.18 * 10)
probability = 0.9974267268461897
```

Therefore, the probability that the client will approve the design is approximately 0.99.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For a complete description of the Sigmoid function, check the ["Logistic function"](https://en.wikipedia.org/wiki/Logistic_function) Wikipedia page.</p></details>

-----------------------

## Date - 2024-01-20


## Title - Early stopping


### **Question** :

Train a model for too long, and it will stop generalizing appropriately. Don't train it long enough, and it won't learn.

That's a critical tradeoff when building a machine learning model, and finding the perfect number of iterations is essential to achieving the results we expect.

Early stopping is a technique that helps.

**Which of the following statements about early stopping is correct?**


### **Choices** :

- Early stopping is a regularization technique that increases the model's generalization error.
- Early stopping is a regularization technique that under-constraints your model.
- Early stopping is a regularization technique that prevents overfitting.
- Early stopping is a regularization technique that prevents underfitting.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Early stopping](https://articles.bnomial.com/early-stopping) looks for a specific metric and stops the training process when it realizes that the metric is going in the wrong direction. Its goal is to prevent the model from overfitting, but it doesn't do anything to avoid underfitting.

Many experts recommend leaving the model under-constrained when using early stopping. That way, we can find the exact number of epochs when the model starts overfitting without having any other regularization technique obscuring those results. That, however, doesn't mean that early stopping under-constrains a model, so the second choice is incorrect.

Finally, early stopping helps the model generalize to unseen data, opposite to the first choice argument.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Early stopping"](https://articles.bnomial.com/early-stopping) for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models.</p></details>

-----------------------

## Date - 2024-01-21


## Title - Random splits


### **Question** :

Saylor is building a classification model.

She labeled the dataset and randomly split the data into training, validation, and testing sets. 

After training and evaluating the model, Saylor realized the results were too good to be true. Deploying the model to production validated her assumption: the model was a failure.

After some research, Saylor discovered the problem was with the random split. 

**Which of the following kinds of data is susceptible to this problem?**


### **Choices** :

- Time series data.
- Data that doesn't change over time.
- Data that show up in clusters, like news articles.
- Data where individual samples are correlated.


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Any dataset with correlation or groupings between individual samples is not a good candidate for random splitting. When used, data from each group will be present in each set, leading to a leaky validation strategy. Here is an excerpt from [The Kaggle Book](https://amzn.to/3kbanRb):

> In a leaky validation strategy, the problem is that you have arranged your validation strategy in a way that favors better validation scores because some information leaks from the training data.

This problem can happen anytime the data is grouped. Data that doesn't change over time is usually fine for random splits.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.* ["Target Leakage in Machine Learning"](https://www.youtube.com/watch?v=dWhdWxgt5SU) is a YouTube presentation that covers leakage, including during the partitioning of a dataset.</p></details>

-----------------------

## Date - 2024-01-22


## Title - Regular process


### **Question** :

Elise received a massive dataset for her new project.

Unfortunately, there were too many features, so Elise spent quite a bit of time performing feature selection and creating a new dataset containing only the new features.

After she finished, Elise started her regular process: split the data, set the testing aside, and train a classifier on the training set.

**What do you think about Elise's setup?**


### **Choices** :

- Elise's setup is not valid.
- Elise's setup is problematic.
- Elise's setup should work as expected.
- We need more information to decide.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>There's a problem with Elise's setup.

She used the entire dataset for feature selection. She should have split the dataset first and performed feature selection on the training and testing data separately. 

When doing feature selection on the entire dataset, Elise risks leaking information from the soon-to-be test data into the model. A leak will lead to a model that performs too well on the existing data but will do poorly on future unseen data.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check [Data Leakage in Machine Learning](https://machinelearningmastery.com/data-leakage-machine-learning/) for an introduction to data leaks and how to prevent them. * ["Train, Validation, Test Split for Machine Learning"](https://blog.roboflow.com/train-test-split/) goes into detail about the importance of each split.</p></details>

-----------------------

## Date - 2024-01-23


## Title - Fighting overfitting


### **Question** :

What worries Kamila more than anything else is dealing with models that overfit. She never had apparent issues with underfitting, but regularizing a model that doesn't work well on production data is always a chore.

But Kamila's problem wasn't the process of fixing the model but the underlying theory of why that process worked in the first place.

So here she is, staring at two different regularization techniques. The first is L1, while the second is L2 regularization.

**Which of the following illustrates how these two techniques work? Select all that apply.**


### **Choices** :

- L1 regularization uses the sum of absolute values of the weights to penalize the loss function.
- L1 regularization uses the sum of squares of the weights to penalize the loss function.
- L2 regularization uses the sum of absolute values of the weights to penalize the loss function.
- L2 regularization uses the sum of squares of the weights to penalize the loss function.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>L1 regularization helps with overfitting by shrinking the model's parameters toward zero, making certain features irrelevant and preventing the model from using them to make predictions. L1 regularization uses the sum of absolute values of the weights to penalize the loss function.

On the other hand, L2 regularization fights overfitting by forcing weights to be small but not exactly zero. That means that the model can still use irrelevant features to make predictions, but the overall impact of those features will be limited. L2 regularization uses the sum of squares of the weights to penalize the loss function.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Fighting Overfitting With L1 or L2 Regularization: Which One Is Better?"](https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization) will give you a complete introduction to L1 and L2 regularization.</p></details>

-----------------------

## Date - 2024-01-24


## Title - Dependent Nuclear reactor


### **Question** :

Joanna is the structural engineer in charge of designing a nuclear reactor. Her team plans to build the plant next to the coast, so they need to understand the probability and effects of natural disasters.

After consulting with local experts, they know that the probability of a large earthquake in the area is `10^-4`, and the probability of a tsunami is also `10^-4`. However, they think a tsunami always happens whenever a large earthquake occurs.

**What's the probability of having both an earthquake and a tsunami simultaneously?**


### **Choices** :

- The probability of having both events simultaneously is `10^-8`.
- The probability of having both events simultaneously is `10^-5`.
- The probability of having both events simultaneously is `10^-4`.
- The probability of having both events simultaneously is `1`.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We can compute the probability of having both an earthquake and a tsunami simultaneously using the following formula:

```
P(T and E) = P(E) * P(T|E)
```

We know the probability of an earthquake `P(E) = 10^-4`, and the probability of a tsunami `P(T)` is also `10^-4`. We also know these events are dependent, and the probability of a tsunami given that an earthquake happens `P(T|E)` is `1`.

We can now substitute the values in our initial equation:

```
P(T and E) = P(E) * P(T|E)
P(T and E) = 10^-4 * 1
P(T and E) = 10^-4
```</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Cartoon Guide to Statistics_](https://amzn.to/3eg9iIo) is a fun and instructive introduction to probabilities and statistics.</p></details>

-----------------------

## Date - 2024-01-25


## Title - Approaching a problem


### **Question** :

Alexandria is a university professor who wants to build an algorithm to predict whether a student will pass or fail a course based on their past academic performance. 

She has access to a labeled dataset with detailed academic information from thousands of students, including their grades and whether they passed or failed their courses.

Alexandria is not sure which approach to take and would like some guidance.

**Understanding that there are many ways to approach a problem, what would be your first recommendation to Alexandria?**


### **Choices** :

- The best way to approach this problem is with Unsupervised Learning by using a clustering algorithm.
- The best way to approach this problem is with Supervised Learning by using a regression algorithm.
- The best way to approach this problem is with Supervised Learning by using a classification algorithm.
- The best way to approach this problem is with Reinforcement Learning.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Alexandria is trying to answer a question with only two possible answers: Is a student going to pass or fail?

There could be many ways to approach this problem, but we can go with what we know in this case.

Alexandria has access to labeled data, so she could set some of her data aside for testing purposes and build a binary classification model to predict a binary target value. This target could be whether or not a student passes or fails a course.

This is likely the more straightforward way to approach Alexandria's problem: A Supervised Learning binary classification model.

We could also frame this as a regression problem depending on the data we have for each student, but nothing in the description points to that being a good strategy.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Supervised and Unsupervised Machine Learning Algorithms"](https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/) for an explanation of supervised and unsupervised learning algorithms.</p></details>

-----------------------

## Date - 2024-01-26


## Title - Critical solution


### **Question** :

Lola has been building machine learning systems for a decade, and her specialty is neural networks.

Lola maintains a critical solution that helps her company make money: a multi-class classification model that she and her team have worked on for a few years.

Almost every layer of Lola's model uses ReLU except the last layer, which uses softmax.

**Which of the following statements are true about the softmax activation function when used in the output layer of a neural network?**


### **Choices** :

- The softmax function is not differentiable, and that's why Lola can only use it in the output layer of the network.
- The softmax function is differentiable, and that's why Lola can use it in the output layer of the network.
- The softmax function turns a vector of real values into an integer value representing the correct class index.
- The softmax function turns a vector of real values into a vector of probabilities that sum to 1.


### **Answer** :

<details><summary>CLICK ME</summary><p>0101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The softmax function is differentiable, which is an essential property to use its result to optimize a cost function. The function turns a vector of real values into another vector of probabilities that sum to 1. These probabilities are proportional to the relative scale of each value in the vector.

While we can use the resultant vector to determine the correct class index, notice that softmax doesn't output this index directly. Instead, it outputs a vector where the index of the larger value will correspond to the correct class.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- Check ["Softmax Activation Function with Python"](https://machinelearningmastery.com/softmax-activation-function-with-python/) for more information about the Softmax function and a way to implement it.- ["What is the Softmax Function"](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer) is a great summary of this function.</p></details>

-----------------------

## Date - 2024-01-27


## Title - Right after the tutorial


### **Question** :

Lainey wants to start practicing after reading a machine-learning tutorial.

Luckily for her, the instructor left a few ideas and shared some datasets students could use.

Lainey needs to be careful. She has limited experience and wants to focus on something she understands. For now, neural networks combined with a log loss should do the trick.

**Which of the following problems are good candidates for Lainey to practice?**


### **Choices** :

- Given a picture of shoes, predict their corresponding brand.
- Predict whether a subscriber will churn over the next month.
- Predict yearly revenue given the number of products sold over the past 24 months.
- Determine whether a notification message is valid or a spam message.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Log loss is a function that we commonly use in classification problems. It returns the negative logarithm of the product of probabilities.

Log loss indicates how close a prediction probability is to the target value. The more the predicted probability differs from the target value, the higher the log loss.

Lainey needs to focus on classification problems. From the list of options, the only problem that doesn't fit is predicting yearly revenue. That's a regression problem, while the other three options are classification problems.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Understanding binary cross-entropy / log loss: A visual explanation"](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) is an excellent introduction to binary cross-entropy.- Another article that helps understand binary cross-entropy is ["Binary Cross Entropy/Log Loss for Binary Classification"](https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/).</p></details>

-----------------------

## Date - 2024-01-28


## Title - Reporting scores


### **Question** :

It was late, and Amara had to send the report of her last exam.

She didn't have time to do much with the test scores, so she decided to report the median of the scores and leave any further analysis for another day.

Her students' scores were: 5, 5, 3, 4, 5, 2, 5, 5, 2, 5, and 3.

**What is the median value that Amara should report?**


### **Choices** :

- The median value of the dataset is 2.
- The median value of the dataset is 3.
- The median value of the dataset is 4.
- The median value of the dataset is 5.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The mean, median, and mode are different ways to measure the center of a dataset. Each of these metrics tries to summarize a set of values using a single number.

Amara can find the median by looking at the middle number after sorting the scores. In case there are two middle numbers, then she can use the mean of those two numbers.

Amara has 11 scores, and the middle number after sorting the list is `5`.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Mean Median Mode: What They Are, How to Find Them"](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/) for a complete explanation of the Mean, Median, and Mode.</p></details>

-----------------------

## Date - 2024-01-29


## Title - Tanh's interpretation


### **Question** :

Alyssa is looking into activation functions trying to understand how they work.

Sigmoid and Tanh were popular choices, but Alyssa read about how Tanh was the preferred choice for many practitioners.

Here is the Tanh formula:

![Tanh](https://user-images.githubusercontent.com/1126730/197406160-92aa0f03-ede8-43bc-853a-546360bc2287.jpg)

**Which of the following is a correct interpretation of this function?**


### **Choices** :

- The output of the Tanh function could be any real number.
- The output of the Tanh function could be any integer number.
- The output of the Tanh function is a real number between `-1` and `1`.
- The output of the Tanh function is a real number between `0` and `1`.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The Hyperbolic tangent function, or Tanh, takes a value as input and outputs another real value between `-1` and `1`. We say that Tanh "squeezes" the input into that range.

Tanh is continuously differentiable, and its derivative is simple to compute. 

Here is the plot of the Tanh function:

![Tanh plot](https://user-images.githubusercontent.com/1126730/197406179-9f80cd61-2241-489e-98a7-bd63bf74beba.jpg)</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For a complete description of the Tanh function, check the ["Hyperbolic Functions"](https://en.wikipedia.org/wiki/Hyperbolic_functions) Wikipedia page.</p></details>

-----------------------

## Date - 2024-01-30


## Title - Daily question


### **Question** :

Alayna found Bnomial, an online website that posts one machine learning question daily.

Perfect! It was an excellent way for Alayna to practice consistently!

The first question she found was about neural networks:

**Which of the following sentences are true about neural networks?**


### **Choices** :

- We can optimize neural networks using the Gradient Descent algorithm.
- Neural networks can find the optimal solution for convex problems.
- Neural networks can use a mix of activation functions.
- Neural networks can approximate any function when using non-linear activations.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Gradient descent is an iterative optimization algorithm used to find the local minimum of a function. The algorithm works by taking steps proportional to the negative of the function's gradient at the current point. Gradient Descent is an excellent choice to optimize neural networks.

Using Gradient descent, we can find the optimal solution for a convex problem, assuming we configure the algorithm with an appropriate learning rate.

When setting up a neural network, we can use a mix of activation functions. For example, ReLU in the hidden layers and Softmax in the output layer of a classification model.

Finally, thanks to the [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem), we can turn a two-layer neural network into a universal function approximator when using non-linear activation functions.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Gradient Descent For Machine Learning"](https://machinelearningmastery.com/gradient-descent-for-machine-learning/) for a description of the algorithm.* Check the ["Universal approximation theorem"](https://en.wikipedia.org/wiki/Universal_approximation_theorem) on Wikipedia for more information about the power of neural networks.</p></details>

-----------------------

## Date - 2024-01-31


## Title - Rotating beers


### **Question** :

Avianna works at a beer factory where she processes pictures of the bottles before they are shipped out. 

She noticed that the pictures were often taken with different degrees of rotation, and the Convolutional Neural Network she built wasn't ready to handle this.

**Which of the following approaches could Avianna propose to handle rotation in the pictures?**


### **Choices** :

- Adding a data preprocessing step to properly rotate every image before giving the data to the model.
- Clustering rotated images before sending them through the model and using this as a separate class.
- Include rotated versions of the images in the training data to build some rotation invariability into the model.
- Correctly configure the network since Convolutional Neural Networks are inherently rotation invariant.


### **Answer** :

<details><summary>CLICK ME</summary><p>1010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Convolutional Neural Networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) are [translation invariant but not rotation invariant](https://pyimagesearch.com/2021/05/14/are-cnns-invariant-to-translation-rotation-and-scaling/). They can recognize the same patterns independently of where they show in an image, but they don't have the same ability to recognize shapes with various degrees of rotation. 

There's also no out-of-the-box mechanism for a layer to recognize the orientation of an image and much less "rotate" it to its correct position.

Dealing with rotated images is a common problem, especially with user data. The solution usually boils down to the following two options:

1. Training a model that learns to recognize pictures regardless of their orientation.
2. Completely sidestepping the problem and always using images in the appropriate position to train the model.

A way to train our model to recognize pictures regardless of their orientation is to extend the dataset with rotated images. If we expect to see images at 0, 90, and 180 degrees, we must teach our model to recognize them. ["Correcting Image Orientation Using Convolutional Neural Networks"](https://d4nst.github.io/2017/01/12/image-orientation/) is an excellent article covering this approach.

Finally, we can extend the pipeline that feeds the model with a step that ensures images are always in the same position. That way, our model doesn't have to deal with rotation. This may not always be possible, but sometimes we have access to metadata that could help with that—for example, using accelerometer information to straighten the picture.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Correcting Image Orientation Using Convolutional Neural Networks"](https://d4nst.github.io/2017/01/12/image-orientation/) is a great article covering practical ways to get a network to recognize rotated pictures.* ["Are CNNs invariant to translation, rotation, and scaling?"](https://pyimagesearch.com/2021/05/14/are-cnns-invariant-to-translation-rotation-and-scaling/) goes into more detail about whether convolutional neural networks are translation, rotation, and scale invariant.* Check ["How Do Convolutional Layers Work in Deep Learning Neural Networks?"](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/) for an introduction to how convolutional layers work.</p></details>

-----------------------

## Date - 2024-02-01


## Title - Cross-entropy variations


### **Question** :

Fiona found a Keras example online training a neural network and using cross-entropy loss. 

Fiona was familiar with a couple of implementations of the cross-entropy loss: `CategoricalCrossentropy` and `SparseCategoricalCrossentropy.`

**When should you use every one of these implementations of the cross-entropy loss?**


### **Choices** :

- We should use `SparseCategoricalCrossentropy` when the labels in the dataset are one-hot encoded.
- We should use `SparseCategoricalCrossentropy` when the labels in the dataset are integer values.
- We should use `CategoricalCrossentropy` when the labels in the dataset are one-hot encoded.
- We should use `CategoricalCrossentropy` when the labels in the dataset are integer values.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Keras' `SparseCategoricalCrossentropy` computes the cross-entropy loss between the labels and predictions. It works when the labels in the dataset are integer values, for example, `1`, `2`, and `3`.

Keras' `CategoricalCrossentropy`, on the other hand, has the same function but works when the labels in the dataset are one-hot encoded, for example, `[1, 0, 0]`, `[0, 1, 0]`, and `[0, 0, 1]`.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Here is Keras' [SparseCategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) and [CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) documentation.</p></details>

-----------------------

## Date - 2024-02-02


## Title - Connections between units


### **Question** :

A "feedback loop" is when connections between units form a directed cycle, thus creating loops in a neural network. This feature allows networks to save information in the hidden layers.

Only some neural network types support the concept of feedback loops.

**Which of the following support feedback loops?**


### **Choices** :

- Multilayer Perceptron
- Feed Forward Neural Network
- Recurrent Neural Networks
- Convolutional Neural Networks


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Recurrent Neural Networks have an advantage over traditional feed-forward networks when working with time series or data points that depend upon previous samples.

The magic ingredient of Recurrent Neural Networks is the ability to store the information of previous inputs to generate the following sequence output. Recurrent Neural Networks do this by implementing the concept of a "feedback loop" or "feedback connection." These are connections feeding the hidden layers of the neural network back into themselves.

None of the other options support the concept of feedback loops. Only Recurrent Neural Networks do.

[Long Short-Term Memory](https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/) (LSTM) networks are a type of Recurrent Neural Network that is very popular for sequence prediction problems.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [An Introduction To Recurrent Neural Networks And The Math That Powers Them](https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/)* [Recurrent neural network](https://en.wikipedia.org/wiki/Recurrent_neural_network)* [A Gentle Introduction to Long Short-Term Memory Networks by the Experts](https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/)</p></details>

-----------------------

## Date - 2024-02-03


## Title - Dental office


### **Question** :

Arielle worked as a receptionist at a busy dental office, always looking for ways to improve the practice's efficiency. She had recently become interested in machine learning and was eager to learn more about how it could be applied in her work.

Arielle was already busy, so she wanted to maximize her value by spending her study time on the more impactful algorithms that could help her at work.

One of the first lessons she learned was the No Free Lunch Theorem.

**How is the No Free Lunch Theorem relevant to Arielle in this situation?**


### **Choices** :

- It suggests that Arielle should focus on studying the most complex algorithms since they are likely to be the most impactful.
- It suggests that Arielle should focus on studying the simplest algorithms since they are likely to be the most effective.
- It suggests that Arielle should focus on studying supervised learning algorithms since they are the most widely used in machine learning.
- It suggests that Arielle should focus on studying a variety of different algorithms since there is no single algorithm that is universally the best for all possible tasks.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The [No Free Lunch Theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem) states that there is no single machine learning algorithm that is universally the best for all possible tasks. This means that the performance of a machine learning algorithm depends on the specific characteristics of the data and task at hand, and there is no one-size-fits-all solution.

In this situation, Arielle is looking to improve the efficiency of the dental office and wants to use machine learning to help solve specific problems. She knows that the No Free Lunch Theorem means that she will need to carefully consider the specific needs and challenges of the practice when selecting a machine learning algorithm. 

This means that she should focus on studying various algorithms to be prepared to try out different approaches and find the best solution for each problem.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["No Free Lunch Theorem for Machine Learning"](https://machinelearningmastery.com/no-free-lunch-theorem-for-machine-learning/) for a complete explanation of this theorem.</p></details>

-----------------------

## Date - 2024-02-04


## Title - Fortune 500


### **Question** :

Of all the data that Lucille received, she must decide which features will help build a machine-learning model.

Lucille's team is focusing on Fortune 500 companies, and their model will predict what other companies have a shot at joining the list.

**Which of the following columns on the dataset are meaningful features for the model?**


### **Choices** :

- The phone number of the company.
- The company's profits from the previous year.
- The number of employees that work for the company.
- The company's name.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The phone numbers and names will not help predict whether a company has an opportunity of becoming a Fortune 500 company. Lucille should discard these features.

The number of employees and profits from the previous year should be helpful for the model. Large companies that make a lot of profit are excellent indicators to solve this problem.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Feature Engineering for Machine Learning_](https://amzn.to/3SsnLAc) is an excellent book covering feature engineering.* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.</p></details>

-----------------------

## Date - 2024-02-05


## Title - Classifying customers


### **Question** :

Ariyah was working on a machine learning project to predict the likelihood of a customer churning based on their past behavior. She had been using a Decision Tree model to classify customers.

While researching Decision Trees, Ariyah read about pruning, but she didn't understand how it would help her work.

**What is the primary purpose of pruning a Decision Tree?**


### **Choices** :

- To prevent overfitting or the phenomenon where the model fits the training data too closely and performs poorly on new data.
- To reduce the time required for testing the model on new data.
- To conserve space for storing the Decision Tree model on a computer or server.
- To decrease the model's error on the training set.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Decision Trees are a popular machine-learning model for classification and regression tasks. They build a tree-like decision model based on the data's features to predict the output label of a given input sample. However, if the Decision Tree is allowed to grow too large, it can become overly complex and may start to fit the training data too closely, a phenomenon known as overfitting.

Pruning is a technique that can be used to reduce the size of a Decision Tree and prevent overfitting. It involves removing nodes from the tree that are not contributing significantly to the model's accuracy. This can improve the model's generalization performance and reduce the risk of overfitting. 

While pruning a Decision Tree can also save computing time and space for storing the model, the primary purpose of pruning is to avoid overfitting the training set.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Decision tree pruning"](https://en.wikipedia.org/wiki/Decision_tree_pruning) covers the process of pruning a tree to reduce overfitting.* ["How to tune a Decision Tree?"](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680) is a great article talking about different ways to tune a decision tree, including the effects of setting the maximum depth.</p></details>

-----------------------

## Date - 2024-02-06


## Title - Intensive Care Unit


### **Question** :

Briella was working at a hospital, where she was tasked with developing a machine learning model to predict whether a patient will be admitted to the intensive care unit (ICU). 

She had access to medical data for thousands of patients, with labels indicating whether each patient was admitted to the ICU or not. 

After studying the available techniques, Briella had several options for building a binary classification model. 

**Which of the following techniques can she use to build a binary classification model?**


### **Choices** :

- Briella can use logistic regression.
- Briella can use k-nearest neighbors.
- Briella can use neural networks.
- Briella can use decision trees.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The four choices are examples of supervised learning techniques that you could use to build a binary classification model.

Logistic regression is a popular technique for binary classification that uses a linear model to predict the likelihood of an outcome. 

K-nearest neighbors is a non-parametric technique that uses a similarity measure to classify data points based on their neighbors. It is often used for classification tasks, including binary classification.

Neural networks can learn complex patterns in the data and can be used for a wide range of tasks, including binary classification.

Decision trees are a type of machine learning model that uses a tree-like structure to make predictions. They are simple to understand and can be effective for binary classification tasks.

Although they all work, they are very different approaches, and some might lead to better results on this dataset than others.

However, the problem doesn't give us any information about the characteristics of the data, so it's hard to disqualify any of the choices based on what we know.

Therefore, every choice is a correct answer.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check out ["Binary classification"](https://en.wikipedia.org/wiki/Binary_classification) in Wikipedia for more information.* ["4 Types of Classification Tasks in Machine Learning"](https://machinelearningmastery.com/types-of-classification-in-machine-learning/) is a great article going over different ways to implement a classification model.</p></details>

-----------------------

## Date - 2024-02-07


## Title - Human-like language


### **Question** :

Madilyn works in the natural language processing department at a tech company. She has been tasked with building a model that can understand and generate human-like language.

Madilyn decides to use a transformer model for this task. She knows that transformers are a type of deep learning model that has been proven effective in natural language processing tasks.

**Which of the following are characteristics of transformers?**


### **Choices** :

- Transformers use attention mechanisms to allow the model to focus on specific parts of the input.
- Transformers can handle variable-length input and output sequences.
- Transformers are based on recurrent neural networks and use feedback loops to process the input data.
- Transformers can only work with categorical data.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Transformers are a type of deep learning model widely used in Natural Language processing tasks, such as machine translation and text summarization. They were introduced in the 2017 paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al.

One of the key characteristics of transformers is their use of attention mechanisms. Attention allows the model to focus on specific input parts, which is useful when processing long data sequences. This differs from traditional recurrent neural networks, which use feedback loops to process the input data.

Another characteristic of transformers is their ability to handle variable-length input and output sequences. This is useful when dealing with natural language data, which often has varying lengths.

Transformers are not limited to working with categorical data. They can also process numerical data.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check the ["Attention is All You Need"](https://arxiv.org/abs/1706.03762) paper by Vaswani et al. for an introduction to Transformers.* The [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) is a fantastic visual guide that explains the inner workings of the Transformer architecture in an easy-to-understand manner.</p></details>

-----------------------

## Date - 2024-02-08


## Title - Tom's definition


### **Question** :

Using Tom M. Mitchell’s definition of Machine Learning: 

> "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."

**Which of the following is the correct breakdown for the problem of recognizing handwritten digits?**


### **Choices** :

- **Task T:** Classifying handwritten digits. **Performance Measure P:** Number of correctly classified digits. **Experience E:** A dataset of handwritten digits.
- **Task T:** Classifying handwritten digits. **Performance Measure P:** Percentage of correctly classified digits. **Experience E:** Number of classified digits.
- **Task T:** Classifying handwritten digits. **Performance Measure P:** Percentage of correctly classified digits. **Experience E:** A dataset of handwritten digits and their corresponding classifications.
- **Task T:** Classifying handwritten digits. **Performance Measure P:** Percentage of correctly classified digits. **Experience E:** A dataset of handwritten digits.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The correct breakdown for the problem of recognizing handwritten digits is "**Task T:** Classifying handwritten digits. **Performance Measure P:** Percentage of correctly classified digits. **Experience E:** A dataset of handwritten digits and their corresponding classifications."

The **Performance Measure P** can't be the number of classified digits because that metric doesn't measure the quality of the model.

The **Experience E** can't be the number of classified digits. Instead, it should be a dataset of digits, including their correct classification.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Machine Learning"](https://deepai.org/machine-learning-glossary-and-terms/machine-learning) for an introduction to machine learning, including Tom Mitchel's original framing.</p></details>

-----------------------

## Date - 2024-02-09


## Title - Large Models


### **Question** :

Gracelyn was a curious computer science student who had always been fascinated by natural language processing. 

When she heard about the new Large Language Models (LLMs) that had recently been developed, she knew she had to learn more.

At the time, some of the most popular LLMs were BERT, with 110 million parameters, GPT-3, with 175 billion parameters, and PaLM, with 540 billion parameters.

But Gracelyn wasn't sure what exactly do these parameters mean.

**Which of the following is the meaning of "parameters" in the context of  Large Language Models (LLM)?**


### **Choices** :

- The different learning rate values that were used to train the model.
- The size of the input vocabulary used to train the model.
- The number of connections between the neurons of the model.
- The size of the attention mechanism used by the model.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In the context of Large Language Models (LLM), "parameters" refers to the number of connections between the neurons in the model. The number of parameters determines the complexity of the model and the amount of information it can store and process.

Something to keep in mind is that the larger the number of parameters, the more sophisticated the model is, but the more computationally expensive will be to train and use.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_Neural Networks and Deep Learning_](http://neuralnetworksanddeeplearning.com/index.html) is a free online book written by [Michael Nielsen](https://twitter.com/michael_nielsen) with a great introduction to neural networks.</p></details>

-----------------------

## Date - 2024-02-10


## Title - Theater department


### **Question** :

Vivienne works in the data science department of a theater. She is tasked with building a model to predict which plays will be successful based on their genre, cast, and other factors.

Vivienne decides to use a decision tree for this task. She knows that decision trees are powerful models that can help her understand the relationships between the input features and the target variable.

**Which of the following are characteristics of decision trees?**


### **Choices** :

- Decision trees are non-parametric models that make no assumptions about the data distribution.
- Decision trees can handle both numerical and categorical data.
- Decision trees split the data into subsets based on the most important features.
- Decision trees are easy to interpret and explain to non-technical stakeholders.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Decision trees are a popular machine-learning technique that is used for classification and regression tasks. They are a type of non-parametric model, which means that they make no assumptions about the distribution of the data. This allows decision trees to capture complex patterns in the data without requiring us to make any assumptions.

Decision trees can handle numerical and categorical data, making them versatile tools for many real-world problems. They split the data into subsets based on the most important features, and this process continues until the data is fully classified or the tree reaches the maximum depth.

One of the benefits of decision trees is that they are easy to interpret and explain to non-technical stakeholders. The tree structure shows the sequence of decisions made to classify the data, which can help us understand the relationships between the input features and the target variable.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Decision tree learning"](https://en.wikipedia.org/wiki/Decision_tree_learning) is the Wikipedia page where you can read about decision trees.* The Scikit-Learn's ["Decision Trees"](https://scikit-learn.org/stable/modules/tree.html) page contains an extensive list of advantages and disadvantages of decision trees.</p></details>

-----------------------

## Date - 2024-02-11


## Title - Playing Stratego


### **Question** :

Remington was always eager to try new strategies while playing Stratego. She believed that the key to success was to constantly adapt and improve.

Remington had a strategy for winning her Stratego matches: Sometimes, she would make a random move. Other times, she would only make moves that she knew would give her an advantage. Over time, Remington became skilled at predicting her opponent's moves and choosing the best course of action.

Remington, a reinforcement learning engineer, described her approach as a "0.6 epsilon greedy policy."

**Which of the following correctly summarizes Remington's approach while playing Stratego?**


### **Choices** :

- Remington makes a random move 60% of the time and only makes moves she knows will give her an advantage 40% of the time.
- Remington makes a random move 40% of the time and makes moves she knows will give her an advantage 60% of the time.
- Remington emphasizes exploiting her knowledge more than trying new strategies.
- Remington usually makes moves that she knows will give her an advantage and otherwise makes random moves.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>In reinforcement learning, an ["epsilon greedy policy"](https://developers.google.com/machine-learning/glossary#epsilon-greedy-policy) refers to a strategy that either follows a random policy with epsilon probability or a greedy policy otherwise.  

A [greedy policy](https://developers.google.com/machine-learning/glossary#greedy-policy) always chooses the action with the highest expected return. In Remington's case, this would be executing the strategies she already knows. She explores and discovers new strategies when following a [random policy](https://developers.google.com/machine-learning/glossary#random_policy).

Remington describes her approach as a "0.6 epsilon greedy policy" because she follows a random strategy about 60% of the time. In other words, Remington spends most of her time exploring rather than exploiting what she already knows.

As an additional note, in reinforcement learning, after accumulating knowledge from exploring, the algorithm reduces the value of epsilon to shift from following a random to a greedy policy.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["The exploration-exploitation trade-off: intuitions and strategies"](https://towardsdatascience.com/the-exploration-exploitation-dilemma-f5622fbe1e82) is a great introduction to the exploration vs. exploitation tradeoff.* Check Google's machine learning glossary definition of ["Epsilon greedy policy"](https://developers.google.com/machine-learning/glossary#epsilon-greedy-policy).</p></details>

-----------------------

## Date - 2024-02-12


## Title - Dataset statistics


### **Question** :

The MNIST dataset is a collection of handwritten digit images that has been modified from two datasets originally collected by the National Institute of Standards and Technology in the United States. 

It is a popular dataset and can be easily loaded in a [Google Colab](https://colab.research.google.com/) notebook using the provided code snippet:

```
from keras.datasets import mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()
```

The dataset contains every digit from 0 to 9.

**Could you help us determine what of the following statements are correct about the dataset?**


### **Choices** :

- The digit 1 has the most images in the dataset, totaling 7,877 images.
- The digit 0 has the least images in the test set, totaling 600 images.
- The train and test sets contain the same number of images.
- The digit 3 has 6,131 images in the train set.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We need to write some code to answer this question. Let's start by [concatenating](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) the train and test sets:

```
import numpy as np
labels = np.concatenate((y_train, y_test))
```

Notice that we don't need to worry about `X_train` and `X_test` because those arrays contain the images. We can answer the question by looking at the labels only.

[Numpy's `unique()` function](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) will allow us to group and count the labels:

```
digit, count = np.unique(labels, return_counts=1)
```

If we print the count corresponding to digit 1, we will find out that there are 7,877 images:

```
print(count[1])
```

To determine how popular each digit is, we can print the entire `count` array and get the following result: `[6903, 7877, 6990, 7141, 6824, 6313, 6876, 7293, 6825, 6958]`. Notice that digit 1 is indeed the most popular in the dataset.

We can check how many instances of digit 0 in the test set with the following code that will print 980:

```
print(np.where(y_test == 0)[0].shape[0])
```

We can find the total number of images on each set by printing `y_train.shape[0]` and `y_test.shape[0]`. The result will be 60,000 and 10,000, respectively.

Finally, we can check how many instances of digit 3 in the train set with the following code that will print 6,131:

```
print(np.where(y_train == 3)[0].shape[0])
```</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For more information, you can check Numpy's [concatenate](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) and [unique](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) functions.</p></details>

-----------------------

## Date - 2024-02-13


## Title - Returning a product


### **Question** :

Nina was working on a machine learning model to predict the likelihood of a customer returning a product. She was hired by a retailer that wanted to use her model to improve customer service.

After several weeks of development, Nina had two models that performed well on the validation data, but she wanted to pick only one.

**What is the best way for Nina to decide which model is the best overall?**


### **Choices** :

- Nina should compute the f1-score for both models and choose the higher value.
- Nina should compute the f1-score for both models and choose the lower value.
- Nina should compute the Recall for both models and choose the higher value.
- Nina should compute the Precision for both models and choose the higher value.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The best way for Nina to decide which model is the best overall is to compute the f1-score for both models and choose the higher value. 

The model's f1-score is the harmonic mean between precision and recall. We can use the following formula to compute it:

![F1-Score](https://articles.bnomial.com/images/article-when-accuracy-doesnt-help-3.jpg)

While precision and recall are important metrics, they do not provide a complete picture of its overall performance. Precision measures the proportion of true positive predictions made by the model, while recall measures the proportion of actual positive cases correctly predicted by the model. While a high precision indicates that the model is good at avoiding false positives, a high recall indicates that the model is good at finding all of the positive cases.

In the case of Nina's models, a high precision might indicate that the model is good at predicting which customers are likely to return a product, but it does not necessarily mean that the model is good at finding all of the customers who are likely to return a product. Similarly, a high recall might indicate that the model is good at finding all the customers who are likely to return a product, but it does not necessarily mean that the model is good at avoiding false positives.

Therefore, while precision and recall are important metrics to consider when evaluating a model's performance, they should not be used in isolation to determine which model is the best overall. The F1 score is a better metric to use because it provides a balanced view of a model's precision and recall and allows Nina to choose the model that has the best overall performance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Micro, Macro & Weighted Averages of F1 Score, Clearly Explained"](https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f) is a great article covering how to compute a global F1-Score metric in multi-class classification problem.* Check ["When accuracy doesn't help"](https://articles.bnomial.com/when-accuracy-doesnt-help) for an introduction to precision, recall, and f1-score metrics to measure a machine learning model's performance.</p></details>

-----------------------

## Date - 2024-02-14


## Title - Stable suggestions


### **Question** :

When Vera read the suggestions from her teammates, it wasn't immediately clear what they were trying to tell her.

Vera deployed a model using k-Nearest Neighbors (KNN). She suggested exploring different values of `K` to improve the results. In response, her teammates sent a few suggestions.

Specifically, they mentioned that increasing the value of `K` will make the predictions more stable, while the opposite would happen if Vera decreased it.

**Vera wasn't sure what exactly they meant by "stable." What do you think they meant?**


### **Choices** :

- Vera's teammates were referring to the bias of the algorithm.
- Vera's teammates were referring to the variance of the algorithm.
- Vera's teammates were referring to the dimensionality of the algorithm.
- Vera's teammates were referring to the speed of the algorithm.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Vera's teammates were referring to the variance of the algorithm. More variance means the algorithm's results will be less stable, while less variance means the results will be more stable.

The smaller the value of `K`, the more variance and less bias KNN will exhibit. For example, if we use `K = 1`, a single sample close to our observation will cause the algorithm to return the wrong prediction. Imagine an observation surrounded by many instances from class A and only one from class B that's closer than everything else. Since `K=1`, the algorithm will incorrectly predict the observation as class B. 

Conversely, the larger the value of `K`, the less variance and more bias KNN will exhibit. Since KNN uses an average or majority voting, no individual sample will cause the algorithm to return the wrong prediction. Setting `K` to a value that's too large will make the algorithm underfit because it won't capture the variance in the dataset.

Here is a quote from ["Why Does Increasing k Decrease Variance in kNN?"](https://towardsdatascience.com/why-does-increasing-k-decrease-variance-in-knn-9ed6de2f5061):

> If we take the limit as `K` approaches the size of the dataset, we will get a model that just predicts the class that appears more frequently in the dataset [...]. This is the model with the highest bias, but the variance is 0 [...]. High bias because it has failed to capture any local information about the model, but 0 variance because it predicts the exact same thing for any new data point.

In summary, the smaller the value of `K` is, the lower the bias and the higher the variance. The larger the value of `K` is, the higher the bias and the lower the variance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Why Does Increasing k Decrease Variance in kNN?"](https://towardsdatascience.com/why-does-increasing-k-decrease-variance-in-knn-9ed6de2f5061) is a great article diving into the relationship of `K` and the variance of KNN.* For a more general introduction to the bias-variance trade-off, check ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/).</p></details>

-----------------------

## Date - 2024-02-15


## Title - Every book is the same


### **Question** :

I have a ton of machine learning books at home.

Great books, most of them, but I can't help but notice that most start their story precisely at the same place:

Linear and Logistic Regression.

I get it. Understanding the fundamentals is essential, but at some point, most books should move on and cover different and more exciting topics.

**In the meantime, I have to ask you to select which of the following statements are true about these two techniques:**


### **Choices** :

- In Linear Regression, we find the best line that predicts the output. In Logistic Regression, we use a sigmoid curve to classify the samples.
- We use Linear Regression for solving Regression problems. We use Logistic Regression for solving Classification problems.
- In Linear Regression, we predict the values of continuous variables. In Logistic Regression, we predict the values of categorical variables.
- Linear Regression uses a set of independent variables to predict a continuous dependent variable. Logistic Regression uses a set of dependent variables to predict a categorical independent variable.


### **Answer** :

<details><summary>CLICK ME</summary><p>1110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Linear Regression is probably the most popular Supervised Learning technique in machine learning. Its goal is to fit the best line through the data to predict a continuous output. Logistic Regression is also a Supervised Learning technique, with the difference that we use a sigmoid function to map values between 0 and 1. By doing this, we can use Logistic Regression to predict a categorical variable.

In Regression problems, we need to predict continuous outcomes, for example, a person's age, salary, or home price. Linear Regression helps with that. In Classification problems, we need to predict a categorical outcome, for example, whether the person is sick or whether they will get a job. We can use Logistic Regression for that.

Finally, Linear and Logistic Regression use a set of independent variables to predict a dependent variable. In Linear Regression, the dependent variable is continuous, while the dependent variable is categorical in Logistic Regression. Therefore, the final choice of this question is incorrect.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Linear Regression for Machine Learning"](https://machinelearningmastery.com/linear-regression-for-machine-learning/) is an introduction to Linear Regression.* ["Logistic Regression for Machine Learning"](https://machinelearningmastery.com/logistic-regression-for-machine-learning/) is an introduction to Logistic Regression.</p></details>

-----------------------

## Date - 2024-02-16


## Title - Guessing the model


### **Question** :

Octavia had a problem with her model and decided to investigate.

She split her dataset into training and testing. Set the testing data aside and created four other random splits from the training data, each with 25% of the samples.

She then trained a model on each of the four groups and tested them on the testing data to determine the accuracy of each model. She grabbed the results and computed the variance between them.

Here is what Octavia's code looked like:

![Pseudocode](https://user-images.githubusercontent.com/1126730/199803699-e056bcea-133f-45b1-9c52-8e68d75f5bed.png)

After running her function, Octavia found out that the result was a value much higher than what she expected.

**If you had to guess, which model do you think Octavia is using?**


### **Choices** :

- Octavia could be using a Decision Tree.
- Octavia could be using k-Nearest Neighbors.
- Octavia could be using Linear Regression.
- Octavia could be using Logistic Regression.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>After Octavia trains each model and computes its accuracy on the testing set, a couple of things could happen:

1. The accuracy of each model is significantly different.
2. The accuracy of each model is relatively similar.

We know that Octavia split the training dataset into four random groups, so we should expect each of these groups to be similar. If the model's performance on each differs significantly, we know Octavia is using a high-variance model.

On the other hand, if the model's performance is very similar across all four groups, we know we are looking at a stable, high-bias model that doesn't change much with new data.

The variance between the four accuracies is high, so Octavia must be using a high-variance model. From the list, both Decision Trees and k-Nearest Neighbors are high-variance models.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning"](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) for an introduction to bias and variance.* The Wikipedia page on bias and variance is also a good resource: ["Bias–variance tradeoff"](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).</p></details>

-----------------------

## Date - 2024-02-17


## Title - Juice packing


### **Question** :

Winter works at a juice-packing company where she uses a machine-learning model to predict the demand for different juice flavors.

After training and evaluating the model, Winter was confident that it was ready for production. The model was deployed, and the company started using it to plan its production and distribution.

During the first few months, everything was working as expected. But then, the company noticed that the model consistently overestimated the demand for certain flavors. 

**What could be the cause of the problem with the model?**


### **Choices** :

- The model is underfitting and needs more complexity.
- The model is overfitting and needs more regularization.
- The model is suffering from data drift.
- The model is suffering from sampling bias.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Machine learning models are trained on a historical dataset and make predictions based on that data. If the data distribution changes over time, the model can become less accurate and give poor predictions. This phenomenon is known as data drift.

Data drift can occur for various reasons, such as changes in the underlying process that generates the data, changes in the environment, or changes in how the data is collected. In this case, it is likely that the demand for certain flavors of juice has changed over time, and the model is not capturing that change.

Underfitting, overfitting, and sampling bias are not relevant in this scenario. Underfitting occurs when the model is too simple and cannot capture the underlying relationship in the data. Overfitting occurs when the model is too complex and captures noise in the data instead of the relationship. Sampling bias occurs when the training data does not represent the entire population. In this case, the problem appeared after a few months the model was deployed, so underfitting, overfitting, and sampling bias are not the cause.

The most likely explanation for the problem is that the model suffers from data drift and needs to be retrained on more recent data to capture the changes in the demand for juice flavors.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Data Drift vs. Concept Drift: What Are the Main Differences?"](https://deepchecks.com/data-drift-vs-concept-drift-what-are-the-main-differences/) is a great introduction to data and concept drift.* ["Why You Should Care About Data and Concept Drift" ](https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift) is a great article from [Evidently AI](https://evidentlyai.com/) focusing on the importance of monitoring your models.</p></details>

-----------------------

## Date - 2024-02-18


## Title - Student performance


### **Question** :

Jayla is a data science student at a top university. 

She has been working on a project to build a model that predicts student success. She uses a deep learning model to analyze past student performance data and predict how well current students will do.

Unfortunately, Jayla has struggled to get her model to perform well. She has tried different techniques and hyperparameters, but nothing seems to work.

Jayla is starting to think that her model suffers from the exploding gradient problem. 

**Which of the following techniques will make Jayla's model more robust to the exploding gradient problem?**


### **Choices** :

- Gradient clipping
- Weight regularization
- Increasing the batch size
- Modifying the model architecture to include batch normalization


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The Exploding Gradient Problem is an issue that occurs in training artificial neural networks when large error gradients accumulate and result in very large updates to the network weights during training. This can cause the network to become unstable and prevent it from learning effectively.

One way to overcome the exploding gradient problem is to use gradient clipping, where the gradients are capped at a maximum value to prevent them from growing too large. This can help to stabilize the network and improve its performance.

Another technique is weight regularization, where the network weights are constrained to prevent them from becoming too large. This can help to prevent the gradients from exploding and improve the stability of the network.

Increasing the batch size is not a valid solution to the exploding gradient problem. This technique has nothing to do with the gradients becoming large and exploding.

Modifying the model architecture to include batch normalization can help with the exploding gradient problem. Batch normalization gets rid of the extreme gradients that accumulate, leading to the elimination of the weight fluctuations that result from the increasing gradients.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Exploding Gradient Problem"](https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem) for an introduction to this problem.* ["A Gentle Introduction to Exploding Gradients in Neural Networks"](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/) is a great article covering Exploding Gradients in detail.</p></details>

-----------------------

## Date - 2024-02-19


## Title - Four rounds


### **Question** :

Four rounds of interviews later, Tessa felt so overwhelmed that she started questioning every one of her answers.

As soon as she got to her hotel, she wrote down every question she could remember. She wanted to gain some confidence before going to bed.

She remembered one particular question vividly but couldn't recall her answer. Tessa had to pick every example of an ensemble method from a list of choices.

**Assuming the following are the choices, what of the following are examples of ensemble methods?**


### **Choices** :

- Random Forest
- Decision Trees
- AdaBoost
- Bagging


### **Answer** :

<details><summary>CLICK ME</summary><p>1011</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Ensembling is where we combine a group of models to produce a new model that yields better results than any initial individual models. 

Decision Trees is the only method from the list that's not an ensemble method. 

[Random Forest](https://en.wikipedia.org/wiki/Random_forest) is an algorithm that consists of many individual decision trees. It uses bootstrap aggregating to combine these trees to reach a solution much better than the one provided by any of the individual trees.

[AdaBoost](https://en.wikipedia.org/wiki/AdaBoost) is another ensembling technique. It's also called Adaptive Boosting and, as the name implies, uses boosting.

Finally, [Bootstrap aggregating](https://en.wikipedia.org/wiki/Bootstrap_aggregating), also called "bagging," is also a popular machine learning ensembling technique.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Bagging and Random Forest Ensemble Algorithms for Machine Learning"](https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/) is a great introduction to bagging and Random Forest.* Check out ["Understanding Random Forest"](https://towardsdatascience.com/understanding-random-forest-58381e0602d2) to understand how it works and why it's effective.* [_The Kaggle Book_](https://amzn.to/3kbanRb) is an excellent source for information about ensembling techniques.</p></details>

-----------------------

## Date - 2024-02-20


## Title - Outcome of matches


### **Question** :

Blair is the data scientist for a professional soccer team. 

She is working on a project to build a model that predicts the outcome of matches. Before she can train her model, she needs to understand her data. She decides to start by performing Exploratory Data Analysis (EDA).

**Which of the following are some of the steps Blair takes during this process?**


### **Choices** :

- When doing EDA, Blair needs to evaluate the performance of her models on the data to understand how well they can make predictions.
- During EDA, Blair needs to understand the features in the dataset and the distribution of their values. This will help her identify potential problems or patterns that she can use to improve her model.
- During EDA, Blair needs to assess the data quality, including checking for missing or corrupt values. This is important because poor-quality data can lead to poor model performance.
- During EDA, Blair needs to learn the distribution of the target variable, which is the variable that she is trying to predict.


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Exploratory Data Analysis is an essential step in any machine-learning project. 

It allows us to explore and understand our data before we build a model. During this process, we investigate the dataset to discover valuable patterns, spot any potential anomalies, and use statistics and plots to test different hypotheses and check assumptions. 

We also focus on understanding the distribution of the target variable and assessing the data quality, including checking for missing or corrupt values.

Building a model is not part of the Exploratory Data Analysis process, so we don't evaluate the model's performance during this phase.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["What is Exploratory Data Analysis"](https://www.ibm.com/cloud/learn/exploratory-data-analysis) for an explanation of the EDA process.* The [_Feature Engineering for Machine Learning_](https://amzn.to/3JWZxZl) is a great book covering the entire Feature Engineering process.</p></details>

-----------------------

## Date - 2024-02-21


## Title - Image recognition


### **Question** :

Dahlia has been working on an image recognition app that uses a deep learning model to classify different objects. She's been struggling to get her model to perform appropriately.

Dahlia knows her model suffers from the vanishing gradient problem. She decides to research every possible option to improve her model.

**Which of the following techniques will help Dahlia's model overcome the vanishing gradient problem?**


### **Choices** :

- Dahlia should try using the ReLU activation function, which is known to reduce the vanishing gradient problem.
- Dahlia should consider introducing Batch Normalization to her model architecture to combat the vanishing gradient problem.
- Dahlia should increase the batch size to combat the vanishing gradient problem.
- Dahlia should carefully initialize the weights of her model, such as using He initialization, to reduce the vanishing gradient problem.


### **Answer** :

<details><summary>CLICK ME</summary><p>1101</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The [Vanishing Gradient Problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) occurs when the gradients of the loss function approach zero. This prevents the model from updating the weights and learning effectively. This problem is widespread when using the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) and [tanh](https://www.sciencedirect.com/topics/mathematics/hyperbolic-tangent-function) activation functions in deep neural networks.

One way to overcome the vanishing gradient problem is to use the [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation function. ReLU is less likely to saturate, and its derivative is 1 for values larger than zero. This can help prevent the gradients from becoming too small and vanishing.

Another technique that can help is introducing Batch Normalization to the model architecture. This normalizes the input to a layer, ensuring that the values don't reach the edges where the derivatives are too small. We can combat the vanishing gradient problem by modifying the input this way.

Increasing the batch size is not a valid solution to the vanishing gradient problem. This technique has nothing to do with the gradients becoming small and vanishing.

Carefully initializing the model weights can also help reduce the vanishing gradient problem. If we use sigmoid or tanh as our activation functions, and many of the weights are initialized with values too small or too large, we will end up with derivatives close to zero. Using [He initialization](https://arxiv.org/abs/1704.08863) can prevent this from happening.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["How to Fix the Vanishing Gradients Problem Using the ReLU"](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/) is a great explanation of how to approach this problem.* Check ["On weight initialization in deep neural networks"](https://arxiv.org/abs/1704.08863). It's an excellent paper covering weight initialization.</p></details>

-----------------------

## Date - 2024-02-22


## Title - Master's exam


### **Question** :

Millie is taking an exam for her Master's degree in machine learning.

One of the questions tests her knowledge of Supervised Learning techniques. She needs to select every problem she can solve using Supervised Learning. 

**Which of the following problems should Millie select as examples of Supervised Learning?**


### **Choices** :

- Given a dataset of emails and their classification, build an application to determine whether an email is spam.
- Given a dataset of audio files and their text transcripts, build an application that turns any audio snippet into text.
- Given a dataset of translations between English and Spanish, build an application that turns any sentence written in English into Spanish.
- Given a dataset of images of circuit boards and whether they work, build an application that determines if a picture of a circuit board corresponds to a working board.


### **Answer** :

<details><summary>CLICK ME</summary><p>1111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>There are multiple ways to approach these problems, but we can solve all of them using [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning).

Supervised learning involves using labeled training data to make predictions or classify data. Every one of these examples gives us a dataset with examples of inputs and outputs.

Transcription and translation are interesting examples because we can also solve these problems using Unsupervised Learning. For this particular question, Millie has access to labeled data, and she can frame the problem using Supervised Learning.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Supervised and Unsupervised Machine Learning Algorithms"](https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/) is an excellent introduction to the differences between supervised and unsupervised learning.</p></details>

-----------------------

## Date - 2024-02-23


## Title - News cycle


### **Question** :

Elaina works for a national pollster, and she is building a machine-learning model to classify the topic from the text of a news article.

She has access to a dataset containing every news article from the past year. Elaina's goal is to classify each piece into ten topics. Her team labeled the dataset, and Elaina will use a supervised learning model to make the predictions.

**How would you recommend Elaina split her dataset into training and validation?**


### **Choices** :

- Elaina should split her dataset based on the date of the story, ensuring every article around the same date goes into the same split.
- Elaina should split her dataset based on the classification of every story, ensuring every article from the same class goes into the same split.
- Elaina should split her dataset based on the classification of every story, ensuring every article from the same class goes into a different split.
- Elaina should split her dataset randomly, ensuring each split properly represents the overall dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>The samples in Elaina's dataset are not independent. Multiple stories about the same topic will appear around the same time. For example, during a major upset win in sports, Elaina will find various news articles related to the same topic.

If Elaina splits her dataset randomly, the training and validation sets will likely contain the same stories. This will lead to a leaky validation strategy. Here is an excerpt from [The Kaggle Book](https://amzn.to/3kbanRb):

> In a leaky validation strategy, the problem is that you have arranged your validation strategy in a way that favors better validation scores because some information leaks from the training data.

Elaina's model can use the date as a clue to predict the topic of a news article: any story happening around the same date will likely belong to the same topic, which will make some of the samples from the validation set simple for the model to predict.

The solution is to group news articles by their date and ensure they go together into the same set.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* [_The Kaggle Book_](https://amzn.to/3kbanRb) explains different feature engineering techniques.* ["Target Leakage in Machine Learning"](https://www.youtube.com/watch?v=dWhdWxgt5SU) is a YouTube presentation that covers leakage, including during the partitioning of a dataset.</p></details>

-----------------------

## Date - 2024-02-24


## Title - Game theory


### **Question** :

Amari has been studying game theory and has learned about Nash equilibrium. She decides to test her understanding by designing a simple game and playing it with her friend, Jake.

Each player can choose to either cooperate or defect. If both players cooperate, they each receive a reward of $3. If one player defects and the other cooperates, the defector receives a reward of $5, while the cooperator receives nothing. If both players defect, they each receive a reward of $1.

**What is the Nash equilibrium in this game?**


### **Choices** :

- Amari and Jake cooperate, and both receive a reward of $3.
- Amari and Jake defect, and both receive a reward of $1.
- Amari defects and receives $5, while Jake cooperates and receives nothing.
- Amari cooperates and receives nothing, while Jake defects and receives $5.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A Nash equilibrium is a state in which all players in a game have made their best decisions given the decisions of the other players. In this game, the only Nash equilibrium is when both players defect because each receives the highest reward possible given the other player's decision.

If Amari defects and Jake cooperates, Jake will receive a higher reward than if they both cooperate. This means that Jake has the incentive to change his decision to defect, and the game is not in a Nash equilibrium.

Similarly, if Amari cooperates and Jake defects, Amari will receive nothing, which is less than the reward she would receive if they both cooperated. This means that Amari is incentivized to change her decision to cooperate, and the game is not in a Nash equilibrium.

Therefore, the only Nash equilibrium in this game is when both players defect.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["An Introduction to the Nash Equilibrium"](https://builtin.com/data-science/nash-equilibrium) for a great explanation of one of the cornerstones of Game Theory.</p></details>

-----------------------

## Date - 2024-02-25


## Title - Candy flavors


### **Question** :

Jocelyn has been selling candy online for about a year, and after a lot of back and forth, she wants to introduce a second flavor. 

She is trying to run a tight ship, so she needs to ensure she doesn't order too much of the new flavor from her manufacturer. Jocelyn knows she needs to predict how much candy she will sell in the coming weeks.

After a few days, Jocelyn builds an algorithm to predict the probability of an existing customer buying the new flavor. The function looks like this:

```
probability = σ(a + 0.45b - 0.84)
```

Where:
* `a`: Indicates whether the customer bought candy during the last month.
* `b`: Indicates whether the customer bought more than one candy flavor before.
* `σ`: Is the sigmoid function.

**Assuming Patricia is a customer who bought two packages of different candy flavors last week, what would be the probability of her purchasing the new flavor?**


### **Choices** :

- The probability is `0.26`
- The probability is `0.65`
- The probability is `0.80`
- The probability is `1.0`


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We need to understand how to compute the Sigmoid function to answer this question. The Sigmoid function takes a value as input and outputs another real value between `0` and `1`. We say that Sigmoid "squeezes" the input into that range.

Here is the formula:

![Sigmoid](https://user-images.githubusercontent.com/1126730/196755518-311dd425-676e-4c85-be1f-467694879c30.jpg)

In Python, the Sigmoid function looks like this:

```
import math

def sigmoid(x):
    return 1/(1 + math.exp(-x))
```

We can now answer the question by solving the following equation:

```
probability = sigmoid(a + 0.45 * b - 0.84)
```

Patricia bought candy during the last month, so `a = 1`, and she bought different flavors, so `b = 1`:

```
probability = sigmoid(1 + 0.45 * 1 - 0.84)
probability = 0.6479408020806503
```

Therefore, Patricia's probability of buying the new flavor is `0.65`.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* For a complete description of the Sigmoid function, check the ["Logistic function"](https://en.wikipedia.org/wiki/Logistic_function) Wikipedia page.</p></details>

-----------------------

## Date - 2024-02-26


## Title - Model recipes


### **Question** :

Gia's been working on a model to classify photos of food. 

Her company is building an application that will let users snap a picture of a plate at a restaurant and show them a potential recipe so they can cook it at home.

After a year of work, Gia's model was working great. The company launched the model worldwide and started monitoring user feedback.

Unfortunately, users from an Asian country complained because the model wasn't working for them.

**What is the most likely reason for the problem?**


### **Choices** :

- Gia's model didn't have enough complexity to learn all the data, so it's normal to have problems with certain regions.
- Gia needed to train the model for more time to fully capture the dataset's information.
- Gia's model is suffering from data drift.
- Gia's model is suffering from sampling bias.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>A common problem in machine learning is that a model that shows promising results during evaluation doesn't perform well when deployed in production. 

Gia's model doesn't lack complexity. If that were the case, the model would be underfitting and give poor results across all regions where the application works. The training time is not a problem either for similar reasons.

Data and concept drift are common problems with models in production. However, they arise when the environment changes over time, and so does the input to the model. In this case, the problem appeared straight after deployment.

The most likely reason for this problem is that Gia didn't have enough data from the Asian country where the application is failing, so the model is struggling to recognize food from that region.

This issue is called "sampling bias." It explains why the problem occurred in one particular country. Sampling bias is difficult to detect during development because the data is missing from the training and test datasets, so we can't notice it while evaluating the model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Sampling bias"](https://en.wikipedia.org/wiki/Sampling_bias) for a complete explanation of this problem.</p></details>

-----------------------

## Date - 2024-02-27


## Title - Driving sales


### **Question** :

Wynter is working at a marketing agency and is trying to build a machine-learning model to predict which advertisements will be most effective in driving sales. 

Unfortunately, this is Wynter's first real project, and the model is not doing great. Her colleague recommended tuning the model's hyperparameters, but neither of them knows much about that.

**Which of the following best describes how hyperparameter tuning impacts the performance of a model?**


### **Choices** :

- Hyperparameter tuning has no impact on the performance of a machine learning model.
- Hyperparameter tuning significantly improves the performance of a machine learning model by isolating the most relevant samples from the dataset.
- Hyperparameter tuning significantly improves the performance of a machine learning model by fine-tuning the model's ability to learn and generalize to new data.
- Hyperparameter tuning significantly improves the performance of a machine learning model by selecting the most relevant features from the dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We use the term "hyperparameter" to refer to the settings we can use to control the learning process. We set these "knobs" and "levers" before training a model. In contrast, we use "parameters" to refer to variables internal to the model whose values we estimate (learn) during the learning process using data.

A good way of thinking about this:

* Parameters: We learn their values during training. We do not set their values manually.
* Hyperparameters: The settings we fix before the learning process. We cannot learn these values during training.

Hyperparameter tuning is the process of adjusting the hyperparameters of a machine-learning model to achieve better performance. Hyperparameters have nothing to do with the data. They aren't about features or samples. Hyperparameter tuning can have a significant impact on the model's ability to learn and generalize to new data. 

We can fine-tune a model to improve its accuracy, precision, recall, and other performance metrics by choosing the values for the hyperparameters that result in the best performance.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Overview of hyperparameter tuning"](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview) is a great introduction to hyperparameters and the process of finding their optimal value.* [Hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization)* [What is the Difference Between a Parameter and a Hyperparameter?](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)</p></details>

-----------------------

## Date - 2024-02-28


## Title - All-or-nothing affair


### **Question** :

For Charlee, multi-choice questions were always challenging.

She was much better with words and always found a way to reason while elaborating on her answers. But multi-choice questions were cold and to the point. They were an all-or-nothing affair. 

Charlee didn't have a choice. If she wanted to graduate, she had to answer the question:

**Which of the following sentences are true about neural networks?**


### **Choices** :

- We can only optimize neural networks using the Gradient Descent algorithm.
- Neural networks can find the optimal solution for convex problems.
- Neural networks can find the optimal solution for concave problems.
- Neural networks can approximate any function when using non-linear activations.


### **Answer** :

<details><summary>CLICK ME</summary><p>0111</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Gradient descent is an iterative optimization algorithm used to find the local minimum of a function. The algorithm works by taking steps proportional to the negative of the function's gradient at the current point. Gradient descent is an excellent choice to optimize neural networks, but it's not the only way. We can use, for example, the Adam algorithm, a combination of the AdaGrad and RMSProp algorithms.

Using Gradient descent, we can find the optimal solution for convex and concave problems. The former depends on a suitable configuration of the learning rate, while the latter is not guaranteed: the algorithm might converge to a local minimum instead of the global minimum. However, finding optimal solutions for both types of objective functions is possible.

Finally, thanks to the [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem), we can turn a two-layer neural network into a universal function approximator when using non-linear activation functions.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Gradient Descent For Machine Learning"](https://machinelearningmastery.com/gradient-descent-for-machine-learning/) for a description of the algorithm.* Check the ["Universal approximation theorem"](https://en.wikipedia.org/wiki/Universal_approximation_theorem) on Wikipedia for more information about the power of neural networks.* ["Gentle Introduction to the Adam Optimization Algorithm for Deep Learning"](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning) is an excellent explanation of the Adam algorithm.</p></details>

-----------------------

## Date - 2024-02-29


## Title - Advertising spending


### **Question** :

Lily works at an advertising agency where she used a model to predict how much a client should spend on advertising.

After training and testing her model, Lily felt confident it was ready for production: its performance was excellent. She deployed the model, and the agency started using it to make advertising recommendations to its clients.

Hours later, a client complained that the model's predictions were consistently off by a large margin. 

**What could be the cause of the problem with the model?**


### **Choices** :

- The model is underfitting and needs more complexity.
- The model is overfitting and needs more regularization.
- The model is suffering from data drift.
- The model is suffering from sampling bias.


### **Answer** :

<details><summary>CLICK ME</summary><p>0001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Overfitting and underfitting are common problems in machine learning. They occur when a model is not appropriately trained or has too much or too little capacity to learn from the data. Overfitting occurs when the model has learned the noise in the data and is unable to generalize to new data, while underfitting occurs when the model is unable to learn the underlying patterns in the data and performs poorly on both the training and test sets.

In this example, Lily's model did well during training and testing, so assuming she had good data, it's unlikely the model was either overfitting or underfitting.

Data drift is also unlikely. Data drift occurs when the data distribution changes over time, but in this case, Lily's model failed to work immediately after deployment, so there hasn't been enough time for the data to drift.

Sampling bias is likely the cause of the problem. Sampling bias occurs when the training data does not represent the entire population. Lily might have trained the model on different data from the client's, and that's why the predictions are not correct.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Sampling bias"](https://en.wikipedia.org/wiki/Sampling_bias) for a complete explanation of this problem.</p></details>

-----------------------

## Date - 2024-03-01


## Title - Type I


### **Question** :

In statistics, the notion of a statistical error is integral to hypothesis testing. When testing the null hypothesis, there are two types of errors: **type I** and **type II**.

Look at this confusion matrix of a hypothetical machine learning model that classifies spam emails. The "P" stands for "Positive" samples, and the "N" stands for "Negative" samples. Our default assumption is that emails are not spam.

![Confusion Matrix](https://user-images.githubusercontent.com/1126730/188163753-6424565d-3c8c-421e-95fb-7d89aba1453a.jpg)

**How many type I errors does this model have?**


### **Choices** :

- There are 75 type I errors.
- There are 22 type I errors.
- There are 12 type I errors.
- There are 121 type I errors.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Type I errors are the same as **false positives**. For example, if we mark a valid email as spam, we are in the presence of a false positive. Type I errors are the rejection of a true [null hypothesis](https://www.investopedia.com/terms/n/null_hypothesis.asp) by mistake.

On the other hand, Type II errors are the same as **false negatives**. For example, if we let a spam message pass as a valid email, we are in the presence of a false negative. This is a type II error because we accept the conclusion of the email being good, even though it is incorrect. Type II errors are the acceptance of a false null hypothesis by mistake.

Therefore, there are a total of 12 type I errors in this hypothetical model.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Confusion Matrix"](https://articles.bnomial.com/confusion-matrix) for a full explanation of how a confusion matrix works and how you can use them as part of your work.* Check out ["Type I and type II errors"](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors) for the definition and examples of each type of error. * ["Understanding Null Hypothesis Testing"](https://opentextbc.ca/researchmethods/chapter/understanding-null-hypothesis-testing/) is an excellent article about hypothesis testing.</p></details>

-----------------------

## Date - 2024-03-02


## Title - Classification partitions


### **Question** :

Morgan works at a research lab specializing in machine learning and data analysis. 

She is currently writing a paper on entropy in classification and wants to ensure she accurately describes the concept to her readers. She uses a Decision Tree as the foundation of her explanations.

**In this context, what does high entropy say about the partitions in a classification problem?**


### **Choices** :

- High entropy means the partitions are pure.
- High entropy means the partitions are not pure.
- High entropy means the partitions are useful.
- High entropy means the partitions are not useful.


### **Answer** :

<details><summary>CLICK ME</summary><p>0100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Entropy measures how random the information being processed is. The higher the entropy, the harder it is to draw conclusions from that information.

Decision Trees use a "purity" metric to split at each node. Low entropy leads to pure nodes, where 100% of the data belongs to a single partition, while high entropy leads to impure nodes, where the data is split evenly between partitions.

High entropy means that the different classes are mixed and not well separated. If a partition has high entropy, the class labels are mixed or not pure. This makes it hard for a classifier to predict the class label of new samples accurately. Low entropy means that the classes are more distinct and easier to predict.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Understanding Entropy: the Golden Measurement of Machine Learning"](https://towardsdatascience.com/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3) for an introduction to entropy.</p></details>

-----------------------

## Date - 2024-03-03


## Title - Presentation slides


### **Question** :

Vanessa is preparing a conference talk on different log functions and the best approach to use each.

She finished most of the slides, but the final one needs some work: Vanessa plans to end her presentation by talking about the Log loss.

Vanessa wants to mention a few problems where the log loss is helpful.

**Which of the following problems should Vanessa include in her presentation?**


### **Choices** :

- Predict whether a dog will bark in the middle of the night.
- Determine whether a credit card transaction is invalid.
- Predict the stock price at the time the market closes.
- Predict how much rain will fall the next day.


### **Answer** :

<details><summary>CLICK ME</summary><p>1100</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Log loss is a function that we commonly use in classification problems. It returns the negative logarithm of the product of probabilities.

Log loss indicates how close a prediction probability is to the target value. The more the predicted probability differs from the target value, the higher the log loss.

Vanessa should only include examples of classification problems. From the list of options, predicting whether a dog will bark and determining whether a credit card transaction is valid are classification problems. The other two problems are regression problems and not a good fit for the log loss.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>- ["Understanding binary cross-entropy / log loss: A visual explanation"](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) is an excellent introduction to binary cross-entropy.- Another article that helps understand binary cross-entropy is ["Binary Cross Entropy/Log Loss for Binary Classification"](https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/).</p></details>

-----------------------

## Date - 2024-03-04


## Title - Administrative assistant


### **Question** :

Noelle is a student who takes college classes at night while she works full-time as an administrative assistant. As part of her Machine Learning course, she is learning about Perceptrons and how to implement them from scratch.

One of the class assignments involves writing and running a Perceptron by hand. For this assignment, she is given a training sample `[-2.0, 4.0]` with a true label of `0`.

**Assuming the initial weights of the Perceptron are initialized with zeros, and the predicted label is `1`, what does the weight vector look like after one pass?**


### **Choices** :

- The weight vector will be `[0.0, 0.0]`.
- The weight vector will be `[-2.0, 4.0]`.
- The weight vector will be `[2.0, -4.0]`.
- The weight vector will be `[-1.0, 2.0]`.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Since the true label is `0` but the Perceptron predicted `1`, the error will be `0 - 1 = -1`, therefore, the Perceptron will update the weight vector by the negative feature vector.

As a reminder, the following snippet of code illustrates how to update the weights of a Perceptron:

```
import numpy as np

x = np.array([-2.0, 4.0])
w = np.array([0.0, 0.0])
error = -1
w = w + error * x
print(w)
```

The feature vector is `[-2.0, 4.0]`, and the weights were initialized with `[0.0, 0.0]`. The update will set them to `[2.0, -4.0].`</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* The ["Perceptron Algorithm for Classification in Python"](https://machinelearningmastery.com/perceptron-algorithm-for-classification-in-python/) is a great introduction to the Perceptron.</p></details>

-----------------------

## Date - 2024-03-05


## Title - Robotic facility


### **Question** :

Makayla works at a robotics facility where she processes pictures of the robots before they are shipped out.

Makayla noticed that the pictures of the robots often had different degrees of rotation and realized that the Convolutional Neural Network she had built couldn't process these images properly.

**Which of the following approaches should Makayla use to handle rotation in the pictures?**


### **Choices** :

- Add a layer at the beginning of the model that rotates the data to the correct position.
- Add a layer at the end of the model that rotates the data to the correct position.
- Include rotated versions of the images in the training data to build some rotation invariability into the model.
- Replace the Convolutional Neural Network with a Multilayer Perceptron because they are rotation invariant.


### **Answer** :

<details><summary>CLICK ME</summary><p>0010</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>[Convolutional Neural Networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) are [translation invariant but not rotation invariant](https://pyimagesearch.com/2021/05/14/are-cnns-invariant-to-translation-rotation-and-scaling/). They can recognize the same patterns independently of where they show in an image, but they don't have the same ability to recognize shapes with various degrees of rotation. Multilayer Perceptrons are not rotation invariant either.

There's also no out-of-the-box mechanism for a layer to recognize the orientation of an image and much less "rotate" it to its correct position. 

A way to train a model to recognize pictures regardless of their orientation is to extend the dataset with rotated images. If we expect to see images at 0, 90, and 180 degrees, we must teach our model to recognize them. ["Correcting Image Orientation Using Convolutional Neural Networks"](https://d4nst.github.io/2017/01/12/image-orientation/) is an excellent article covering this approach.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* ["Correcting Image Orientation Using Convolutional Neural Networks"](https://d4nst.github.io/2017/01/12/image-orientation/) is a great article covering practical ways to get a network to recognize rotated pictures.* ["Are CNNs invariant to translation, rotation, and scaling?"](https://pyimagesearch.com/2021/05/14/are-cnns-invariant-to-translation-rotation-and-scaling/) goes into more detail about whether convolutional neural networks are translation, rotation, and scale invariant.* Check ["How Do Convolutional Layers Work in Deep Learning Neural Networks?"](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/) for an introduction to how convolutional layers work.</p></details>

-----------------------

## Date - 2024-03-06


## Title - Filtering features


### **Question** :

Ruth is a software engineer who has recently joined a machine learning startup. Her team is working on a project that involves building a model to classify different types of flowers based on their physical characteristics.

While reviewing the code, Ruth came across the line `X[y == 1, 0]`. She knows that `X` is the feature matrix and `y` is the target vector that holds the class labels for each sample. However, she is unsure what the line `X[y == 1, 0]` does.

**Assuming the code uses the Numpy library, which of the following is the correct interpretation of `X[y == 1, 0]`?**


### **Choices** :

- The code returns the first feature of every sample that belongs to class 1.
- The code returns every sample that belongs to class 1.
- The code returns the second feature of every sample that belongs to class 0.
- The code returns the label for the first sample in the dataset.


### **Answer** :

<details><summary>CLICK ME</summary><p>1000</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Let's break down `X[y == 1, 0]`:

We have `y == 1`, which will return `True` for every sample that belongs to class 1, and `False` otherwise. We use this as a selection mask on the set `X`, which will return every row that belongs to class 1.

Notice, however, that we don't return every feature from `X`. Instead, we only return the first feature (index = 0.)

Therefore, the code returns the first feature of every sample that belongs to class 1.

We can write a simple snippet of code to illustrate this:

```
import numpy as np

X = np.array([[2, 1], [4, 3], [6, 5], [8, 7]])
y = np.array([0, 1, 1, 0])
print(X[y == 1, 0])
```

If we run the above code snippet, we will get:

```
[4 6]
```

Notice how the result is the first feature of the second and third rows, which correspond to the samples with class 1.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check the ["NumPy quickstart"](https://numpy.org/doc/stable/user/quickstart.html) tutorial for an introduction to Numpy.</p></details>

-----------------------

## Date - 2024-03-07


## Title - AND logic gate


### **Question** :

We want to use a Perceptron to represent the AND logic gate. As a reminder, here is how the AND gate works:

* 0 and 0 = 0
* 0 and 1 = 0
* 1 and 0 = 0
* 1 and 1 = 1

Our Perceptron will have two inputs, two weights, and a bias parameter. 

**Which of the following parameters will make our Perceptron act as an AND gate?**


### **Choices** :

- `w1 = 0.6`, `w2 = 0.6`, `b = 0.0`
- `w1 = 0.6`, `w2 = 0.6`, `b = -0.8`
- `w1 = 1.0`, `w2 = 1.0`, `b = -1.5`
- You need more than one Perceptron to represent the AND gate.


### **Answer** :

<details><summary>CLICK ME</summary><p>0110</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>We can represent an AND gate using a single Perceptron. Here is a simple implementation with two input values, `x1` and `x2`:

```
def perceptron(x1, x2, w1, w2, b):
    return int((x1*w1 + x2*w2 + b) > 0)
```

Using this function, we can try the different configurations suggested in this question. Here is an example of running the Perceptron for the AND gate using a set of parameters:

```
w1 = 1.0
w2 = 1.0
b = -1.5
assert perceptron(0, 0, w1, w2, b) == 0
assert perceptron(0, 1, w1, w2, b) == 0
assert perceptron(1, 0, w1, w2, b) == 0
assert perceptron(1, 1, w1, w2, b) == 1
```

Notice how each `assert` validates a specific pair of inputs. If there are no errors, then we can conclude the parameters work.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Neural Representation of AND, OR, NOT, XOR and XNOR Logic Gates (Perceptron Algorithm)"](https://medium.com/@stanleydukor/neural-representation-of-and-or-not-xor-and-xnor-logic-gates-perceptron-algorithm-b0275375fea1) for a deep dive on how to set up a Perceptron to represent multiple logic gates.</p></details>

-----------------------

## Date - 2024-03-08


## Title - Excellent book


### **Question** :

Trinity found an excellent explanation of One-Hot Encoding in one of her books.

She plans to use the technique with some features of her dataset, but she wanted to do some research before pulling the trigger.

**Which of the following statements do you think Trinity found in the book?**


### **Choices** :

- One-Hot Encoding creates one additional column for each possible category.
- One-Hot Encoding encodes a numerical feature into its categorical representation.
- One-Hot Encoding transforms string variables into a single integer.
- One-Hot Encoding transforms string variables using a numerical representation.


### **Answer** :

<details><summary>CLICK ME</summary><p>1001</p></details>


### **Explaination** :

<details><summary>CLICK ME</summary><p>Categorical data are variables that contain label values rather than numeric values. For example, a variable representing the temperature with values "hot," "warm," and "cold" is a categorical variable.

Although some algorithms can use categorical data directly, most can't: they require the data to be numeric. One-Hot Encoding is one of the techniques we can use to turn categorical data into a numerical representation.

For example, assume we have a dataset with a single feature called "temperature" that could have the values "hot," "warm," and "cold." Applying One-Hot Encoding will get us a new dataset with three features, one for each value of the original "temperature" column. 

A sample that had the value "warm" in the previous column will now have the value `0` for both "hot" and "cold" and the value `1` under the "warm" feature.

Therefore, One-Hot Encoding creates one additional column for each possible category, and these columns use a numerical representation.</p></details>


### **References**: 

<details><summary>CLICK ME</summary><p>* Check ["Why One-Hot Encode Data in Machine Learning?"](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) for an explanation of how One-Hot Encoding works.</p></details>

-----------------------

